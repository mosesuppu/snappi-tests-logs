13/11/2025 17:46:18 conftest.pytest_sessionstart             L0481 INFO   | Invoking /data/harshit/sonic-mgmt/tests/build-gnmi-stubs.sh with base directory: /data/harshit/sonic-mgmt/tests
13/11/2025 17:46:19 conftest.pytest_sessionstart             L0491 INFO   | Output of /data/harshit/sonic-mgmt/tests/build-gnmi-stubs.sh:
Removing existing directory: /data/harshit/sonic-mgmt/tests/common/sai_validation/github.com/openconfig
Creating directory: /data/harshit/sonic-mgmt/tests/common/sai_validation/github.com/openconfig
Removing existing directory: /data/harshit/sonic-mgmt/tests/common/sai_validation/generated
Creating directory: /data/harshit/sonic-mgmt/tests/common/sai_validation/generated
Cloning https://github.com/openconfig/gnmi.git into /data/harshit/sonic-mgmt/tests/common/sai_validation/github.com/openconfig
Generating gRPC stubs...
gRPC stubs generated successfully.
Creating empty __init__.py files under /data/harshit/sonic-mgmt/tests/common/sai_validation/generated/github/
Moving generated files to correct locations...
Files moved successfully.
Removing /data/harshit/sonic-mgmt/tests/common/sai_validation/generated/github.com directory...
/data/harshit/sonic-mgmt/tests/common/sai_validation/generated/github.com directory removed successfully.

13/11/2025 17:46:19 conftest.pytest_sessionstart             L0502 INFO   | Added /data/harshit/sonic-mgmt/tests/common/sai_validation/generated to sys.path
13/11/2025 17:46:21 __init__.pytest_collection_modifyitems   L0664 INFO   | Available basic facts that can be used in conditional skip:
{
  "topo_type": "ptf",
  "topo_name": "ptf64",
  "testbed": "vms-snappi-sonic",
  "asic_subtype": "broadcom",
  "asic_type": "broadcom",
  "branch": "202505",
  "build_date": "Thu Nov  6 23:47:13 UTC 2025",
  "build_number": 29,
  "build_version": "202505.29-dirty-20251106.112914",
  "built_by": "_devpublish@qnc-sonic-05",
  "commit_id": "c4bbc9931",
  "debian_version": "12.12",
  "feature_status": {
    "bgp": "enabled",
    "bmp": "enabled",
    "database": "always_enabled",
    "dhcp_relay": "disabled",
    "eventd": "enabled",
    "frr_bmp": "enabled",
    "gnmi": "enabled",
    "lldp": "enabled",
    "macsec": "disabled",
    "mgmt-framework": "enabled",
    "mux": "always_disabled",
    "nat": "disabled",
    "pmon": "enabled",
    "radv": "enabled",
    "sflow": "disabled",
    "snmp": "enabled",
    "swss": "enabled",
    "syncd": "enabled",
    "teamd": "enabled",
    "telemetry": "disabled"
  },
  "hwsku": "Juniper-QFX5241-64-OD",
  "is_chassis": false,
  "is_dpu": false,
  "is_mgmt_ipv6_only": false,
  "is_multi_asic": false,
  "is_smartswitch": false,
  "is_supervisor": false,
  "kernel_version": "6.1.0",
  "libswsscommon": "1.0.0",
  "num_asic": 1,
  "platform": "x86_64-juniper_qfx5241-r0",
  "release": "202505",
  "secure_boot_image": "no",
  "sonic_os_version": 12,
  "sonic_utilities": 1.2,
  "asic_gen": "unknown",
  "minigraph_interfaces": [
    {
      "addr": "10.0.0.0",
      "attachto": "Ethernet0",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.1",
      "prefixlen": 31,
      "subnet": "10.0.0.0/31"
    },
    {
      "addr": "fc00::1",
      "attachto": "Ethernet0",
      "mask": "126",
      "peer_addr": "fc00::2",
      "prefixlen": 126,
      "subnet": "fc00::/126"
    },
    {
      "addr": "10.0.0.26",
      "attachto": "Ethernet104",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.27",
      "prefixlen": 31,
      "subnet": "10.0.0.26/31"
    },
    {
      "addr": "fc00::35",
      "attachto": "Ethernet104",
      "mask": "126",
      "peer_addr": "fc00::36",
      "prefixlen": 126,
      "subnet": "fc00::34/126"
    },
    {
      "addr": "10.0.0.28",
      "attachto": "Ethernet112",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.29",
      "prefixlen": 31,
      "subnet": "10.0.0.28/31"
    },
    {
      "addr": "fc00::39",
      "attachto": "Ethernet112",
      "mask": "126",
      "peer_addr": "fc00::3a",
      "prefixlen": 126,
      "subnet": "fc00::38/126"
    },
    {
      "addr": "10.0.0.30",
      "attachto": "Ethernet120",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.31",
      "prefixlen": 31,
      "subnet": "10.0.0.30/31"
    },
    {
      "addr": "fc00::3d",
      "attachto": "Ethernet120",
      "mask": "126",
      "peer_addr": "fc00::3e",
      "prefixlen": 126,
      "subnet": "fc00::3c/126"
    },
    {
      "addr": "10.0.0.32",
      "attachto": "Ethernet128",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.33",
      "prefixlen": 31,
      "subnet": "10.0.0.32/31"
    },
    {
      "addr": "fc00::41",
      "attachto": "Ethernet128",
      "mask": "126",
      "peer_addr": "fc00::42",
      "prefixlen": 126,
      "subnet": "fc00::40/126"
    },
    {
      "addr": "10.0.0.34",
      "attachto": "Ethernet136",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.35",
      "prefixlen": 31,
      "subnet": "10.0.0.34/31"
    },
    {
      "addr": "fc00::45",
      "attachto": "Ethernet136",
      "mask": "126",
      "peer_addr": "fc00::46",
      "prefixlen": 126,
      "subnet": "fc00::44/126"
    },
    {
      "addr": "10.0.0.36",
      "attachto": "Ethernet144",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.37",
      "prefixlen": 31,
      "subnet": "10.0.0.36/31"
    },
    {
      "addr": "fc00::49",
      "attachto": "Ethernet144",
      "mask": "126",
      "peer_addr": "fc00::4a",
      "prefixlen": 126,
      "subnet": "fc00::48/126"
    },
    {
      "addr": "10.0.0.38",
      "attachto": "Ethernet152",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.39",
      "prefixlen": 31,
      "subnet": "10.0.0.38/31"
    },
    {
      "addr": "fc00::4d",
      "attachto": "Ethernet152",
      "mask": "126",
      "peer_addr": "fc00::4e",
      "prefixlen": 126,
      "subnet": "fc00::4c/126"
    },
    {
      "addr": "10.0.0.4",
      "attachto": "Ethernet16",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.5",
      "prefixlen": 31,
      "subnet": "10.0.0.4/31"
    },
    {
      "addr": "fc00::9",
      "attachto": "Ethernet16",
      "mask": "126",
      "peer_addr": "fc00::a",
      "prefixlen": 126,
      "subnet": "fc00::8/126"
    },
    {
      "addr": "10.0.0.40",
      "attachto": "Ethernet160",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.41",
      "prefixlen": 31,
      "subnet": "10.0.0.40/31"
    },
    {
      "addr": "fc00::51",
      "attachto": "Ethernet160",
      "mask": "126",
      "peer_addr": "fc00::52",
      "prefixlen": 126,
      "subnet": "fc00::50/126"
    },
    {
      "addr": "10.0.0.42",
      "attachto": "Ethernet168",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.43",
      "prefixlen": 31,
      "subnet": "10.0.0.42/31"
    },
    {
      "addr": "fc00::55",
      "attachto": "Ethernet168",
      "mask": "126",
      "peer_addr": "fc00::56",
      "prefixlen": 126,
      "subnet": "fc00::54/126"
    },
    {
      "addr": "10.0.0.44",
      "attachto": "Ethernet176",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.45",
      "prefixlen": 31,
      "subnet": "10.0.0.44/31"
    },
    {
      "addr": "fc00::59",
      "attachto": "Ethernet176",
      "mask": "126",
      "peer_addr": "fc00::5a",
      "prefixlen": 126,
      "subnet": "fc00::58/126"
    },
    {
      "addr": "10.0.0.46",
      "attachto": "Ethernet184",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.47",
      "prefixlen": 31,
      "subnet": "10.0.0.46/31"
    },
    {
      "addr": "fc00::5d",
      "attachto": "Ethernet184",
      "mask": "126",
      "peer_addr": "fc00::5e",
      "prefixlen": 126,
      "subnet": "fc00::5c/126"
    },
    {
      "addr": "10.0.0.48",
      "attachto": "Ethernet192",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.49",
      "prefixlen": 31,
      "subnet": "10.0.0.48/31"
    },
    {
      "addr": "fc00::61",
      "attachto": "Ethernet192",
      "mask": "126",
      "peer_addr": "fc00::62",
      "prefixlen": 126,
      "subnet": "fc00::60/126"
    },
    {
      "addr": "10.0.0.50",
      "attachto": "Ethernet200",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.51",
      "prefixlen": 31,
      "subnet": "10.0.0.50/31"
    },
    {
      "addr": "fc00::65",
      "attachto": "Ethernet200",
      "mask": "126",
      "peer_addr": "fc00::66",
      "prefixlen": 126,
      "subnet": "fc00::64/126"
    },
    {
      "addr": "10.0.0.52",
      "attachto": "Ethernet208",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.53",
      "prefixlen": 31,
      "subnet": "10.0.0.52/31"
    },
    {
      "addr": "fc00::69",
      "attachto": "Ethernet208",
      "mask": "126",
      "peer_addr": "fc00::6a",
      "prefixlen": 126,
      "subnet": "fc00::68/126"
    },
    {
      "addr": "10.0.0.54",
      "attachto": "Ethernet216",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.55",
      "prefixlen": 31,
      "subnet": "10.0.0.54/31"
    },
    {
      "addr": "fc00::6d",
      "attachto": "Ethernet216",
      "mask": "126",
      "peer_addr": "fc00::6e",
      "prefixlen": 126,
      "subnet": "fc00::6c/126"
    },
    {
      "addr": "10.0.0.56",
      "attachto": "Ethernet224",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.57",
      "prefixlen": 31,
      "subnet": "10.0.0.56/31"
    },
    {
      "addr": "fc00::71",
      "attachto": "Ethernet224",
      "mask": "126",
      "peer_addr": "fc00::72",
      "prefixlen": 126,
      "subnet": "fc00::70/126"
    },
    {
      "addr": "10.0.0.58",
      "attachto": "Ethernet232",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.59",
      "prefixlen": 31,
      "subnet": "10.0.0.58/31"
    },
    {
      "addr": "fc00::75",
      "attachto": "Ethernet232",
      "mask": "126",
      "peer_addr": "fc00::76",
      "prefixlen": 126,
      "subnet": "fc00::74/126"
    },
    {
      "addr": "10.0.0.6",
      "attachto": "Ethernet24",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.7",
      "prefixlen": 31,
      "subnet": "10.0.0.6/31"
    },
    {
      "addr": "fc00::d",
      "attachto": "Ethernet24",
      "mask": "126",
      "peer_addr": "fc00::e",
      "prefixlen": 126,
      "subnet": "fc00::c/126"
    },
    {
      "addr": "10.0.0.60",
      "attachto": "Ethernet240",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.61",
      "prefixlen": 31,
      "subnet": "10.0.0.60/31"
    },
    {
      "addr": "fc00::79",
      "attachto": "Ethernet240",
      "mask": "126",
      "peer_addr": "fc00::7a",
      "prefixlen": 126,
      "subnet": "fc00::78/126"
    },
    {
      "addr": "10.0.0.62",
      "attachto": "Ethernet248",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.63",
      "prefixlen": 31,
      "subnet": "10.0.0.62/31"
    },
    {
      "addr": "fc00::7d",
      "attachto": "Ethernet248",
      "mask": "126",
      "peer_addr": "fc00::7e",
      "prefixlen": 126,
      "subnet": "fc00::7c/126"
    },
    {
      "addr": "10.0.0.64",
      "attachto": "Ethernet256",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.65",
      "prefixlen": 31,
      "subnet": "10.0.0.64/31"
    },
    {
      "addr": "fc00::81",
      "attachto": "Ethernet256",
      "mask": "126",
      "peer_addr": "fc00::82",
      "prefixlen": 126,
      "subnet": "fc00::80/126"
    },
    {
      "addr": "10.0.0.66",
      "attachto": "Ethernet264",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.67",
      "prefixlen": 31,
      "subnet": "10.0.0.66/31"
    },
    {
      "addr": "fc00::85",
      "attachto": "Ethernet264",
      "mask": "126",
      "peer_addr": "fc00::86",
      "prefixlen": 126,
      "subnet": "fc00::84/126"
    },
    {
      "addr": "10.0.0.68",
      "attachto": "Ethernet272",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.69",
      "prefixlen": 31,
      "subnet": "10.0.0.68/31"
    },
    {
      "addr": "fc00::89",
      "attachto": "Ethernet272",
      "mask": "126",
      "peer_addr": "fc00::8a",
      "prefixlen": 126,
      "subnet": "fc00::88/126"
    },
    {
      "addr": "10.0.0.70",
      "attachto": "Ethernet280",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.71",
      "prefixlen": 31,
      "subnet": "10.0.0.70/31"
    },
    {
      "addr": "fc00::8d",
      "attachto": "Ethernet280",
      "mask": "126",
      "peer_addr": "fc00::8e",
      "prefixlen": 126,
      "subnet": "fc00::8c/126"
    },
    {
      "addr": "10.0.0.72",
      "attachto": "Ethernet288",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.73",
      "prefixlen": 31,
      "subnet": "10.0.0.72/31"
    },
    {
      "addr": "fc00::91",
      "attachto": "Ethernet288",
      "mask": "126",
      "peer_addr": "fc00::92",
      "prefixlen": 126,
      "subnet": "fc00::90/126"
    },
    {
      "addr": "10.0.0.74",
      "attachto": "Ethernet296",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.75",
      "prefixlen": 31,
      "subnet": "10.0.0.74/31"
    },
    {
      "addr": "fc00::95",
      "attachto": "Ethernet296",
      "mask": "126",
      "peer_addr": "fc00::96",
      "prefixlen": 126,
      "subnet": "fc00::94/126"
    },
    {
      "addr": "10.0.0.76",
      "attachto": "Ethernet304",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.77",
      "prefixlen": 31,
      "subnet": "10.0.0.76/31"
    },
    {
      "addr": "fc00::99",
      "attachto": "Ethernet304",
      "mask": "126",
      "peer_addr": "fc00::9a",
      "prefixlen": 126,
      "subnet": "fc00::98/126"
    },
    {
      "addr": "10.0.0.78",
      "attachto": "Ethernet312",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.79",
      "prefixlen": 31,
      "subnet": "10.0.0.78/31"
    },
    {
      "addr": "fc00::9d",
      "attachto": "Ethernet312",
      "mask": "126",
      "peer_addr": "fc00::9e",
      "prefixlen": 126,
      "subnet": "fc00::9c/126"
    },
    {
      "addr": "10.0.0.8",
      "attachto": "Ethernet32",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.9",
      "prefixlen": 31,
      "subnet": "10.0.0.8/31"
    },
    {
      "addr": "fc00::11",
      "attachto": "Ethernet32",
      "mask": "126",
      "peer_addr": "fc00::12",
      "prefixlen": 126,
      "subnet": "fc00::10/126"
    },
    {
      "addr": "10.0.0.80",
      "attachto": "Ethernet320",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.81",
      "prefixlen": 31,
      "subnet": "10.0.0.80/31"
    },
    {
      "addr": "fc00::a1",
      "attachto": "Ethernet320",
      "mask": "126",
      "peer_addr": "fc00::a2",
      "prefixlen": 126,
      "subnet": "fc00::a0/126"
    },
    {
      "addr": "10.0.0.82",
      "attachto": "Ethernet328",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.83",
      "prefixlen": 31,
      "subnet": "10.0.0.82/31"
    },
    {
      "addr": "fc00::a5",
      "attachto": "Ethernet328",
      "mask": "126",
      "peer_addr": "fc00::a6",
      "prefixlen": 126,
      "subnet": "fc00::a4/126"
    },
    {
      "addr": "10.0.0.84",
      "attachto": "Ethernet336",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.85",
      "prefixlen": 31,
      "subnet": "10.0.0.84/31"
    },
    {
      "addr": "fc00::a9",
      "attachto": "Ethernet336",
      "mask": "126",
      "peer_addr": "fc00::aa",
      "prefixlen": 126,
      "subnet": "fc00::a8/126"
    },
    {
      "addr": "10.0.0.86",
      "attachto": "Ethernet344",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.87",
      "prefixlen": 31,
      "subnet": "10.0.0.86/31"
    },
    {
      "addr": "fc00::ad",
      "attachto": "Ethernet344",
      "mask": "126",
      "peer_addr": "fc00::ae",
      "prefixlen": 126,
      "subnet": "fc00::ac/126"
    },
    {
      "addr": "10.0.0.88",
      "attachto": "Ethernet352",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.89",
      "prefixlen": 31,
      "subnet": "10.0.0.88/31"
    },
    {
      "addr": "fc00::b1",
      "attachto": "Ethernet352",
      "mask": "126",
      "peer_addr": "fc00::b2",
      "prefixlen": 126,
      "subnet": "fc00::b0/126"
    },
    {
      "addr": "10.0.0.90",
      "attachto": "Ethernet360",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.91",
      "prefixlen": 31,
      "subnet": "10.0.0.90/31"
    },
    {
      "addr": "fc00::b5",
      "attachto": "Ethernet360",
      "mask": "126",
      "peer_addr": "fc00::b6",
      "prefixlen": 126,
      "subnet": "fc00::b4/126"
    },
    {
      "addr": "10.0.0.92",
      "attachto": "Ethernet368",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.93",
      "prefixlen": 31,
      "subnet": "10.0.0.92/31"
    },
    {
      "addr": "fc00::b9",
      "attachto": "Ethernet368",
      "mask": "126",
      "peer_addr": "fc00::ba",
      "prefixlen": 126,
      "subnet": "fc00::b8/126"
    },
    {
      "addr": "10.0.0.94",
      "attachto": "Ethernet376",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.95",
      "prefixlen": 31,
      "subnet": "10.0.0.94/31"
    },
    {
      "addr": "fc00::bd",
      "attachto": "Ethernet376",
      "mask": "126",
      "peer_addr": "fc00::be",
      "prefixlen": 126,
      "subnet": "fc00::bc/126"
    },
    {
      "addr": "10.0.0.96",
      "attachto": "Ethernet384",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.97",
      "prefixlen": 31,
      "subnet": "10.0.0.96/31"
    },
    {
      "addr": "fc00::c1",
      "attachto": "Ethernet384",
      "mask": "126",
      "peer_addr": "fc00::c2",
      "prefixlen": 126,
      "subnet": "fc00::c0/126"
    },
    {
      "addr": "10.0.0.98",
      "attachto": "Ethernet392",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.99",
      "prefixlen": 31,
      "subnet": "10.0.0.98/31"
    },
    {
      "addr": "fc00::c5",
      "attachto": "Ethernet392",
      "mask": "126",
      "peer_addr": "fc00::c6",
      "prefixlen": 126,
      "subnet": "fc00::c4/126"
    },
    {
      "addr": "10.0.0.10",
      "attachto": "Ethernet40",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.11",
      "prefixlen": 31,
      "subnet": "10.0.0.10/31"
    },
    {
      "addr": "fc00::15",
      "attachto": "Ethernet40",
      "mask": "126",
      "peer_addr": "fc00::16",
      "prefixlen": 126,
      "subnet": "fc00::14/126"
    },
    {
      "addr": "10.0.0.100",
      "attachto": "Ethernet400",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.101",
      "prefixlen": 31,
      "subnet": "10.0.0.100/31"
    },
    {
      "addr": "fc00::c9",
      "attachto": "Ethernet400",
      "mask": "126",
      "peer_addr": "fc00::ca",
      "prefixlen": 126,
      "subnet": "fc00::c8/126"
    },
    {
      "addr": "10.0.0.102",
      "attachto": "Ethernet408",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.103",
      "prefixlen": 31,
      "subnet": "10.0.0.102/31"
    },
    {
      "addr": "fc00::cd",
      "attachto": "Ethernet408",
      "mask": "126",
      "peer_addr": "fc00::ce",
      "prefixlen": 126,
      "subnet": "fc00::cc/126"
    },
    {
      "addr": "10.0.0.104",
      "attachto": "Ethernet416",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.105",
      "prefixlen": 31,
      "subnet": "10.0.0.104/31"
    },
    {
      "addr": "fc00::d1",
      "attachto": "Ethernet416",
      "mask": "126",
      "peer_addr": "fc00::d2",
      "prefixlen": 126,
      "subnet": "fc00::d0/126"
    },
    {
      "addr": "20.1.1.0",
      "attachto": "Ethernet420",
      "mask": "255.255.255.254",
      "peer_addr": "20.1.1.1",
      "prefixlen": 31,
      "subnet": "20.1.1.0/31"
    },
    {
      "addr": "2000:1::1",
      "attachto": "Ethernet420",
      "mask": "126",
      "peer_addr": "2000:1::2",
      "prefixlen": 126,
      "subnet": "2000:1::/126"
    },
    {
      "addr": "10.0.0.106",
      "attachto": "Ethernet424",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.107",
      "prefixlen": 31,
      "subnet": "10.0.0.106/31"
    },
    {
      "addr": "fc00::d5",
      "attachto": "Ethernet424",
      "mask": "126",
      "peer_addr": "fc00::d6",
      "prefixlen": 126,
      "subnet": "fc00::d4/126"
    },
    {
      "addr": "20.1.1.2",
      "attachto": "Ethernet428",
      "mask": "255.255.255.254",
      "peer_addr": "20.1.1.3",
      "prefixlen": 31,
      "subnet": "20.1.1.2/31"
    },
    {
      "addr": "2000:1::5",
      "attachto": "Ethernet428",
      "mask": "126",
      "peer_addr": "2000:1::6",
      "prefixlen": 126,
      "subnet": "2000:1::4/126"
    },
    {
      "addr": "10.0.0.108",
      "attachto": "Ethernet432",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.109",
      "prefixlen": 31,
      "subnet": "10.0.0.108/31"
    },
    {
      "addr": "fc00::d9",
      "attachto": "Ethernet432",
      "mask": "126",
      "peer_addr": "fc00::da",
      "prefixlen": 126,
      "subnet": "fc00::d8/126"
    },
    {
      "addr": "20.1.1.4",
      "attachto": "Ethernet436",
      "mask": "255.255.255.254",
      "peer_addr": "20.1.1.5",
      "prefixlen": 31,
      "subnet": "20.1.1.4/31"
    },
    {
      "addr": "2000:1::9",
      "attachto": "Ethernet436",
      "mask": "126",
      "peer_addr": "2000:1::a",
      "prefixlen": 126,
      "subnet": "2000:1::8/126"
    },
    {
      "addr": "10.0.0.110",
      "attachto": "Ethernet440",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.111",
      "prefixlen": 31,
      "subnet": "10.0.0.110/31"
    },
    {
      "addr": "fc00::dd",
      "attachto": "Ethernet440",
      "mask": "126",
      "peer_addr": "fc00::de",
      "prefixlen": 126,
      "subnet": "fc00::dc/126"
    },
    {
      "addr": "20.1.1.6",
      "attachto": "Ethernet444",
      "mask": "255.255.255.254",
      "peer_addr": "20.1.1.7",
      "prefixlen": 31,
      "subnet": "20.1.1.6/31"
    },
    {
      "addr": "2000:1::d",
      "attachto": "Ethernet444",
      "mask": "126",
      "peer_addr": "2000:1::e",
      "prefixlen": 126,
      "subnet": "2000:1::c/126"
    },
    {
      "addr": "10.0.0.112",
      "attachto": "Ethernet448",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.113",
      "prefixlen": 31,
      "subnet": "10.0.0.112/31"
    },
    {
      "addr": "fc00::e1",
      "attachto": "Ethernet448",
      "mask": "126",
      "peer_addr": "fc00::e2",
      "prefixlen": 126,
      "subnet": "fc00::e0/126"
    },
    {
      "addr": "10.0.0.114",
      "attachto": "Ethernet456",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.115",
      "prefixlen": 31,
      "subnet": "10.0.0.114/31"
    },
    {
      "addr": "fc00::e5",
      "attachto": "Ethernet456",
      "mask": "126",
      "peer_addr": "fc00::e6",
      "prefixlen": 126,
      "subnet": "fc00::e4/126"
    },
    {
      "addr": "10.0.0.116",
      "attachto": "Ethernet464",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.117",
      "prefixlen": 31,
      "subnet": "10.0.0.116/31"
    },
    {
      "addr": "fc00::e9",
      "attachto": "Ethernet464",
      "mask": "126",
      "peer_addr": "fc00::ea",
      "prefixlen": 126,
      "subnet": "fc00::e8/126"
    },
    {
      "addr": "10.0.0.118",
      "attachto": "Ethernet472",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.119",
      "prefixlen": 31,
      "subnet": "10.0.0.118/31"
    },
    {
      "addr": "fc00::ed",
      "attachto": "Ethernet472",
      "mask": "126",
      "peer_addr": "fc00::ee",
      "prefixlen": 126,
      "subnet": "fc00::ec/126"
    },
    {
      "addr": "10.0.0.12",
      "attachto": "Ethernet48",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.13",
      "prefixlen": 31,
      "subnet": "10.0.0.12/31"
    },
    {
      "addr": "fc00::19",
      "attachto": "Ethernet48",
      "mask": "126",
      "peer_addr": "fc00::1a",
      "prefixlen": 126,
      "subnet": "fc00::18/126"
    },
    {
      "addr": "10.0.0.120",
      "attachto": "Ethernet480",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.121",
      "prefixlen": 31,
      "subnet": "10.0.0.120/31"
    },
    {
      "addr": "fc00::f1",
      "attachto": "Ethernet480",
      "mask": "126",
      "peer_addr": "fc00::f2",
      "prefixlen": 126,
      "subnet": "fc00::f0/126"
    },
    {
      "addr": "10.0.0.122",
      "attachto": "Ethernet488",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.123",
      "prefixlen": 31,
      "subnet": "10.0.0.122/31"
    },
    {
      "addr": "fc00::f5",
      "attachto": "Ethernet488",
      "mask": "126",
      "peer_addr": "fc00::f6",
      "prefixlen": 126,
      "subnet": "fc00::f4/126"
    },
    {
      "addr": "10.0.0.124",
      "attachto": "Ethernet496",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.125",
      "prefixlen": 31,
      "subnet": "10.0.0.124/31"
    },
    {
      "addr": "fc00::f9",
      "attachto": "Ethernet496",
      "mask": "126",
      "peer_addr": "fc00::fa",
      "prefixlen": 126,
      "subnet": "fc00::f8/126"
    },
    {
      "addr": "10.0.0.126",
      "attachto": "Ethernet504",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.127",
      "prefixlen": 31,
      "subnet": "10.0.0.126/31"
    },
    {
      "addr": "fc00::fd",
      "attachto": "Ethernet504",
      "mask": "126",
      "peer_addr": "fc00::fe",
      "prefixlen": 126,
      "subnet": "fc00::fc/126"
    },
    {
      "addr": "10.0.0.14",
      "attachto": "Ethernet56",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.15",
      "prefixlen": 31,
      "subnet": "10.0.0.14/31"
    },
    {
      "addr": "fc00::1d",
      "attachto": "Ethernet56",
      "mask": "126",
      "peer_addr": "fc00::1e",
      "prefixlen": 126,
      "subnet": "fc00::1c/126"
    },
    {
      "addr": "10.0.0.16",
      "attachto": "Ethernet64",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.17",
      "prefixlen": 31,
      "subnet": "10.0.0.16/31"
    },
    {
      "addr": "fc00::21",
      "attachto": "Ethernet64",
      "mask": "126",
      "peer_addr": "fc00::22",
      "prefixlen": 126,
      "subnet": "fc00::20/126"
    },
    {
      "addr": "10.0.0.18",
      "attachto": "Ethernet72",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.19",
      "prefixlen": 31,
      "subnet": "10.0.0.18/31"
    },
    {
      "addr": "fc00::25",
      "attachto": "Ethernet72",
      "mask": "126",
      "peer_addr": "fc00::26",
      "prefixlen": 126,
      "subnet": "fc00::24/126"
    },
    {
      "addr": "10.0.0.2",
      "attachto": "Ethernet8",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.3",
      "prefixlen": 31,
      "subnet": "10.0.0.2/31"
    },
    {
      "addr": "fc00::5",
      "attachto": "Ethernet8",
      "mask": "126",
      "peer_addr": "fc00::6",
      "prefixlen": 126,
      "subnet": "fc00::4/126"
    },
    {
      "addr": "10.0.0.20",
      "attachto": "Ethernet80",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.21",
      "prefixlen": 31,
      "subnet": "10.0.0.20/31"
    },
    {
      "addr": "fc00::29",
      "attachto": "Ethernet80",
      "mask": "126",
      "peer_addr": "fc00::2a",
      "prefixlen": 126,
      "subnet": "fc00::28/126"
    },
    {
      "addr": "10.0.0.22",
      "attachto": "Ethernet88",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.23",
      "prefixlen": 31,
      "subnet": "10.0.0.22/31"
    },
    {
      "addr": "fc00::2d",
      "attachto": "Ethernet88",
      "mask": "126",
      "peer_addr": "fc00::2e",
      "prefixlen": 126,
      "subnet": "fc00::2c/126"
    },
    {
      "addr": "10.0.0.24",
      "attachto": "Ethernet96",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.25",
      "prefixlen": 31,
      "subnet": "10.0.0.24/31"
    },
    {
      "addr": "fc00::31",
      "attachto": "Ethernet96",
      "mask": "126",
      "peer_addr": "fc00::32",
      "prefixlen": 126,
      "subnet": "fc00::30/126"
    }
  ],
  "minigraph_portchannels": {},
  "minigraph_portchannel_interfaces": [],
  "minigraph_neighbors": {
    "Ethernet0": {
      "name": "ARISTA01T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet104": {
      "name": "ARISTA14T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet112": {
      "name": "ARISTA15T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet120": {
      "name": "ARISTA16T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet128": {
      "name": "ARISTA01T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet136": {
      "name": "ARISTA02T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet144": {
      "name": "ARISTA03T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet152": {
      "name": "ARISTA04T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet16": {
      "name": "ARISTA03T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet160": {
      "name": "ARISTA05T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet168": {
      "name": "ARISTA06T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet176": {
      "name": "ARISTA07T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet184": {
      "name": "ARISTA08T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet192": {
      "name": "ARISTA09T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet200": {
      "name": "ARISTA10T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet208": {
      "name": "ARISTA11T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet216": {
      "name": "ARISTA12T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet224": {
      "name": "ARISTA13T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet232": {
      "name": "ARISTA14T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet24": {
      "name": "ARISTA04T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet240": {
      "name": "ARISTA15T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet248": {
      "name": "ARISTA16T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet256": {
      "name": "ARISTA17T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet264": {
      "name": "ARISTA18T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet272": {
      "name": "ARISTA19T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet280": {
      "name": "ARISTA20T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet288": {
      "name": "ARISTA21T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet296": {
      "name": "ARISTA22T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet304": {
      "name": "ARISTA23T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet312": {
      "name": "ARISTA24T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet32": {
      "name": "ARISTA05T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet320": {
      "name": "ARISTA25T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet328": {
      "name": "ARISTA26T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet336": {
      "name": "ARISTA27T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet344": {
      "name": "ARISTA28T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet352": {
      "name": "ARISTA29T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet360": {
      "name": "ARISTA30T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet368": {
      "name": "ARISTA31T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet376": {
      "name": "ARISTA32T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet384": {
      "name": "ARISTA33T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet392": {
      "name": "ARISTA34T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet40": {
      "name": "ARISTA06T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet400": {
      "name": "ARISTA35T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet408": {
      "name": "ARISTA36T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet416": {
      "name": "ARISTA37T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet420": {
      "name": "ARISTA37T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet424": {
      "name": "ARISTA38T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet428": {
      "name": "ARISTA38T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet432": {
      "name": "ARISTA39T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet436": {
      "name": "ARISTA39T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet440": {
      "name": "ARISTA40T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet444": {
      "name": "ARISTA40T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet448": {
      "name": "ARISTA41T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet456": {
      "name": "ARISTA42T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet464": {
      "name": "ARISTA43T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet472": {
      "name": "ARISTA44T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet48": {
      "name": "ARISTA07T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet480": {
      "name": "ARISTA45T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet488": {
      "name": "ARISTA46T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet496": {
      "name": "ARISTA47T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet504": {
      "name": "ARISTA48T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet56": {
      "name": "ARISTA08T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet64": {
      "name": "ARISTA09T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet72": {
      "name": "ARISTA10T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet8": {
      "name": "ARISTA02T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet80": {
      "name": "ARISTA11T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet88": {
      "name": "ARISTA12T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet96": {
      "name": "ARISTA13T2",
      "namespace": "",
      "port": "Ethernet1"
    }
  },
  "VOQ_INBAND_INTERFACE": {},
  "BGP_VOQ_CHASSIS_NEIGHBOR": {},
  "INTERFACE": {
    "Ethernet0": {
      "10.0.0.0/31": {},
      "FC00::1/126": {}
    },
    "Ethernet104": {
      "10.0.0.26/31": {},
      "FC00::35/126": {}
    },
    "Ethernet112": {
      "10.0.0.28/31": {},
      "FC00::39/126": {}
    },
    "Ethernet120": {
      "10.0.0.30/31": {},
      "FC00::3D/126": {}
    },
    "Ethernet128": {
      "10.0.0.32/31": {},
      "FC00::41/126": {}
    },
    "Ethernet136": {
      "10.0.0.34/31": {},
      "FC00::45/126": {}
    },
    "Ethernet144": {
      "10.0.0.36/31": {},
      "FC00::49/126": {}
    },
    "Ethernet152": {
      "10.0.0.38/31": {},
      "FC00::4D/126": {}
    },
    "Ethernet16": {
      "10.0.0.4/31": {},
      "FC00::9/126": {}
    },
    "Ethernet160": {
      "10.0.0.40/31": {},
      "FC00::51/126": {}
    },
    "Ethernet168": {
      "10.0.0.42/31": {},
      "FC00::55/126": {}
    },
    "Ethernet176": {
      "10.0.0.44/31": {},
      "FC00::59/126": {}
    },
    "Ethernet184": {
      "10.0.0.46/31": {},
      "FC00::5D/126": {}
    },
    "Ethernet192": {
      "10.0.0.48/31": {},
      "FC00::61/126": {}
    },
    "Ethernet200": {
      "10.0.0.50/31": {},
      "FC00::65/126": {}
    },
    "Ethernet208": {
      "10.0.0.52/31": {},
      "FC00::69/126": {}
    },
    "Ethernet216": {
      "10.0.0.54/31": {},
      "FC00::6D/126": {}
    },
    "Ethernet224": {
      "10.0.0.56/31": {},
      "FC00::71/126": {}
    },
    "Ethernet232": {
      "10.0.0.58/31": {},
      "FC00::75/126": {}
    },
    "Ethernet24": {
      "10.0.0.6/31": {},
      "FC00::D/126": {}
    },
    "Ethernet240": {
      "10.0.0.60/31": {},
      "FC00::79/126": {}
    },
    "Ethernet248": {
      "10.0.0.62/31": {},
      "FC00::7D/126": {}
    },
    "Ethernet256": {
      "10.0.0.64/31": {},
      "FC00::81/126": {}
    },
    "Ethernet264": {
      "10.0.0.66/31": {},
      "FC00::85/126": {}
    },
    "Ethernet272": {
      "10.0.0.68/31": {},
      "FC00::89/126": {}
    },
    "Ethernet280": {
      "10.0.0.70/31": {},
      "FC00::8D/126": {}
    },
    "Ethernet288": {
      "10.0.0.72/31": {},
      "FC00::91/126": {}
    },
    "Ethernet296": {
      "10.0.0.74/31": {},
      "FC00::95/126": {}
    },
    "Ethernet304": {
      "10.0.0.76/31": {},
      "FC00::99/126": {}
    },
    "Ethernet312": {
      "10.0.0.78/31": {},
      "FC00::9D/126": {}
    },
    "Ethernet32": {
      "10.0.0.8/31": {},
      "FC00::11/126": {}
    },
    "Ethernet320": {
      "10.0.0.80/31": {},
      "FC00::A1/126": {}
    },
    "Ethernet328": {
      "10.0.0.82/31": {},
      "FC00::A5/126": {}
    },
    "Ethernet336": {
      "10.0.0.84/31": {},
      "FC00::A9/126": {}
    },
    "Ethernet344": {
      "10.0.0.86/31": {},
      "FC00::AD/126": {}
    },
    "Ethernet352": {
      "10.0.0.88/31": {},
      "FC00::B1/126": {}
    },
    "Ethernet360": {
      "10.0.0.90/31": {},
      "FC00::B5/126": {}
    },
    "Ethernet368": {
      "10.0.0.92/31": {},
      "FC00::B9/126": {}
    },
    "Ethernet376": {
      "10.0.0.94/31": {},
      "FC00::BD/126": {}
    },
    "Ethernet384": {
      "10.0.0.96/31": {},
      "FC00::C1/126": {}
    },
    "Ethernet392": {
      "10.0.0.98/31": {},
      "FC00::C5/126": {}
    },
    "Ethernet40": {
      "10.0.0.10/31": {},
      "FC00::15/126": {}
    },
    "Ethernet400": {
      "10.0.0.100/31": {},
      "FC00::C9/126": {}
    },
    "Ethernet408": {
      "10.0.0.102/31": {},
      "FC00::CD/126": {}
    },
    "Ethernet416": {
      "10.0.0.104/31": {},
      "FC00::D1/126": {}
    },
    "Ethernet420": {
      "20.1.1.0/31": {},
      "2000:1::1/126": {}
    },
    "Ethernet424": {
      "10.0.0.106/31": {},
      "FC00::D5/126": {}
    },
    "Ethernet428": {
      "20.1.1.2/31": {},
      "2000:1::5/126": {}
    },
    "Ethernet432": {
      "10.0.0.108/31": {},
      "FC00::D9/126": {}
    },
    "Ethernet436": {
      "20.1.1.4/31": {},
      "2000:1::9/126": {}
    },
    "Ethernet440": {
      "10.0.0.110/31": {},
      "FC00::DD/126": {}
    },
    "Ethernet444": {
      "20.1.1.6/31": {},
      "2000:1::d/126": {}
    },
    "Ethernet448": {
      "10.0.0.112/31": {},
      "FC00::E1/126": {}
    },
    "Ethernet456": {
      "10.0.0.114/31": {},
      "FC00::E5/126": {}
    },
    "Ethernet464": {
      "10.0.0.116/31": {},
      "FC00::E9/126": {}
    },
    "Ethernet472": {
      "10.0.0.118/31": {},
      "FC00::ED/126": {}
    },
    "Ethernet48": {
      "10.0.0.12/31": {},
      "FC00::19/126": {}
    },
    "Ethernet480": {
      "10.0.0.120/31": {},
      "FC00::F1/126": {}
    },
    "Ethernet488": {
      "10.0.0.122/31": {},
      "FC00::F5/126": {}
    },
    "Ethernet496": {
      "10.0.0.124/31": {},
      "FC00::F9/126": {}
    },
    "Ethernet504": {
      "10.0.0.126/31": {},
      "FC00::FD/126": {}
    },
    "Ethernet512": {
      "10.0.0.128/31": {}
    },
    "Ethernet513": {
      "10.0.0.130/31": {}
    },
    "Ethernet56": {
      "10.0.0.14/31": {},
      "FC00::1D/126": {}
    },
    "Ethernet64": {
      "10.0.0.16/31": {},
      "FC00::21/126": {}
    },
    "Ethernet72": {
      "10.0.0.18/31": {},
      "FC00::25/126": {}
    },
    "Ethernet8": {
      "10.0.0.2/31": {},
      "FC00::5/126": {}
    },
    "Ethernet80": {
      "10.0.0.20/31": {},
      "FC00::29/126": {}
    },
    "Ethernet88": {
      "10.0.0.22/31": {},
      "FC00::2D/126": {}
    },
    "Ethernet96": {
      "10.0.0.24/31": {},
      "FC00::31/126": {}
    }
  },
  "switch_type": "",
  "type": "LeafRouter",
  "switch": {
    "ACL_ACTION|PACKET_ACTION": "COPY,DROP,FORWARD",
    "ASIC_SDK_HEALTH_EVENT": "false",
    "ECMP_HASH_ALGORITHM": "CRC,XOR,CRC_32LO,CRC_32HI,CRC_CCITT,CRC_XOR",
    "ECMP_HASH_ALGORITHM_CAPABLE": "true",
    "ECMP_HASH_CAPABLE": "false",
    "HASH|NATIVE_HASH_FIELD_LIST": "SRC_IP,DST_IP,VLAN_ID,IP_PROTOCOL,ETHERTYPE,L4_SRC_PORT,L4_DST_PORT,SRC_MAC,DST_MAC,IN_PORT,IPV6_FLOW_LABEL",
    "ICMP_OFFLOAD_CAPABLE": "false",
    "LAG_HASH_ALGORITHM": "CRC,XOR,CRC_32LO,CRC_32HI,CRC_CCITT,CRC_XOR",
    "LAG_HASH_ALGORITHM_CAPABLE": "true",
    "LAG_HASH_CAPABLE": "false",
    "LAG_TPID_CAPABLE": "false",
    "MAX_NEXTHOP_GROUP_COUNT": "4095",
    "MIRROR": "true",
    "MIRRORV6": "true",
    "ORDERED_ECMP_CAPABLE": "true",
    "PATH_TRACING_CAPABLE": "false",
    "PFC_DLR_INIT_CAPABLE": "true",
    "PORT_EGRESS_SAMPLE_CAPABLE": "false",
    "PORT_TPID_CAPABLE": "true",
    "REG_FATAL_ASIC_SDK_HEALTH_CATEGORY": "false",
    "REG_NOTICE_ASIC_SDK_HEALTH_CATEGORY": "false",
    "REG_WARNING_ASIC_SDK_HEALTH_CATEGORY": "false",
    "SWITCH_TRIMMING_CAPABLE": "false",
    "SWITCH|NUMBER_OF_TRAFFIC_CLASSES": "8",
    "SWITCH|NUMBER_OF_UNICAST_QUEUES": "8",
    "SWITCH|PACKET_TRIMMING_DSCP_RESOLUTION_MODE": "N/A",
    "SWITCH|PACKET_TRIMMING_QUEUE_RESOLUTION_MODE": "N/A"
  },
  "macsec_en": false
}
13/11/2025 17:46:21 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_single_lossless_prio_reboot[warm] setup  ====================
13/11/2025 17:46:21 __init__.set_default                     L0053 INFO   | Completeness level not set during test execution. Setting to default level: CompletenessLevel.basic
13/11/2025 17:46:21 __init__.check_test_completeness         L0151 INFO   | Test has no defined levels. Continue without test completeness checks
13/11/2025 17:46:21 conftest.enhance_inventory               L0345 INFO   | Inventory file: ['../ansible/snappi-sonic']
13/11/2025 17:46:24 ptfhost_utils.run_icmp_responder_session L0326 INFO   | Skip running icmp_responder at session level, it is only for dualtor testbed with active-active mux ports.
13/11/2025 17:46:24 __init__._sanity_check                   L0442 INFO   | Skip sanity check according to command line argument
13/11/2025 17:46:24 conftest.collect_before_test             L2741 INFO   | Dumping Disk and Memory Space information before test on crdc-garnet-sonic-ud
13/11/2025 17:46:25 conftest.collect_before_test             L2745 INFO   | Collecting core dumps before test on crdc-garnet-sonic-ud
13/11/2025 17:46:26 conftest.collect_before_test             L2754 INFO   | Collecting running config before test on crdc-garnet-sonic-ud
13/11/2025 17:46:30 conftest.restore_golden_config_db        L3416 INFO   | [restore_golden_config_db] Restored /etc/sonic/golden_config_db.json
13/11/2025 17:46:30 conftest.temporarily_disable_route_check L3034 INFO   | Skipping temporarily_disable_route_check fixture
13/11/2025 17:46:30 conftest.run_yang_validation             L3665 INFO   | Running YANG validation on crdc-garnet-sonic-ud (pre-test)
13/11/2025 17:46:34 conftest.run_yang_validation             L3681 INFO   | YANG validation passed on crdc-garnet-sonic-ud (pre-test)
13/11/2025 17:46:34 conftest.generate_params_dut_hostname    L1544 INFO   | Using DUTs ['crdc-garnet-sonic-ud'] in testbed 'vms-snappi-sonic'
13/11/2025 17:46:34 conftest.set_rand_one_dut_hostname       L0658 INFO   | Randomly select dut crdc-garnet-sonic-ud for testing
13/11/2025 17:46:34 __init__._fixture_generator_decorator    L0081 INFO   | -------------------- fixture enable_packet_aging_after_test setup starts --------------------
13/11/2025 17:46:34 __init__._fixture_generator_decorator    L0085 INFO   | -------------------- fixture enable_packet_aging_after_test setup ends --------------------
13/11/2025 17:46:34 __init__._fixture_generator_decorator    L0081 INFO   | -------------------- fixture rand_lossless_prio setup starts --------------------
13/11/2025 17:46:34 __init__._fixture_generator_decorator    L0085 INFO   | -------------------- fixture rand_lossless_prio setup ends --------------------
13/11/2025 17:46:34 __init__._fixture_generator_decorator    L0081 INFO   | -------------------- fixture rand_lossy_prio setup starts --------------------
13/11/2025 17:46:34 __init__._fixture_generator_decorator    L0085 INFO   | -------------------- fixture rand_lossy_prio setup ends --------------------
13/11/2025 17:46:34 __init__._fixture_generator_decorator    L0081 INFO   | -------------------- fixture start_pfcwd_after_test setup starts --------------------
13/11/2025 17:46:34 __init__._fixture_generator_decorator    L0085 INFO   | -------------------- fixture start_pfcwd_after_test setup ends --------------------
13/11/2025 17:46:34 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture snappi_api_serv_ip setup starts --------------------
13/11/2025 17:46:34 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture snappi_api_serv_ip setup ends --------------------
13/11/2025 17:46:34 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture snappi_api_serv_port setup starts --------------------
13/11/2025 17:46:34 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture snappi_api_serv_port setup ends --------------------
13/11/2025 17:46:34 __init__._fixture_generator_decorator    L0081 INFO   | -------------------- fixture snappi_api setup starts --------------------
13/11/2025 17:46:34 __init__._fixture_generator_decorator    L0085 INFO   | -------------------- fixture snappi_api setup ends --------------------
13/11/2025 17:46:35 conftest.rand_one_dut_front_end_hostname L0694 INFO   | Randomly select dut crdc-garnet-sonic-ud for testing
13/11/2025 17:46:35 conftest.generate_port_lists             L1636 INFO   | Generate dut_port_map: {'crdc-garnet-sonic-ud': ['crdc-garnet-sonic-ud|Ethernet420', 'crdc-garnet-sonic-ud|Ethernet428', 'crdc-garnet-sonic-ud|Ethernet436', 'crdc-garnet-sonic-ud|Ethernet444']}
13/11/2025 17:46:35 conftest.generate_port_lists             L1659 INFO   | Generate port_list: ['crdc-garnet-sonic-ud|Ethernet420', 'crdc-garnet-sonic-ud|Ethernet428', 'crdc-garnet-sonic-ud|Ethernet436', 'crdc-garnet-sonic-ud|Ethernet444']
13/11/2025 17:46:35 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture prio_dscp_map setup starts --------------------
13/11/2025 17:46:36 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture prio_dscp_map setup ends --------------------
13/11/2025 17:46:36 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture all_prio_list setup starts --------------------
13/11/2025 17:46:36 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture all_prio_list setup ends --------------------
13/11/2025 17:46:36 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture is_pfc_enabled setup starts --------------------
13/11/2025 17:46:37 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture is_pfc_enabled setup ends --------------------
13/11/2025 17:46:37 __init__.loganalyzer                     L0077 INFO   | Log analyzer is disabled
13/11/2025 17:46:37 __init__.memory_utilization              L0143 INFO   | Hostname: crdc-garnet-sonic-ud, Hwsku: Juniper-QFX5241-64-OD, Platform: x86_64-juniper_qfx5241-r0
13/11/2025 17:46:37 memory_utilization.parse_and_register_co L0365 INFO   | Loading memory monitoring commands for hwsku: Juniper-QFX5241-64-OD
13/11/2025 17:46:37 memory_utilization.register_command      L0023 INFO   | Registering command: name=monit, cmd=sudo monit validate, memory_params={'memory_usage': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 10}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 70}}}, memory_check=<function parse_monit_validate_output at 0x7a79b47e4940>
13/11/2025 17:46:37 memory_utilization.register_command      L0023 INFO   | Registering command: name=top, cmd=top -b -n 1, memory_params={'bgpd': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}, 'zebra': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}}, memory_check=<function parse_top_output at 0x7a79b47e4160>
13/11/2025 17:46:37 memory_utilization.register_command      L0023 INFO   | Registering command: name=free, cmd=free -m, memory_params={'used': {'memory_increase_threshold': {'type': 'percentage', 'value': '20%'}, 'memory_high_threshold': None}}, memory_check=<function parse_free_output at 0x7a79b47e48b0>
13/11/2025 17:46:37 memory_utilization.register_command      L0023 INFO   | Registering command: name=docker, cmd=docker stats --no-stream, memory_params={'snmp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'pmon': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'lldp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'gnmi': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}, 'radv': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 3}}, 'syncd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 5}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 18}}, 'bgp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 4}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 14}}, 'teamd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 5}}, 'swss': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 3}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'database': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}}, memory_check=<function parse_docker_stats_output at 0x7a79b47e49d0>
13/11/2025 17:46:37 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_bgp, cmd=vtysh -c "show memory bgp", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 256}}}, memory_check=<function parse_frr_memory_output at 0x7a79b47e4a60>
13/11/2025 17:46:37 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_zebra, cmd=vtysh -c "show memory zebra", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 128}}}, memory_check=<function parse_frr_memory_output at 0x7a79b47e4a60>
13/11/2025 17:46:37 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture snappi_testbed_config setup starts --------------------
13/11/2025 17:46:38 snappi_fixtures.snappi_testbed_config    L0526 INFO   | Configuring TGEN L1: link_training=False, rs_fec=False (DUT derived)
13/11/2025 17:46:43 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture snappi_testbed_config setup ends --------------------
13/11/2025 17:46:43 conftest.setup_dualtor_mux_ports         L3494 INFO   | skip setup dualtor mux cables on non-dualtor testbed
13/11/2025 17:46:49 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 105.9 MB, holding: 99614720.0 bytes, small: 0.0 bytes, ordinary: 11401216.0 bytes
13/11/2025 17:46:50 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 39.8 MB, holding: 34603008.0 bytes, small: 0.0 bytes, ordinary: 7163904.0 bytes
13/11/2025 17:46:50 __init__.pytest_runtest_setup            L0064 INFO   | Before test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {'memory_usage': 13.9}, 'top': {'zebra': 41.9, 'bgpd': 120.7}, 'free': {'used': 4406}, 'docker': {'snmp': 0.3, 'pmon': 0.7, 'lldp': 0.2, 'gnmi': 0.5, 'bgp': 0.8, 'radv': 0.1, 'syncd': 4.3, 'teamd': 0.2, 'swss': 0.4, 'database': 0.3}, 'frr_bgp': {'used': 105.9}, 'frr_zebra': {'used': 39.8}}}, 'after_test': {'crdc-garnet-sonic-ud': {}}}
13/11/2025 17:46:50 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_single_lossless_prio_reboot[warm] call ====================
13/11/2025 17:46:50 sonic.get_asic_name                      L1860 INFO   | asic: th5
13/11/2025 17:46:50 helper.skip_warm_reboot                  L0050 INFO   | Reboot type warm is  supported on broadcom switches
13/11/2025 17:47:04 connection._warn                         L0332 WARNING| Verification of certificates is disabled
13/11/2025 17:47:04 connection._info                         L0329 INFO   | Determining the platform and rest_port using the 10.204.101.155 address...
13/11/2025 17:47:04 connection._warn                         L0332 WARNING| Unable to connect to http://10.204.101.155:8001.
13/11/2025 17:47:04 connection._info                         L0329 INFO   | Connection established to `https://10.204.101.155:8001 on linux`
13/11/2025 17:47:15 connection._info                         L0329 INFO   | Using IxNetwork api server version 10.80.2412.18
13/11/2025 17:47:15 connection._info                         L0329 INFO   | User info IxNetwork/apiserver/admin-64-894092
13/11/2025 17:47:16 snappi_api.info                          L1419 INFO   | snappi-1.27.1
13/11/2025 17:47:16 snappi_api.info                          L1419 INFO   | snappi_ixnetwork-1.27.2
13/11/2025 17:47:16 snappi_api.info                          L1419 INFO   | ixnetwork_restpy-1.6.1
13/11/2025 17:47:16 snappi_api.info                          L1419 INFO   | Config validation 0.027s
13/11/2025 17:47:18 snappi_api.info                          L1419 INFO   | Ports configuration 1.595s
13/11/2025 17:47:19 snappi_api.info                          L1419 INFO   | Captures configuration 0.171s
13/11/2025 17:47:21 snappi_api.info                          L1419 INFO   | Add location hosts [10.207.65.42] 2.328s
13/11/2025 17:47:24 snappi_api.info                          L1419 INFO   | Location hosts ready [10.207.65.42] 2.223s
13/11/2025 17:47:24 snappi_api.info                          L1419 INFO   | Speed conversion is not require for (port.name, speed) : [('Port 0', 'starTwoByFourHundredGigNonFannedOutPAM4'), ('Port 1', 'starTwoByFourHundredGigNonFannedOutPAM4'), ('Port 2', 'starTwoByFourHundredGigNonFannedOutPAM4'), ('Port 3', 'starTwoByFourHundredGigNonFannedOutPAM4')]
13/11/2025 17:47:24 snappi_api.info                          L1419 INFO   | Aggregation mode speed change 0.629s
13/11/2025 17:47:25 snappi_api.info                          L1419 INFO   | Location preemption [10.207.65.42;1;1, 10.207.65.42;1;2, 10.207.65.42;1;3, 10.207.65.42;1;4] 0.149s
13/11/2025 17:47:42 snappi_api.info                          L1419 INFO   | Location connect [Port 0, Port 1, Port 2, Port 3] 16.629s
13/11/2025 17:47:42 snappi_api.info                          L1419 INFO   | Location state check [Port 0, Port 1, Port 2, Port 3] 0.349s
13/11/2025 17:47:42 snappi_api.info                          L1419 INFO   | Location configuration 23.862s
13/11/2025 17:47:55 snappi_api.info                          L1419 INFO   | Layer1 configuration 12.907s
13/11/2025 17:47:56 snappi_api.info                          L1419 INFO   | Lag Configuration 0.117s
13/11/2025 17:47:56 snappi_api.info                          L1419 INFO   | Convert device config : 0.311s
13/11/2025 17:47:56 snappi_api.info                          L1419 INFO   | Create IxNetwork device config : 0.001s
13/11/2025 17:47:57 snappi_api.info                          L1419 INFO   | Push IxNetwork device config : 0.701s
13/11/2025 17:47:57 snappi_api.info                          L1419 INFO   | Devices configuration 1.108s
13/11/2025 17:48:06 snappi_api.info                          L1419 INFO   | Flows configuration 9.633s
13/11/2025 17:48:13 snappi_api.info                          L1419 INFO   | Start interfaces 6.279s
13/11/2025 17:48:13 snappi_api.info                          L1419 INFO   | IxNet - The Traffic Item was modified. Please perform a Traffic Generate to update the associated traffic Flow Groups
13/11/2025 17:48:13 traffic_generation.run_traffic           L0616 INFO   | Wait for Arp to Resolve ...
13/11/2025 17:48:27 traffic_generation.run_traffic           L0639 INFO   | Starting transmit on all flows ...
13/11/2025 17:48:31 snappi_api.info                          L1419 INFO   | Flows generate/apply 2.979s
13/11/2025 17:48:43 snappi_api.info                          L1419 INFO   | Flows clear statistics 12.040s
13/11/2025 17:48:43 snappi_api.info                          L1419 INFO   | Captures start 0.000s
13/11/2025 17:48:47 snappi_api.info                          L1419 INFO   | Flows start 3.809s
13/11/2025 17:48:47 traffic_generation.run_traffic           L0644 INFO   | Issuing a warm reboot on the dut crdc-garnet-sonic-ud
13/11/2025 17:48:47 parallel_utils.wrapper                   L0326 INFO   | Running reboot via synchronized decorator
13/11/2025 17:48:47 parallel_utils.wrapper                   L0335 INFO   | Running original reboot as par_followers is -1
13/11/2025 17:48:47 reboot.reboot                            L0333 INFO   | Reboot crdc-garnet-sonic-ud: wait[0.01], timeout[300]
13/11/2025 17:48:47 reboot.reboot                            L0335 INFO   | DUT crdc-garnet-sonic-ud create a file /dev/shm/test_reboot before rebooting
13/11/2025 17:48:48 reboot.reboot                            L0338 INFO   | DUT OS Version: 202505.29-dirty-20251106.112914
13/11/2025 17:48:48 dut_utils.creds_on_dut                   L0487 INFO   | dut crdc-garnet-sonic-ud belongs to groups ['snappi-sonic', 'sonic', 'sonic_juniper_qfx5241', 'fanout']
13/11/2025 17:48:48 dut_utils.creds_on_dut                   L0512 INFO   | skip empty var file /data/harshit/sonic-mgmt/tests/common/helpers/../../../ansible/group_vars/all/env.yml
13/11/2025 17:48:48 dut_utils.creds_on_dut                   L0512 INFO   | skip empty var file /data/harshit/sonic-mgmt/tests/common/helpers/../../../ansible/group_vars/all/corefile_uploader.yml
13/11/2025 17:48:48 transport._log                           L1873 INFO   | Connected (version 2.0, client OpenSSH_9.2p1)
13/11/2025 17:48:48 transport._log                           L1873 INFO   | Auth banner: b'Debian GNU/Linux 12 \\n \\l\n\n'
13/11/2025 17:48:48 transport._log                           L1873 INFO   | Authentication (password) successful!
13/11/2025 17:48:49 reboot.try_create_dut_console            L0680 WARNING| Fail to create dut console. Please check console config or if console works ro not. 'ManagementIp'
13/11/2025 17:48:49 reboot.collect_console_log               L0695 WARNING| dut console is not ready, we cannot get log by console
13/11/2025 17:48:55 reboot.wait_for_shutdown                 L0186 INFO   | waiting for ssh to drop on crdc-garnet-sonic-ud
13/11/2025 17:48:55 reboot.execute_reboot_command            L0230 INFO   | rebooting crdc-garnet-sonic-ud with command "warm-reboot"
13/11/2025 17:49:46 reboot.wait_for_startup                  L0207 INFO   | waiting for ssh to startup on crdc-garnet-sonic-ud
13/11/2025 17:49:46 reboot.ssh_connection_with_retry         L0736 INFO   | Checking ssh connection using the following params: {'host_ip': '10.207.68.120', 'port': 22, 'delay': 0, 'timeout': 300, 'search_regex': 'OpenSSH_[\\w\\.]+ Debian'}
13/11/2025 17:50:41 reboot.ssh_connection_with_retry         L0742 INFO   | Connection succeeded
13/11/2025 17:50:41 reboot.wait_for_startup                  L0222 INFO   | ssh has started up on crdc-garnet-sonic-ud
13/11/2025 17:50:41 traffic_generation.run_traffic           L0655 INFO   | Polling DUT for traffic statistics for 23 seconds ...
13/11/2025 17:52:19 traffic_generation.run_traffic           L0674 INFO   | Polling TGEN for in-flight traffic statistics...
13/11/2025 17:52:21 traffic_generation.run_traffic           L0679 INFO   | In-flight traffic statistics for flows: ['Test Flow Prio 3', 'Background Flow Prio 1', 'Background Flow Prio 4', 'Background Flow Prio 5', 'Background Flow Prio 6', 'Background Flow Prio 2', 'Background Flow Prio 0']
13/11/2025 17:52:21 traffic_generation.run_traffic           L0680 INFO   | In-flight TX frames: [7034077, 63697318, 63697318, 63697318, 63697318, 63697318, 63697318]
13/11/2025 17:52:21 traffic_generation.run_traffic           L0681 INFO   | In-flight RX frames: [67730, 63697318, 63697318, 63697318, 63697318, 63697318, 63697318]
13/11/2025 17:52:41 traffic_generation.run_traffic           L0682 INFO   | DUT polling complete
13/11/2025 17:52:41 traffic_generation.run_traffic           L0693 INFO   | Checking if all flows have stopped. Attempt #1
13/11/2025 17:52:43 traffic_generation.run_traffic           L0700 INFO   | All test and background traffic flows stopped
13/11/2025 17:52:45 traffic_generation.run_traffic           L0723 INFO   | Dumping per-flow statistics
13/11/2025 17:52:47 traffic_generation.run_traffic           L0725 INFO   | Stopping transmit on all remaining flows
13/11/2025 17:52:54 snappi_api.info                          L1419 INFO   | Flows stop 6.617s
13/11/2025 17:52:55 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_single_lossless_prio_reboot[warm] teardown ====================
13/11/2025 17:53:01 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 105.9 MB, holding: 99614720.0 bytes, small: 0.0 bytes, ordinary: 11390976.0 bytes
13/11/2025 17:53:01 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 40.3 MB, holding: 32505856.0 bytes, small: 0.0 bytes, ordinary: 9737216.0 bytes
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: monit-memory_usage
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for monit-memory_usage due to zero value
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-bgpd
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for top:bgpd: 128.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-zebra
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for top:zebra: 128.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: free-used
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for free:used: 881.2
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-snmp
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:snmp: 4.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:snmp: 2.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-pmon
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:pmon: 8.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:pmon: 2.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-lldp
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:lldp: 4.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:lldp: 2.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-gnmi
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:gnmi: 6.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:gnmi: 2.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-radv
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:radv: 3.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:radv: 2.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-syncd
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:syncd: 18.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:syncd: 5.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-bgp
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:bgp: 14.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:bgp: 4.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-teamd
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:teamd: 5.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:teamd: 2.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-swss
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:swss: 8.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:swss: 3.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-database
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:database: 6.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:database: 2.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_bgp-used
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_bgp:used: 256.0
13/11/2025 17:53:01 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [53.0, 64.0])
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_bgp:used: 64.0
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_zebra-used
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_zebra:used: 128.0
13/11/2025 17:53:01 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [19.9, 64.0])
13/11/2025 17:53:01 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_zebra:used: 64.0
13/11/2025 17:53:01 __init__.pytest_runtest_teardown         L0124 INFO   | After test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {'memory_usage': 13.9}, 'top': {'zebra': 41.9, 'bgpd': 120.7}, 'free': {'used': 4406}, 'docker': {'snmp': 0.3, 'pmon': 0.7, 'lldp': 0.2, 'gnmi': 0.5, 'bgp': 0.8, 'radv': 0.1, 'syncd': 4.3, 'teamd': 0.2, 'swss': 0.4, 'database': 0.3}, 'frr_bgp': {'used': 105.9}, 'frr_zebra': {'used': 39.8}}}, 'after_test': {'crdc-garnet-sonic-ud': {'monit': {}, 'top': {'zebra': 21.6, 'bgpd': 109.7}, 'free': {'used': 4267}, 'docker': {'snmp': 0.2, 'pmon': 0.7, 'lldp': 0.2, 'gnmi': 0.3, 'bgp': 0.7, 'radv': 0.1, 'syncd': 4.8, 'teamd': 0.1, 'swss': 0.4, 'database': 0.3}, 'frr_bgp': {'used': 105.9}, 'frr_zebra': {'used': 40.3}}}}
13/11/2025 17:53:01 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_single_lossless_prio_reboot[fast] setup  ====================
13/11/2025 17:53:01 __init__.set_default                     L0053 INFO   | Completeness level not set during test execution. Setting to default level: CompletenessLevel.basic
13/11/2025 17:53:01 __init__.check_test_completeness         L0151 INFO   | Test has no defined levels. Continue without test completeness checks
13/11/2025 17:53:02 __init__.loganalyzer                     L0077 INFO   | Log analyzer is disabled
13/11/2025 17:53:02 __init__.memory_utilization              L0143 INFO   | Hostname: crdc-garnet-sonic-ud, Hwsku: Juniper-QFX5241-64-OD, Platform: x86_64-juniper_qfx5241-r0
13/11/2025 17:53:02 memory_utilization.parse_and_register_co L0365 INFO   | Loading memory monitoring commands for hwsku: Juniper-QFX5241-64-OD
13/11/2025 17:53:02 memory_utilization.register_command      L0023 INFO   | Registering command: name=monit, cmd=sudo monit validate, memory_params={'memory_usage': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 10}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 70}}}, memory_check=<function parse_monit_validate_output at 0x7a79b47e4940>
13/11/2025 17:53:02 memory_utilization.register_command      L0023 INFO   | Registering command: name=top, cmd=top -b -n 1, memory_params={'bgpd': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}, 'zebra': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}}, memory_check=<function parse_top_output at 0x7a79b47e4160>
13/11/2025 17:53:02 memory_utilization.register_command      L0023 INFO   | Registering command: name=free, cmd=free -m, memory_params={'used': {'memory_increase_threshold': {'type': 'percentage', 'value': '20%'}, 'memory_high_threshold': None}}, memory_check=<function parse_free_output at 0x7a79b47e48b0>
13/11/2025 17:53:02 memory_utilization.register_command      L0023 INFO   | Registering command: name=docker, cmd=docker stats --no-stream, memory_params={'snmp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'pmon': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'lldp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'gnmi': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}, 'radv': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 3}}, 'syncd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 5}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 18}}, 'bgp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 4}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 14}}, 'teamd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 5}}, 'swss': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 3}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'database': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}}, memory_check=<function parse_docker_stats_output at 0x7a79b47e49d0>
13/11/2025 17:53:02 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_bgp, cmd=vtysh -c "show memory bgp", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 256}}}, memory_check=<function parse_frr_memory_output at 0x7a79b47e4a60>
13/11/2025 17:53:02 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_zebra, cmd=vtysh -c "show memory zebra", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 128}}}, memory_check=<function parse_frr_memory_output at 0x7a79b47e4a60>
13/11/2025 17:53:02 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture snappi_testbed_config setup starts --------------------
13/11/2025 17:53:03 snappi_fixtures.snappi_testbed_config    L0526 INFO   | Configuring TGEN L1: link_training=False, rs_fec=False (DUT derived)
13/11/2025 17:53:05 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture snappi_testbed_config setup ends --------------------
13/11/2025 17:53:05 conftest.setup_dualtor_mux_ports         L3494 INFO   | skip setup dualtor mux cables on non-dualtor testbed
13/11/2025 17:53:11 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 105.9 MB, holding: 99614720.0 bytes, small: 0.0 bytes, ordinary: 11390976.0 bytes
13/11/2025 17:53:12 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 40.3 MB, holding: 32505856.0 bytes, small: 0.0 bytes, ordinary: 9736192.0 bytes
13/11/2025 17:53:12 __init__.pytest_runtest_setup            L0064 INFO   | Before test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {}, 'top': {'zebra': 21.6, 'bgpd': 109.7}, 'free': {'used': 4252}, 'docker': {'snmp': 0.2, 'pmon': 0.7, 'lldp': 0.2, 'gnmi': 0.3, 'bgp': 0.7, 'radv': 0.1, 'syncd': 4.8, 'teamd': 0.1, 'swss': 0.4, 'database': 0.3}, 'frr_bgp': {'used': 105.9}, 'frr_zebra': {'used': 40.3}}}, 'after_test': {'crdc-garnet-sonic-ud': {}}}
13/11/2025 17:53:12 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_single_lossless_prio_reboot[fast] call ====================
13/11/2025 17:53:12 sonic.get_asic_name                      L1860 INFO   | asic: th5
13/11/2025 17:53:12 helper.skip_warm_reboot                  L0050 INFO   | Reboot type fast is  supported on broadcom switches
13/11/2025 17:53:26 snappi_api.info                          L1419 INFO   | Config validation 0.015s
13/11/2025 17:53:28 snappi_api.info                          L1419 INFO   | Ports configuration 0.281s
13/11/2025 17:53:28 snappi_api.info                          L1419 INFO   | Captures configuration 0.206s
13/11/2025 17:53:28 snappi_api.info                          L1419 INFO   | Location hosts ready [10.207.65.42] 0.069s
13/11/2025 17:53:28 snappi_api.info                          L1419 INFO   | Speed change not require due to redundant Layer1 config
13/11/2025 17:53:28 snappi_api.info                          L1419 INFO   | Aggregation mode speed change 0.006s
13/11/2025 17:53:29 snappi_api.info                          L1419 INFO   | Location preemption [10.207.65.42;1;1, 10.207.65.42;1;2, 10.207.65.42;1;3, 10.207.65.42;1;4] 0.132s
13/11/2025 17:53:30 snappi_api.info                          L1419 INFO   | Location connect [Port 0, Port 1, Port 2, Port 3] 0.109s
13/11/2025 17:53:30 snappi_api.info                          L1419 INFO   | Location state check [Port 0, Port 1, Port 2, Port 3] 0.305s
13/11/2025 17:53:30 snappi_api.info                          L1419 INFO   | Location configuration 2.154s
13/11/2025 17:53:39 snappi_api.info                          L1419 INFO   | Layer1 configuration 8.702s
13/11/2025 17:53:39 snappi_api.info                          L1419 INFO   | Lag Configuration 0.098s
13/11/2025 17:53:40 snappi_api.info                          L1419 INFO   | Convert device config : 0.761s
13/11/2025 17:53:40 snappi_api.info                          L1419 INFO   | Create IxNetwork device config : 0.001s
13/11/2025 17:53:40 snappi_api.info                          L1419 INFO   | Push IxNetwork device config : 0.386s
13/11/2025 17:53:40 snappi_api.info                          L1419 INFO   | Devices configuration 1.238s
13/11/2025 17:53:49 snappi_api.info                          L1419 INFO   | Flows configuration 9.293s
13/11/2025 17:53:53 snappi_api.info                          L1419 INFO   | Start interfaces 3.047s
13/11/2025 17:53:53 snappi_api.info                          L1419 INFO   | IxNet - The Traffic Item was modified. Please perform a Traffic Generate to update the associated traffic Flow Groups
13/11/2025 17:53:53 traffic_generation.run_traffic           L0616 INFO   | Wait for Arp to Resolve ...
13/11/2025 17:54:02 traffic_generation.run_traffic           L0639 INFO   | Starting transmit on all flows ...
13/11/2025 17:54:05 snappi_api.info                          L1419 INFO   | Flows generate/apply 3.000s
13/11/2025 17:54:18 snappi_api.info                          L1419 INFO   | Flows clear statistics 12.404s
13/11/2025 17:54:18 snappi_api.info                          L1419 INFO   | Captures start 0.000s
13/11/2025 17:54:22 snappi_api.info                          L1419 INFO   | Flows start 3.569s
13/11/2025 17:54:22 traffic_generation.run_traffic           L0644 INFO   | Issuing a fast reboot on the dut crdc-garnet-sonic-ud
13/11/2025 17:54:22 parallel_utils.wrapper                   L0326 INFO   | Running reboot via synchronized decorator
13/11/2025 17:54:22 parallel_utils.wrapper                   L0335 INFO   | Running original reboot as par_followers is -1
13/11/2025 17:54:22 reboot.reboot                            L0333 INFO   | Reboot crdc-garnet-sonic-ud: wait[0.01], timeout[180]
13/11/2025 17:54:22 reboot.reboot                            L0335 INFO   | DUT crdc-garnet-sonic-ud create a file /dev/shm/test_reboot before rebooting
13/11/2025 17:54:23 reboot.reboot                            L0338 INFO   | DUT OS Version: 202505.29-dirty-20251106.112914
13/11/2025 17:54:23 dut_utils.creds_on_dut                   L0487 INFO   | dut crdc-garnet-sonic-ud belongs to groups ['snappi-sonic', 'sonic', 'sonic_juniper_qfx5241', 'fanout']
13/11/2025 17:54:23 dut_utils.creds_on_dut                   L0512 INFO   | skip empty var file /data/harshit/sonic-mgmt/tests/common/helpers/../../../ansible/group_vars/all/env.yml
13/11/2025 17:54:23 dut_utils.creds_on_dut                   L0512 INFO   | skip empty var file /data/harshit/sonic-mgmt/tests/common/helpers/../../../ansible/group_vars/all/corefile_uploader.yml
13/11/2025 17:54:30 reboot.wait_for_shutdown                 L0186 INFO   | waiting for ssh to drop on crdc-garnet-sonic-ud
13/11/2025 17:54:30 reboot.execute_reboot_command            L0230 INFO   | rebooting crdc-garnet-sonic-ud with command "fast-reboot"
13/11/2025 17:54:33 utilities._paramiko_ssh                  L1338 INFO   | Cannot access device 10.207.68.120 via ssh, error: timed out
13/11/2025 17:54:33 utilities._paramiko_ssh                  L1338 INFO   | Cannot access device None via ssh, error: [Errno None] Unable to connect to port 22 on 127.0.0.1 or ::1
13/11/2025 17:54:36 reboot.wait_for_startup                  L0207 INFO   | waiting for ssh to startup on crdc-garnet-sonic-ud
13/11/2025 17:54:36 reboot.ssh_connection_with_retry         L0736 INFO   | Checking ssh connection using the following params: {'host_ip': '10.207.68.120', 'port': 22, 'delay': 0, 'timeout': 180, 'search_regex': 'OpenSSH_[\\w\\.]+ Debian'}
13/11/2025 17:54:36 reboot.ssh_connection_with_retry         L0742 INFO   | Connection succeeded
13/11/2025 17:54:36 reboot.wait_for_startup                  L0222 INFO   | ssh has started up on crdc-garnet-sonic-ud
13/11/2025 17:54:36 reboot.reboot                            L0371 WARNING| Failed to get console thread result: [Errno None] Unable to connect to port 22 on 127.0.0.1 or ::1
13/11/2025 17:54:36 traffic_generation.run_traffic           L0655 INFO   | Polling DUT for traffic statistics for 27 seconds ...
13/11/2025 17:55:13 traffic_generation.run_traffic           L0674 INFO   | Polling TGEN for in-flight traffic statistics...
13/11/2025 17:55:14 traffic_generation.run_traffic           L0679 INFO   | In-flight traffic statistics for flows: ['Test Flow Prio 3', 'Background Flow Prio 1', 'Background Flow Prio 4', 'Background Flow Prio 5', 'Background Flow Prio 6', 'Background Flow Prio 2', 'Background Flow Prio 0']
13/11/2025 17:55:14 traffic_generation.run_traffic           L0680 INFO   | In-flight TX frames: [495689655, 77107279, 77107279, 77107279, 77107279, 77107279, 77107279]
13/11/2025 17:55:14 traffic_generation.run_traffic           L0681 INFO   | In-flight RX frames: [0, 0, 0, 0, 0, 0, 0]
13/11/2025 17:56:16 __init__.pytest_runtest_call             L0040 ERROR  | Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/_pytest/python.py", line 1788, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_hooks.py", line 513, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_callers.py", line 139, in _multicall
    raise exception.with_traceback(exception.__traceback__)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_callers.py", line 103, in _multicall
    res = hook_impl.function(*args)
  File "/usr/local/lib/python3.8/dist-packages/_pytest/python.py", line 194, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File "/data/harshit/sonic-mgmt/tests/snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py", line 77, in test_pfc_pause_single_lossless_prio_reboot
    run_pfc_test(api=snappi_api,
  File "/data/harshit/sonic-mgmt/tests/snappi_tests/pfc/files/helper.py", line 255, in run_pfc_test
    tgen_flow_stats, switch_flow_stats, in_flight_flow_metrics = run_traffic(duthost=duthost,
  File "/data/harshit/sonic-mgmt/tests/common/snappi_tests/traffic_generation.py", line 667, in run_traffic
    switch_device_results["tx_frames"][lossless_prio].append(get_egress_queue_count(duthost, switch_tx_port,
  File "/data/harshit/sonic-mgmt/tests/common/snappi_tests/common_helpers.py", line 1123, in get_egress_queue_count
    raw_out = duthost.shell("show queue counters {} | sed -n '/UC{}/p'".format(port, priority))['stdout']
  File "/data/harshit/sonic-mgmt/tests/common/devices/multi_asic.py", line 151, in _run_on_asics
    return getattr(self.sonichost, self.multi_asic_attr)(*module_args, **complex_args)
  File "/data/harshit/sonic-mgmt/tests/common/devices/base.py", line 134, in _run
    raise RunAnsibleModuleFail("run module {} failed".format(self.module_name), res)
tests.common.errors.RunAnsibleModuleFail: run module shell failed, Ansible Results =>
failed = True
msg = Timeout (62s) waiting for privilege escalation prompt: 
_ansible_no_log = False
stdout =
stderr =


13/11/2025 17:56:17 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_single_lossless_prio_reboot[fast] teardown ====================
13/11/2025 17:58:02 memory_utilization.execute_command       L0042 WARNING| Error executing command 'sudo monit validate': Host unreachable in the inventory
13/11/2025 17:58:02 memory_utilization.parse_monit_validate_ L0503 WARNING| Empty output for monit validate command, returning empty values
13/11/2025 17:59:03 memory_utilization.execute_command       L0042 WARNING| Error executing command 'top -b -n 1': Host unreachable in the inventory
13/11/2025 17:59:03 memory_utilization.parse_top_output      L0443 WARNING| Empty output for top command, returning empty values
13/11/2025 18:00:03 memory_utilization.execute_command       L0042 WARNING| Error executing command 'free -m': Host unreachable in the inventory
13/11/2025 18:00:03 memory_utilization.parse_free_output     L0476 WARNING| Empty output for free command, returning empty values
13/11/2025 18:01:03 memory_utilization.execute_command       L0042 WARNING| Error executing command 'docker stats --no-stream': Host unreachable in the inventory
13/11/2025 18:01:03 memory_utilization.parse_docker_stats_ou L0535 WARNING| Empty output for docker stats command, returning empty values
13/11/2025 18:01:04 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 105.8 MB, holding: 99614720.0 bytes, small: 0.0 bytes, ordinary: 11334656.0 bytes
13/11/2025 18:01:05 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 39.6 MB, holding: 34603008.0 bytes, small: 0.0 bytes, ordinary: 6899712.0 bytes
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: monit-memory_usage
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for monit-memory_usage due to zero value
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-bgpd
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for top-bgpd due to zero value
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-zebra
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for top-zebra due to zero value
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: free-used
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for free-used due to zero value
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-snmp
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-snmp due to zero value
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-pmon
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-pmon due to zero value
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-lldp
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-lldp due to zero value
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-gnmi
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-gnmi due to zero value
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-radv
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-radv due to zero value
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-syncd
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-syncd due to zero value
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-bgp
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-bgp due to zero value
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-teamd
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-teamd due to zero value
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-swss
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-swss due to zero value
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-database
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-database due to zero value
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_bgp-used
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_bgp:used: 256.0
13/11/2025 18:01:05 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [53.0, 64.0])
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_bgp:used: 64.0
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_zebra-used
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_zebra:used: 128.0
13/11/2025 18:01:05 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [20.1, 64.0])
13/11/2025 18:01:05 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_zebra:used: 64.0
13/11/2025 18:01:05 __init__.pytest_runtest_teardown         L0124 INFO   | After test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {}, 'top': {'zebra': 21.6, 'bgpd': 109.7}, 'free': {'used': 4252}, 'docker': {'snmp': 0.2, 'pmon': 0.7, 'lldp': 0.2, 'gnmi': 0.3, 'bgp': 0.7, 'radv': 0.1, 'syncd': 4.8, 'teamd': 0.1, 'swss': 0.4, 'database': 0.3}, 'frr_bgp': {'used': 105.9}, 'frr_zebra': {'used': 40.3}}}, 'after_test': {'crdc-garnet-sonic-ud': {'monit': {}, 'top': {}, 'free': {}, 'docker': {}, 'frr_bgp': {'used': 105.8}, 'frr_zebra': {'used': 39.6}}}}
13/11/2025 18:01:05 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_multi_lossless_prio_reboot[warm] setup  ====================
13/11/2025 18:01:05 __init__.set_default                     L0053 INFO   | Completeness level not set during test execution. Setting to default level: CompletenessLevel.basic
13/11/2025 18:01:05 __init__.check_test_completeness         L0151 INFO   | Test has no defined levels. Continue without test completeness checks
13/11/2025 18:01:05 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture lossless_prio_list setup starts --------------------
13/11/2025 18:01:06 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture lossless_prio_list setup ends --------------------
13/11/2025 18:01:06 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture lossy_prio_list setup starts --------------------
13/11/2025 18:01:06 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture lossy_prio_list setup ends --------------------
13/11/2025 18:01:06 __init__.loganalyzer                     L0077 INFO   | Log analyzer is disabled
13/11/2025 18:01:06 __init__.memory_utilization              L0143 INFO   | Hostname: crdc-garnet-sonic-ud, Hwsku: Juniper-QFX5241-64-OD, Platform: x86_64-juniper_qfx5241-r0
13/11/2025 18:01:06 memory_utilization.parse_and_register_co L0365 INFO   | Loading memory monitoring commands for hwsku: Juniper-QFX5241-64-OD
13/11/2025 18:01:06 memory_utilization.register_command      L0023 INFO   | Registering command: name=monit, cmd=sudo monit validate, memory_params={'memory_usage': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 10}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 70}}}, memory_check=<function parse_monit_validate_output at 0x7a79b47e4940>
13/11/2025 18:01:06 memory_utilization.register_command      L0023 INFO   | Registering command: name=top, cmd=top -b -n 1, memory_params={'bgpd': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}, 'zebra': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}}, memory_check=<function parse_top_output at 0x7a79b47e4160>
13/11/2025 18:01:06 memory_utilization.register_command      L0023 INFO   | Registering command: name=free, cmd=free -m, memory_params={'used': {'memory_increase_threshold': {'type': 'percentage', 'value': '20%'}, 'memory_high_threshold': None}}, memory_check=<function parse_free_output at 0x7a79b47e48b0>
13/11/2025 18:01:06 memory_utilization.register_command      L0023 INFO   | Registering command: name=docker, cmd=docker stats --no-stream, memory_params={'snmp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'pmon': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'lldp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'gnmi': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}, 'radv': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 3}}, 'syncd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 5}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 18}}, 'bgp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 4}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 14}}, 'teamd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 5}}, 'swss': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 3}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'database': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}}, memory_check=<function parse_docker_stats_output at 0x7a79b47e49d0>
13/11/2025 18:01:06 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_bgp, cmd=vtysh -c "show memory bgp", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 256}}}, memory_check=<function parse_frr_memory_output at 0x7a79b47e4a60>
13/11/2025 18:01:06 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_zebra, cmd=vtysh -c "show memory zebra", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 128}}}, memory_check=<function parse_frr_memory_output at 0x7a79b47e4a60>
13/11/2025 18:01:06 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture snappi_testbed_config setup starts --------------------
13/11/2025 18:01:07 snappi_fixtures.snappi_testbed_config    L0526 INFO   | Configuring TGEN L1: link_training=False, rs_fec=False (DUT derived)
13/11/2025 18:01:08 multi_asic.get_dut_iface_mac             L0185 ERROR  | Failed to get MAC address for interface "Ethernet420", exception: run module command failed, Ansible Results =>
failed = True
changed = True
rc = 1
cmd = ['cat', '/sys/class/net/Ethernet420/address']
start = 2025-11-13 18:01:00.414924
end = 2025-11-13 18:01:00.420418
delta = 0:00:00.005494
msg = non-zero return code
invocation = {'module_args': {'_raw_params': '  cat /sys/class/net/Ethernet420/address', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}
_ansible_no_log = None
stdout =
stderr =
cat: /sys/class/net/Ethernet420/address: No such file or directory
13/11/2025 18:01:09 multi_asic.get_dut_iface_mac             L0185 ERROR  | Failed to get MAC address for interface "Ethernet428", exception: run module command failed, Ansible Results =>
failed = True
changed = True
rc = 1
cmd = ['cat', '/sys/class/net/Ethernet428/address']
start = 2025-11-13 18:01:00.838720
end = 2025-11-13 18:01:00.844280
delta = 0:00:00.005560
msg = non-zero return code
invocation = {'module_args': {'_raw_params': '  cat /sys/class/net/Ethernet428/address', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}
_ansible_no_log = None
stdout =
stderr =
cat: /sys/class/net/Ethernet428/address: No such file or directory
13/11/2025 18:01:09 multi_asic.get_dut_iface_mac             L0185 ERROR  | Failed to get MAC address for interface "Ethernet436", exception: run module command failed, Ansible Results =>
failed = True
changed = True
rc = 1
cmd = ['cat', '/sys/class/net/Ethernet436/address']
start = 2025-11-13 18:01:01.276054
end = 2025-11-13 18:01:01.281620
delta = 0:00:00.005566
msg = non-zero return code
invocation = {'module_args': {'_raw_params': '  cat /sys/class/net/Ethernet436/address', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}
_ansible_no_log = None
stdout =
stderr =
cat: /sys/class/net/Ethernet436/address: No such file or directory
13/11/2025 18:01:10 multi_asic.get_dut_iface_mac             L0185 ERROR  | Failed to get MAC address for interface "Ethernet444", exception: run module command failed, Ansible Results =>
failed = True
changed = True
rc = 1
cmd = ['cat', '/sys/class/net/Ethernet444/address']
start = 2025-11-13 18:01:01.821977
end = 2025-11-13 18:01:01.827498
delta = 0:00:00.005521
msg = non-zero return code
invocation = {'module_args': {'_raw_params': '  cat /sys/class/net/Ethernet444/address', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}
_ansible_no_log = None
stdout =
stderr =
cat: /sys/class/net/Ethernet444/address: No such file or directory
13/11/2025 18:01:10 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture snappi_testbed_config setup ends --------------------
13/11/2025 18:01:10 conftest.setup_dualtor_mux_ports         L3494 INFO   | skip setup dualtor mux cables on non-dualtor testbed
13/11/2025 18:01:15 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 105.8 MB, holding: 99614720.0 bytes, small: 0.0 bytes, ordinary: 11334656.0 bytes
13/11/2025 18:01:16 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 39.6 MB, holding: 34603008.0 bytes, small: 0.0 bytes, ordinary: 6899712.0 bytes
13/11/2025 18:01:16 __init__.pytest_runtest_setup            L0064 INFO   | Before test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {'memory_usage': 12.4}, 'top': {'zebra': 21.0, 'bgpd': 109.7}, 'free': {'used': 3906}, 'docker': {'snmp': 0.2, 'pmon': 0.7, 'lldp': 0.2, 'gnmi': 0.3, 'bgp': 0.7, 'radv': 0.1, 'syncd': 4.1, 'teamd': 0.1, 'swss': 0.3, 'database': 0.3}, 'frr_bgp': {'used': 105.8}, 'frr_zebra': {'used': 39.6}}}, 'after_test': {'crdc-garnet-sonic-ud': {}}}
13/11/2025 18:01:16 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_multi_lossless_prio_reboot[warm] call ====================
13/11/2025 18:01:17 sonic.get_asic_name                      L1860 INFO   | asic: th5
13/11/2025 18:01:17 helper.skip_warm_reboot                  L0050 INFO   | Reboot type warm is  supported on broadcom switches
13/11/2025 18:01:29 snappi_api.info                          L1419 INFO   | Config validation 0.016s
13/11/2025 18:01:31 snappi_api.info                          L1419 INFO   | Ports configuration 0.302s
13/11/2025 18:01:31 snappi_api.info                          L1419 INFO   | Captures configuration 0.203s
13/11/2025 18:01:32 snappi_api.info                          L1419 INFO   | Location hosts ready [10.207.65.42] 0.102s
13/11/2025 18:01:32 snappi_api.info                          L1419 INFO   | Speed change not require due to redundant Layer1 config
13/11/2025 18:01:32 snappi_api.info                          L1419 INFO   | Aggregation mode speed change 0.021s
13/11/2025 18:01:33 snappi_api.info                          L1419 INFO   | Location preemption [10.207.65.42;1;1, 10.207.65.42;1;2, 10.207.65.42;1;3, 10.207.65.42;1;4] 0.119s
13/11/2025 18:01:33 snappi_api.info                          L1419 INFO   | Location connect [Port 0, Port 1, Port 2, Port 3] 0.096s
13/11/2025 18:01:33 snappi_api.info                          L1419 INFO   | Location state check [Port 0, Port 1, Port 2, Port 3] 0.281s
13/11/2025 18:01:33 snappi_api.info                          L1419 INFO   | Location configuration 2.084s
13/11/2025 18:01:42 snappi_api.info                          L1419 INFO   | Layer1 configuration 8.526s
13/11/2025 18:01:42 snappi_api.info                          L1419 INFO   | Lag Configuration 0.098s
13/11/2025 18:01:43 snappi_api.info                          L1419 INFO   | Convert device config : 0.799s
13/11/2025 18:01:43 snappi_api.info                          L1419 INFO   | Create IxNetwork device config : 0.001s
13/11/2025 18:01:43 snappi_api.info                          L1419 INFO   | Push IxNetwork device config : 0.374s
13/11/2025 18:01:43 snappi_api.info                          L1419 INFO   | Devices configuration 1.273s
13/11/2025 18:01:53 snappi_api.info                          L1419 INFO   | Flows configuration 9.758s
13/11/2025 18:01:56 snappi_api.info                          L1419 INFO   | Start interfaces 3.069s
13/11/2025 18:01:57 snappi_api.info                          L1419 INFO   | IxNet - The Traffic Item was modified. Please perform a Traffic Generate to update the associated traffic Flow Groups
13/11/2025 18:01:57 traffic_generation.run_traffic           L0616 INFO   | Wait for Arp to Resolve ...
13/11/2025 18:03:13 traffic_generation.run_traffic           L0639 INFO   | Starting transmit on all flows ...
13/11/2025 18:03:16 snappi_api.info                          L1419 INFO   | Flows generate/apply 2.271s
13/11/2025 18:03:28 snappi_api.info                          L1419 INFO   | Flows clear statistics 12.287s
13/11/2025 18:03:28 snappi_api.info                          L1419 INFO   | Captures start 0.000s
13/11/2025 18:03:32 snappi_api.info                          L1419 INFO   | Flows start 3.588s
13/11/2025 18:03:33 traffic_generation.run_traffic           L0644 INFO   | Issuing a warm reboot on the dut crdc-garnet-sonic-ud
13/11/2025 18:03:33 parallel_utils.wrapper                   L0326 INFO   | Running reboot via synchronized decorator
13/11/2025 18:03:33 parallel_utils.wrapper                   L0335 INFO   | Running original reboot as par_followers is -1
13/11/2025 18:03:33 reboot.reboot                            L0333 INFO   | Reboot crdc-garnet-sonic-ud: wait[0.01], timeout[300]
13/11/2025 18:03:33 reboot.reboot                            L0335 INFO   | DUT crdc-garnet-sonic-ud create a file /dev/shm/test_reboot before rebooting
13/11/2025 18:03:33 reboot.reboot                            L0338 INFO   | DUT OS Version: 202505.29-dirty-20251106.112914
13/11/2025 18:03:33 dut_utils.creds_on_dut                   L0487 INFO   | dut crdc-garnet-sonic-ud belongs to groups ['snappi-sonic', 'sonic', 'sonic_juniper_qfx5241', 'fanout']
13/11/2025 18:03:33 dut_utils.creds_on_dut                   L0512 INFO   | skip empty var file /data/harshit/sonic-mgmt/tests/common/helpers/../../../ansible/group_vars/all/env.yml
13/11/2025 18:03:33 dut_utils.creds_on_dut                   L0512 INFO   | skip empty var file /data/harshit/sonic-mgmt/tests/common/helpers/../../../ansible/group_vars/all/corefile_uploader.yml
13/11/2025 18:03:34 transport._log                           L1873 INFO   | Connected (version 2.0, client OpenSSH_9.2p1)
13/11/2025 18:03:34 transport._log                           L1873 INFO   | Auth banner: b'Debian GNU/Linux 12 \\n \\l\n\n'
13/11/2025 18:03:34 transport._log                           L1873 INFO   | Authentication (password) successful!
13/11/2025 18:03:34 reboot.try_create_dut_console            L0680 WARNING| Fail to create dut console. Please check console config or if console works ro not. 'ManagementIp'
13/11/2025 18:03:34 reboot.collect_console_log               L0695 WARNING| dut console is not ready, we cannot get log by console
13/11/2025 18:03:40 reboot.wait_for_shutdown                 L0186 INFO   | waiting for ssh to drop on crdc-garnet-sonic-ud
13/11/2025 18:03:40 reboot.execute_reboot_command            L0230 INFO   | rebooting crdc-garnet-sonic-ud with command "warm-reboot"
13/11/2025 18:04:31 reboot.wait_for_startup                  L0207 INFO   | waiting for ssh to startup on crdc-garnet-sonic-ud
13/11/2025 18:04:31 reboot.ssh_connection_with_retry         L0736 INFO   | Checking ssh connection using the following params: {'host_ip': '10.207.68.120', 'port': 22, 'delay': 0, 'timeout': 300, 'search_regex': 'OpenSSH_[\\w\\.]+ Debian'}
13/11/2025 18:04:56 reboot.ssh_connection_with_retry         L0742 INFO   | Connection succeeded
13/11/2025 18:04:56 reboot.wait_for_startup                  L0222 INFO   | ssh has started up on crdc-garnet-sonic-ud
13/11/2025 18:04:56 traffic_generation.run_traffic           L0655 INFO   | Polling DUT for traffic statistics for 31 seconds ...
13/11/2025 18:05:58 __init__.pytest_runtest_call             L0040 ERROR  | Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/_pytest/python.py", line 1788, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_hooks.py", line 513, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_callers.py", line 139, in _multicall
    raise exception.with_traceback(exception.__traceback__)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_callers.py", line 103, in _multicall
    res = hook_impl.function(*args)
  File "/usr/local/lib/python3.8/dist-packages/_pytest/python.py", line 194, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File "/data/harshit/sonic-mgmt/tests/snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py", line 149, in test_pfc_pause_multi_lossless_prio_reboot
    run_pfc_test(api=snappi_api,
  File "/data/harshit/sonic-mgmt/tests/snappi_tests/pfc/files/helper.py", line 255, in run_pfc_test
    tgen_flow_stats, switch_flow_stats, in_flight_flow_metrics = run_traffic(duthost=duthost,
  File "/data/harshit/sonic-mgmt/tests/common/snappi_tests/traffic_generation.py", line 667, in run_traffic
    switch_device_results["tx_frames"][lossless_prio].append(get_egress_queue_count(duthost, switch_tx_port,
  File "/data/harshit/sonic-mgmt/tests/common/snappi_tests/common_helpers.py", line 1123, in get_egress_queue_count
    raw_out = duthost.shell("show queue counters {} | sed -n '/UC{}/p'".format(port, priority))['stdout']
  File "/data/harshit/sonic-mgmt/tests/common/devices/multi_asic.py", line 151, in _run_on_asics
    return getattr(self.sonichost, self.multi_asic_attr)(*module_args, **complex_args)
  File "/data/harshit/sonic-mgmt/tests/common/devices/base.py", line 134, in _run
    raise RunAnsibleModuleFail("run module {} failed".format(self.module_name), res)
tests.common.errors.RunAnsibleModuleFail: run module shell failed, Ansible Results =>
failed = True
msg = Timeout (62s) waiting for privilege escalation prompt: 
_ansible_no_log = False
stdout =
stderr =


13/11/2025 18:05:59 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_multi_lossless_prio_reboot[warm] teardown ====================
13/11/2025 18:07:27 memory_utilization.execute_command       L0042 WARNING| Error executing command 'sudo monit validate': Host unreachable in the inventory
13/11/2025 18:07:27 memory_utilization.parse_monit_validate_ L0503 WARNING| Empty output for monit validate command, returning empty values
13/11/2025 18:08:28 memory_utilization.execute_command       L0042 WARNING| Error executing command 'top -b -n 1': Host unreachable in the inventory
13/11/2025 18:08:28 memory_utilization.parse_top_output      L0443 WARNING| Empty output for top command, returning empty values
13/11/2025 18:08:33 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 105.9 MB, holding: 99614720.0 bytes, small: 0.0 bytes, ordinary: 11392000.0 bytes
13/11/2025 18:08:33 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 39.8 MB, holding: 34603008.0 bytes, small: 0.0 bytes, ordinary: 7169024.0 bytes
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: monit-memory_usage
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for monit-memory_usage due to zero value
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-bgpd
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for top-bgpd due to zero value
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-zebra
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for top-zebra due to zero value
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: free-used
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for free:used: 781.2
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-snmp
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:snmp: 4.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:snmp: 2.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-pmon
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:pmon: 8.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:pmon: 2.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-lldp
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:lldp: 4.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:lldp: 2.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-gnmi
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:gnmi: 6.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:gnmi: 2.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-radv
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:radv: 3.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:radv: 2.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-syncd
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:syncd: 18.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:syncd: 5.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-bgp
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:bgp: 14.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:bgp: 4.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-teamd
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:teamd: 5.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:teamd: 2.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-swss
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:swss: 8.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:swss: 3.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-database
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:database: 6.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:database: 2.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_bgp-used
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_bgp:used: 256.0
13/11/2025 18:08:33 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [52.9, 64.0])
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_bgp:used: 64.0
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_zebra-used
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_zebra:used: 128.0
13/11/2025 18:08:33 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [19.8, 64.0])
13/11/2025 18:08:33 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_zebra:used: 64.0
13/11/2025 18:08:33 __init__.pytest_runtest_teardown         L0124 INFO   | After test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {'memory_usage': 12.4}, 'top': {'zebra': 21.0, 'bgpd': 109.7}, 'free': {'used': 3906}, 'docker': {'snmp': 0.2, 'pmon': 0.7, 'lldp': 0.2, 'gnmi': 0.3, 'bgp': 0.7, 'radv': 0.1, 'syncd': 4.1, 'teamd': 0.1, 'swss': 0.3, 'database': 0.3}, 'frr_bgp': {'used': 105.8}, 'frr_zebra': {'used': 39.6}}}, 'after_test': {'crdc-garnet-sonic-ud': {'monit': {}, 'top': {}, 'free': {'used': 4273}, 'docker': {'snmp': 0.2, 'pmon': 0.7, 'lldp': 0.2, 'gnmi': 0.3, 'bgp': 0.7, 'radv': 0.1, 'syncd': 4.8, 'teamd': 0.1, 'swss': 0.4, 'database': 0.3}, 'frr_bgp': {'used': 105.9}, 'frr_zebra': {'used': 39.8}}}}
13/11/2025 18:08:33 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_multi_lossless_prio_reboot[fast] setup  ====================
13/11/2025 18:08:33 __init__.set_default                     L0053 INFO   | Completeness level not set during test execution. Setting to default level: CompletenessLevel.basic
13/11/2025 18:08:33 __init__.check_test_completeness         L0151 INFO   | Test has no defined levels. Continue without test completeness checks
13/11/2025 18:08:33 __init__.loganalyzer                     L0077 INFO   | Log analyzer is disabled
13/11/2025 18:08:33 __init__.memory_utilization              L0143 INFO   | Hostname: crdc-garnet-sonic-ud, Hwsku: Juniper-QFX5241-64-OD, Platform: x86_64-juniper_qfx5241-r0
13/11/2025 18:08:33 memory_utilization.parse_and_register_co L0365 INFO   | Loading memory monitoring commands for hwsku: Juniper-QFX5241-64-OD
13/11/2025 18:08:33 memory_utilization.register_command      L0023 INFO   | Registering command: name=monit, cmd=sudo monit validate, memory_params={'memory_usage': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 10}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 70}}}, memory_check=<function parse_monit_validate_output at 0x7a79b47e4940>
13/11/2025 18:08:33 memory_utilization.register_command      L0023 INFO   | Registering command: name=top, cmd=top -b -n 1, memory_params={'bgpd': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}, 'zebra': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}}, memory_check=<function parse_top_output at 0x7a79b47e4160>
13/11/2025 18:08:33 memory_utilization.register_command      L0023 INFO   | Registering command: name=free, cmd=free -m, memory_params={'used': {'memory_increase_threshold': {'type': 'percentage', 'value': '20%'}, 'memory_high_threshold': None}}, memory_check=<function parse_free_output at 0x7a79b47e48b0>
13/11/2025 18:08:33 memory_utilization.register_command      L0023 INFO   | Registering command: name=docker, cmd=docker stats --no-stream, memory_params={'snmp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'pmon': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'lldp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'gnmi': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}, 'radv': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 3}}, 'syncd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 5}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 18}}, 'bgp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 4}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 14}}, 'teamd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 5}}, 'swss': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 3}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'database': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}}, memory_check=<function parse_docker_stats_output at 0x7a79b47e49d0>
13/11/2025 18:08:33 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_bgp, cmd=vtysh -c "show memory bgp", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 256}}}, memory_check=<function parse_frr_memory_output at 0x7a79b47e4a60>
13/11/2025 18:08:33 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_zebra, cmd=vtysh -c "show memory zebra", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 128}}}, memory_check=<function parse_frr_memory_output at 0x7a79b47e4a60>
13/11/2025 18:08:33 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture snappi_testbed_config setup starts --------------------
13/11/2025 18:08:35 snappi_fixtures.snappi_testbed_config    L0526 INFO   | Configuring TGEN L1: link_training=False, rs_fec=False (DUT derived)
13/11/2025 18:08:37 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture snappi_testbed_config setup ends --------------------
13/11/2025 18:08:37 conftest.setup_dualtor_mux_ports         L3494 INFO   | skip setup dualtor mux cables on non-dualtor testbed
13/11/2025 18:08:43 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 105.9 MB, holding: 99614720.0 bytes, small: 0.0 bytes, ordinary: 11392000.0 bytes
13/11/2025 18:08:44 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 39.8 MB, holding: 34603008.0 bytes, small: 0.0 bytes, ordinary: 7169024.0 bytes
13/11/2025 18:08:44 __init__.pytest_runtest_setup            L0064 INFO   | Before test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {}, 'top': {'zebra': 21.2, 'bgpd': 109.7}, 'free': {'used': 4253}, 'docker': {'snmp': 0.2, 'pmon': 0.7, 'lldp': 0.2, 'gnmi': 0.3, 'bgp': 0.7, 'radv': 0.1, 'syncd': 4.8, 'teamd': 0.1, 'swss': 0.4, 'database': 0.3}, 'frr_bgp': {'used': 105.9}, 'frr_zebra': {'used': 39.8}}}, 'after_test': {'crdc-garnet-sonic-ud': {}}}
13/11/2025 18:08:44 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_multi_lossless_prio_reboot[fast] call ====================
13/11/2025 18:08:44 sonic.get_asic_name                      L1860 INFO   | asic: th5
13/11/2025 18:08:44 helper.skip_warm_reboot                  L0050 INFO   | Reboot type fast is  supported on broadcom switches
13/11/2025 18:08:58 snappi_api.info                          L1419 INFO   | Config validation 0.016s
13/11/2025 18:09:00 snappi_api.info                          L1419 INFO   | Ports configuration 0.342s
13/11/2025 18:09:00 snappi_api.info                          L1419 INFO   | Captures configuration 0.218s
13/11/2025 18:09:01 snappi_api.info                          L1419 INFO   | Location hosts ready [10.207.65.42] 0.116s
13/11/2025 18:09:01 snappi_api.info                          L1419 INFO   | Speed change not require due to redundant Layer1 config
13/11/2025 18:09:01 snappi_api.info                          L1419 INFO   | Aggregation mode speed change 0.020s
13/11/2025 18:09:02 snappi_api.info                          L1419 INFO   | Location preemption [10.207.65.42;1;1, 10.207.65.42;1;2, 10.207.65.42;1;3, 10.207.65.42;1;4] 0.129s
13/11/2025 18:09:02 snappi_api.info                          L1419 INFO   | Location connect [Port 0, Port 1, Port 2, Port 3] 0.109s
13/11/2025 18:09:02 snappi_api.info                          L1419 INFO   | Location state check [Port 0, Port 1, Port 2, Port 3] 0.268s
13/11/2025 18:09:02 snappi_api.info                          L1419 INFO   | Location configuration 2.102s
13/11/2025 18:09:11 snappi_api.info                          L1419 INFO   | Layer1 configuration 8.503s
13/11/2025 18:09:11 snappi_api.info                          L1419 INFO   | Lag Configuration 0.095s
13/11/2025 18:09:12 snappi_api.info                          L1419 INFO   | Convert device config : 0.694s
13/11/2025 18:09:12 snappi_api.info                          L1419 INFO   | Create IxNetwork device config : 0.001s
13/11/2025 18:09:12 snappi_api.info                          L1419 INFO   | Push IxNetwork device config : 0.374s
13/11/2025 18:09:12 snappi_api.info                          L1419 INFO   | Devices configuration 1.165s
13/11/2025 18:09:22 snappi_api.info                          L1419 INFO   | Flows configuration 9.925s
13/11/2025 18:09:25 snappi_api.info                          L1419 INFO   | Start interfaces 3.044s
13/11/2025 18:09:26 snappi_api.info                          L1419 INFO   | IxNet - The Traffic Item was modified. Please perform a Traffic Generate to update the associated traffic Flow Groups
13/11/2025 18:09:26 traffic_generation.run_traffic           L0616 INFO   | Wait for Arp to Resolve ...
13/11/2025 18:09:35 traffic_generation.run_traffic           L0639 INFO   | Starting transmit on all flows ...
13/11/2025 18:09:37 snappi_api.info                          L1419 INFO   | Flows generate/apply 1.758s
13/11/2025 18:09:49 snappi_api.info                          L1419 INFO   | Flows clear statistics 12.292s
13/11/2025 18:09:49 snappi_api.info                          L1419 INFO   | Captures start 0.000s
13/11/2025 18:09:54 snappi_api.info                          L1419 INFO   | Flows start 3.967s
13/11/2025 18:09:54 traffic_generation.run_traffic           L0644 INFO   | Issuing a fast reboot on the dut crdc-garnet-sonic-ud
13/11/2025 18:09:54 parallel_utils.wrapper                   L0326 INFO   | Running reboot via synchronized decorator
13/11/2025 18:09:54 parallel_utils.wrapper                   L0335 INFO   | Running original reboot as par_followers is -1
13/11/2025 18:09:54 reboot.reboot                            L0333 INFO   | Reboot crdc-garnet-sonic-ud: wait[0.01], timeout[180]
13/11/2025 18:09:54 reboot.reboot                            L0335 INFO   | DUT crdc-garnet-sonic-ud create a file /dev/shm/test_reboot before rebooting
13/11/2025 18:09:55 reboot.reboot                            L0338 INFO   | DUT OS Version: 202505.29-dirty-20251106.112914
13/11/2025 18:09:55 dut_utils.creds_on_dut                   L0487 INFO   | dut crdc-garnet-sonic-ud belongs to groups ['snappi-sonic', 'sonic', 'sonic_juniper_qfx5241', 'fanout']
13/11/2025 18:09:55 dut_utils.creds_on_dut                   L0512 INFO   | skip empty var file /data/harshit/sonic-mgmt/tests/common/helpers/../../../ansible/group_vars/all/env.yml
13/11/2025 18:09:55 dut_utils.creds_on_dut                   L0512 INFO   | skip empty var file /data/harshit/sonic-mgmt/tests/common/helpers/../../../ansible/group_vars/all/corefile_uploader.yml
13/11/2025 18:09:55 transport._log                           L1873 INFO   | Connected (version 2.0, client OpenSSH_9.2p1)
13/11/2025 18:09:55 transport._log                           L1873 INFO   | Auth banner: b'Debian GNU/Linux 12 \\n \\l\n\n'
13/11/2025 18:09:55 transport._log                           L1873 INFO   | Authentication (password) successful!
13/11/2025 18:09:55 reboot.try_create_dut_console            L0680 WARNING| Fail to create dut console. Please check console config or if console works ro not. 'ManagementIp'
13/11/2025 18:09:55 reboot.collect_console_log               L0695 WARNING| dut console is not ready, we cannot get log by console
13/11/2025 18:10:01 reboot.wait_for_shutdown                 L0186 INFO   | waiting for ssh to drop on crdc-garnet-sonic-ud
13/11/2025 18:10:01 reboot.execute_reboot_command            L0230 INFO   | rebooting crdc-garnet-sonic-ud with command "fast-reboot"
13/11/2025 18:10:07 reboot.wait_for_startup                  L0207 INFO   | waiting for ssh to startup on crdc-garnet-sonic-ud
13/11/2025 18:10:07 reboot.ssh_connection_with_retry         L0736 INFO   | Checking ssh connection using the following params: {'host_ip': '10.207.68.120', 'port': 22, 'delay': 0, 'timeout': 180, 'search_regex': 'OpenSSH_[\\w\\.]+ Debian'}
13/11/2025 18:10:13 reboot.ssh_connection_with_retry         L0742 INFO   | Connection succeeded
13/11/2025 18:10:13 reboot.wait_for_startup                  L0222 INFO   | ssh has started up on crdc-garnet-sonic-ud
13/11/2025 18:10:13 traffic_generation.run_traffic           L0655 INFO   | Polling DUT for traffic statistics for 35 seconds ...
13/11/2025 18:11:51 __init__.pytest_runtest_call             L0040 ERROR  | Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/_pytest/python.py", line 1788, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_hooks.py", line 513, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_callers.py", line 139, in _multicall
    raise exception.with_traceback(exception.__traceback__)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_callers.py", line 103, in _multicall
    res = hook_impl.function(*args)
  File "/usr/local/lib/python3.8/dist-packages/_pytest/python.py", line 194, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File "/data/harshit/sonic-mgmt/tests/snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py", line 149, in test_pfc_pause_multi_lossless_prio_reboot
    run_pfc_test(api=snappi_api,
  File "/data/harshit/sonic-mgmt/tests/snappi_tests/pfc/files/helper.py", line 255, in run_pfc_test
    tgen_flow_stats, switch_flow_stats, in_flight_flow_metrics = run_traffic(duthost=duthost,
  File "/data/harshit/sonic-mgmt/tests/common/snappi_tests/traffic_generation.py", line 667, in run_traffic
    switch_device_results["tx_frames"][lossless_prio].append(get_egress_queue_count(duthost, switch_tx_port,
  File "/data/harshit/sonic-mgmt/tests/common/snappi_tests/common_helpers.py", line 1123, in get_egress_queue_count
    raw_out = duthost.shell("show queue counters {} | sed -n '/UC{}/p'".format(port, priority))['stdout']
  File "/data/harshit/sonic-mgmt/tests/common/devices/multi_asic.py", line 151, in _run_on_asics
    return getattr(self.sonichost, self.multi_asic_attr)(*module_args, **complex_args)
  File "/data/harshit/sonic-mgmt/tests/common/devices/base.py", line 134, in _run
    raise RunAnsibleModuleFail("run module {} failed".format(self.module_name), res)
tests.common.errors.RunAnsibleModuleFail: run module shell failed, Ansible Results =>
failed = True
msg = Timeout (62s) waiting for privilege escalation prompt: 
_ansible_no_log = False
stdout =
stderr =


13/11/2025 18:11:51 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_multi_lossless_prio_reboot[fast] teardown ====================
13/11/2025 18:12:49 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 105.8 MB, holding: 99614720.0 bytes, small: 0.0 bytes, ordinary: 11329536.0 bytes
13/11/2025 18:12:50 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 39.6 MB, holding: 34603008.0 bytes, small: 0.0 bytes, ordinary: 6899712.0 bytes
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: monit-memory_usage
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for monit-memory_usage due to zero value
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-bgpd
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for top:bgpd: 128.0
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-zebra
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for top:zebra: 128.0
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: free-used
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for free:used: 850.6
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-snmp
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-snmp due to zero value
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-pmon
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-pmon due to zero value
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-lldp
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-lldp due to zero value
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-gnmi
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-gnmi due to zero value
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-radv
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:radv: 3.0
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:radv: 2.0
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-syncd
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:syncd: 18.0
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:syncd: 5.0
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-bgp
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:bgp: 14.0
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:bgp: 4.0
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-teamd
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:teamd: 5.0
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:teamd: 2.0
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-swss
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:swss: 8.0
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:swss: 3.0
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-database
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:database: 6.0
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:database: 2.0
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_bgp-used
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_bgp:used: 256.0
13/11/2025 18:12:50 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [53.0, 64.0])
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_bgp:used: 64.0
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_zebra-used
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_zebra:used: 128.0
13/11/2025 18:12:50 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [19.9, 64.0])
13/11/2025 18:12:50 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_zebra:used: 64.0
13/11/2025 18:12:50 __init__.pytest_runtest_teardown         L0124 INFO   | After test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {}, 'top': {'zebra': 21.2, 'bgpd': 109.7}, 'free': {'used': 4253}, 'docker': {'snmp': 0.2, 'pmon': 0.7, 'lldp': 0.2, 'gnmi': 0.3, 'bgp': 0.7, 'radv': 0.1, 'syncd': 4.8, 'teamd': 0.1, 'swss': 0.4, 'database': 0.3}, 'frr_bgp': {'used': 105.9}, 'frr_zebra': {'used': 39.8}}}, 'after_test': {'crdc-garnet-sonic-ud': {'monit': {}, 'top': {'zebra': 19.9, 'bgpd': 109.7}, 'free': {'used': 3064}, 'docker': {'bgp': 0.7, 'radv': 0.1, 'syncd': 4.1, 'teamd': 0.1, 'swss': 0.5, 'database': 0.3}, 'frr_bgp': {'used': 105.8}, 'frr_zebra': {'used': 39.6}}}}
13/11/2025 18:12:50 __init__._fixture_generator_decorator    L0093 INFO   | -------------------- fixture snappi_api teardown starts --------------------
13/11/2025 18:12:56 __init__._fixture_generator_decorator    L0102 INFO   | -------------------- fixture snappi_api teardown ends --------------------
13/11/2025 18:12:56 __init__._fixture_generator_decorator    L0093 INFO   | -------------------- fixture start_pfcwd_after_test teardown starts --------------------
13/11/2025 18:12:57 __init__._fixture_generator_decorator    L0102 INFO   | -------------------- fixture start_pfcwd_after_test teardown ends --------------------
13/11/2025 18:12:57 __init__._fixture_generator_decorator    L0093 INFO   | -------------------- fixture rand_lossy_prio teardown starts --------------------
13/11/2025 18:12:57 __init__._fixture_generator_decorator    L0102 INFO   | -------------------- fixture rand_lossy_prio teardown ends --------------------
13/11/2025 18:12:57 __init__._fixture_generator_decorator    L0093 INFO   | -------------------- fixture rand_lossless_prio teardown starts --------------------
13/11/2025 18:12:57 __init__._fixture_generator_decorator    L0102 INFO   | -------------------- fixture rand_lossless_prio teardown ends --------------------
13/11/2025 18:12:57 __init__._fixture_generator_decorator    L0093 INFO   | -------------------- fixture enable_packet_aging_after_test teardown starts --------------------
13/11/2025 18:12:57 __init__._fixture_generator_decorator    L0102 INFO   | -------------------- fixture enable_packet_aging_after_test teardown ends --------------------
13/11/2025 18:12:57 conftest.run_yang_validation             L3665 INFO   | Running YANG validation on crdc-garnet-sonic-ud (post-test)
13/11/2025 18:13:01 conftest.run_yang_validation             L3681 INFO   | YANG validation passed on crdc-garnet-sonic-ud (post-test)
13/11/2025 18:13:01 conftest.temporarily_disable_route_check L3036 INFO   | Skipping temporarily_disable_route_check fixture
13/11/2025 18:13:03 conftest.collect_after_test              L2823 INFO   | Dumping Disk and Memory Space information after test on crdc-garnet-sonic-ud
13/11/2025 18:13:04 conftest.collect_after_test              L2827 INFO   | Collecting core dumps after test on crdc-garnet-sonic-ud
13/11/2025 18:13:05 conftest.collect_after_test              L2838 INFO   | Collecting running config after test on crdc-garnet-sonic-ud
13/11/2025 18:13:06 conftest.core_dump_and_config_check      L2979 WARNING| Core dump or config check failed for snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py, results: {"core_dump_check": {"failed": false, "new_core_dumps": {"crdc-garnet-sonic-ud": []}}, "config_db_check": {"failed": true, "pre_only_config": {"crdc-garnet-sonic-ud": {"null": {}}}, "cur_only_config": {"crdc-garnet-sonic-ud": {"null": {"WARM_RESTART": {"teamd": {"teamsyncd_timer": "1"}}}}}, "inconsistent_config": {"crdc-garnet-sonic-ud": {"null": {}}}}}
13/11/2025 18:13:06 conftest.restore_config_db_and_config_re L2641 INFO   | dut reload called on crdc-garnet-sonic-ud
13/11/2025 18:13:07 parallel_utils.wrapper                   L0326 INFO   | Running config_reload via synchronized decorator
13/11/2025 18:13:07 parallel_utils.wrapper                   L0335 INFO   | Running original config_reload as par_followers is -1
13/11/2025 18:13:07 config_reload.config_reload              L0150 INFO   | reloading config_db
13/11/2025 18:13:17 utilities.wait_until                     L0153 ERROR  | Exception caught while checking _config_reload_cmd_wrapper:Traceback (most recent call last):
  File "/data/harshit/sonic-mgmt/tests/common/utilities.py", line 149, in wait_until
    check_result = condition(*args, **kwargs)
  File "/data/harshit/sonic-mgmt/tests/common/config_reload.py", line 138, in _config_reload_cmd_wrapper
    out = sonic_host.shell(cmd, executable=executable)
  File "/data/harshit/sonic-mgmt/tests/common/devices/multi_asic.py", line 151, in _run_on_asics
    return getattr(self.sonichost, self.multi_asic_attr)(*module_args, **complex_args)
  File "/data/harshit/sonic-mgmt/tests/common/devices/base.py", line 134, in _run
    raise RunAnsibleModuleFail("run module {} failed".format(self.module_name), res)
tests.common.errors.RunAnsibleModuleFail: run module shell failed, Ansible Results =>
failed = True
changed = True
rc = 4
cmd = config reload -y -f &>/dev/null
start = 2025-11-13 18:13:00.079748
end = 2025-11-13 18:13:08.812040
delta = 0:00:08.732292
msg = non-zero return code
invocation = {'module_args': {'executable': '/bin/bash', '_raw_params': 'config reload -y -f &>/dev/null', '_uses_shell': True, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'creates': None, 'removes': None, 'stdin': None}}
_ansible_no_log = None
stdout =
stderr =

, error:run module shell failed, Ansible Results =>
failed = True
changed = True
rc = 4
cmd = config reload -y -f &>/dev/null
start = 2025-11-13 18:13:00.079748
end = 2025-11-13 18:13:08.812040
delta = 0:00:08.732292
msg = non-zero return code
invocation = {'module_args': {'executable': '/bin/bash', '_raw_params': 'config reload -y -f &>/dev/null', '_uses_shell': True, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'creates': None, 'removes': None, 'stdin': None}}
_ansible_no_log = None
stdout =
stderr =

13/11/2025 18:13:43 sonic.critical_process_status            L0701 INFO   | ====== supervisor process status for service database ======
13/11/2025 18:16:30 __init__.pytest_terminal_summary         L0067 INFO   | Can not get Allure report URL. Please check logs
