25/11/2025 08:44:19 conftest.pytest_sessionstart             L0481 INFO   | Invoking /data/harshit/sonic-mgmt/tests/build-gnmi-stubs.sh with base directory: /data/harshit/sonic-mgmt/tests
25/11/2025 08:44:21 conftest.pytest_sessionstart             L0491 INFO   | Output of /data/harshit/sonic-mgmt/tests/build-gnmi-stubs.sh:
Removing existing directory: /data/harshit/sonic-mgmt/tests/common/sai_validation/github.com/openconfig
Creating directory: /data/harshit/sonic-mgmt/tests/common/sai_validation/github.com/openconfig
Removing existing directory: /data/harshit/sonic-mgmt/tests/common/sai_validation/generated
Creating directory: /data/harshit/sonic-mgmt/tests/common/sai_validation/generated
Cloning https://github.com/openconfig/gnmi.git into /data/harshit/sonic-mgmt/tests/common/sai_validation/github.com/openconfig
Generating gRPC stubs...
gRPC stubs generated successfully.
Creating empty __init__.py files under /data/harshit/sonic-mgmt/tests/common/sai_validation/generated/github/
Moving generated files to correct locations...
Files moved successfully.
Removing /data/harshit/sonic-mgmt/tests/common/sai_validation/generated/github.com directory...
/data/harshit/sonic-mgmt/tests/common/sai_validation/generated/github.com directory removed successfully.

25/11/2025 08:44:21 conftest.pytest_sessionstart             L0502 INFO   | Added /data/harshit/sonic-mgmt/tests/common/sai_validation/generated to sys.path
25/11/2025 08:44:22 __init__.pytest_collection_modifyitems   L0664 INFO   | Available basic facts that can be used in conditional skip:
{
  "topo_type": "ptf",
  "topo_name": "ptf64",
  "testbed": "vms-snappi-sonic",
  "asic_subtype": "broadcom",
  "asic_type": "broadcom",
  "branch": "202505",
  "build_date": "Tue Nov 18 00:00:08 UTC 2025",
  "build_number": 58,
  "build_version": "202505.58-dirty-20251117.113556",
  "built_by": "_devpublish@qnc-sonic-05",
  "commit_id": "a30cdc862",
  "debian_version": "12.12",
  "feature_status": {
    "bgp": "enabled",
    "database": "always_enabled",
    "dhcp_relay": "disabled",
    "eventd": "enabled",
    "gnmi": "enabled",
    "lldp": "enabled",
    "macsec": "disabled",
    "mgmt-framework": "enabled",
    "mux": "always_disabled",
    "nat": "disabled",
    "pmon": "enabled",
    "radv": "enabled",
    "sflow": "disabled",
    "snmp": "enabled",
    "swss": "enabled",
    "syncd": "enabled",
    "teamd": "enabled"
  },
  "hwsku": "Juniper-QFX5241-64-OD",
  "is_chassis": false,
  "is_dpu": false,
  "is_mgmt_ipv6_only": false,
  "is_multi_asic": false,
  "is_smartswitch": false,
  "is_supervisor": false,
  "kernel_version": "6.1.0",
  "libswsscommon": "1.0.0",
  "num_asic": 1,
  "platform": "x86_64-juniper_qfx5241-r0",
  "release": "202505",
  "secure_boot_image": "no",
  "sonic_os_version": 12,
  "sonic_utilities": 1.2,
  "asic_gen": "unknown",
  "minigraph_interfaces": [
    {
      "addr": "10.0.0.0",
      "attachto": "Ethernet0",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.1",
      "prefixlen": 31,
      "subnet": "10.0.0.0/31"
    },
    {
      "addr": "fc00::1",
      "attachto": "Ethernet0",
      "mask": "126",
      "peer_addr": "fc00::2",
      "prefixlen": 126,
      "subnet": "fc00::/126"
    },
    {
      "addr": "10.0.0.26",
      "attachto": "Ethernet104",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.27",
      "prefixlen": 31,
      "subnet": "10.0.0.26/31"
    },
    {
      "addr": "fc00::35",
      "attachto": "Ethernet104",
      "mask": "126",
      "peer_addr": "fc00::36",
      "prefixlen": 126,
      "subnet": "fc00::34/126"
    },
    {
      "addr": "10.0.0.28",
      "attachto": "Ethernet112",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.29",
      "prefixlen": 31,
      "subnet": "10.0.0.28/31"
    },
    {
      "addr": "fc00::39",
      "attachto": "Ethernet112",
      "mask": "126",
      "peer_addr": "fc00::3a",
      "prefixlen": 126,
      "subnet": "fc00::38/126"
    },
    {
      "addr": "10.0.0.30",
      "attachto": "Ethernet120",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.31",
      "prefixlen": 31,
      "subnet": "10.0.0.30/31"
    },
    {
      "addr": "fc00::3d",
      "attachto": "Ethernet120",
      "mask": "126",
      "peer_addr": "fc00::3e",
      "prefixlen": 126,
      "subnet": "fc00::3c/126"
    },
    {
      "addr": "10.0.0.32",
      "attachto": "Ethernet128",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.33",
      "prefixlen": 31,
      "subnet": "10.0.0.32/31"
    },
    {
      "addr": "fc00::41",
      "attachto": "Ethernet128",
      "mask": "126",
      "peer_addr": "fc00::42",
      "prefixlen": 126,
      "subnet": "fc00::40/126"
    },
    {
      "addr": "10.0.0.34",
      "attachto": "Ethernet136",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.35",
      "prefixlen": 31,
      "subnet": "10.0.0.34/31"
    },
    {
      "addr": "fc00::45",
      "attachto": "Ethernet136",
      "mask": "126",
      "peer_addr": "fc00::46",
      "prefixlen": 126,
      "subnet": "fc00::44/126"
    },
    {
      "addr": "10.0.0.36",
      "attachto": "Ethernet144",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.37",
      "prefixlen": 31,
      "subnet": "10.0.0.36/31"
    },
    {
      "addr": "fc00::49",
      "attachto": "Ethernet144",
      "mask": "126",
      "peer_addr": "fc00::4a",
      "prefixlen": 126,
      "subnet": "fc00::48/126"
    },
    {
      "addr": "10.0.0.38",
      "attachto": "Ethernet152",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.39",
      "prefixlen": 31,
      "subnet": "10.0.0.38/31"
    },
    {
      "addr": "fc00::4d",
      "attachto": "Ethernet152",
      "mask": "126",
      "peer_addr": "fc00::4e",
      "prefixlen": 126,
      "subnet": "fc00::4c/126"
    },
    {
      "addr": "10.0.0.4",
      "attachto": "Ethernet16",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.5",
      "prefixlen": 31,
      "subnet": "10.0.0.4/31"
    },
    {
      "addr": "fc00::9",
      "attachto": "Ethernet16",
      "mask": "126",
      "peer_addr": "fc00::a",
      "prefixlen": 126,
      "subnet": "fc00::8/126"
    },
    {
      "addr": "10.0.0.40",
      "attachto": "Ethernet160",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.41",
      "prefixlen": 31,
      "subnet": "10.0.0.40/31"
    },
    {
      "addr": "fc00::51",
      "attachto": "Ethernet160",
      "mask": "126",
      "peer_addr": "fc00::52",
      "prefixlen": 126,
      "subnet": "fc00::50/126"
    },
    {
      "addr": "10.0.0.42",
      "attachto": "Ethernet168",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.43",
      "prefixlen": 31,
      "subnet": "10.0.0.42/31"
    },
    {
      "addr": "fc00::55",
      "attachto": "Ethernet168",
      "mask": "126",
      "peer_addr": "fc00::56",
      "prefixlen": 126,
      "subnet": "fc00::54/126"
    },
    {
      "addr": "10.0.0.44",
      "attachto": "Ethernet176",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.45",
      "prefixlen": 31,
      "subnet": "10.0.0.44/31"
    },
    {
      "addr": "fc00::59",
      "attachto": "Ethernet176",
      "mask": "126",
      "peer_addr": "fc00::5a",
      "prefixlen": 126,
      "subnet": "fc00::58/126"
    },
    {
      "addr": "10.0.0.46",
      "attachto": "Ethernet184",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.47",
      "prefixlen": 31,
      "subnet": "10.0.0.46/31"
    },
    {
      "addr": "fc00::5d",
      "attachto": "Ethernet184",
      "mask": "126",
      "peer_addr": "fc00::5e",
      "prefixlen": 126,
      "subnet": "fc00::5c/126"
    },
    {
      "addr": "10.0.0.48",
      "attachto": "Ethernet192",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.49",
      "prefixlen": 31,
      "subnet": "10.0.0.48/31"
    },
    {
      "addr": "fc00::61",
      "attachto": "Ethernet192",
      "mask": "126",
      "peer_addr": "fc00::62",
      "prefixlen": 126,
      "subnet": "fc00::60/126"
    },
    {
      "addr": "10.0.0.50",
      "attachto": "Ethernet200",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.51",
      "prefixlen": 31,
      "subnet": "10.0.0.50/31"
    },
    {
      "addr": "fc00::65",
      "attachto": "Ethernet200",
      "mask": "126",
      "peer_addr": "fc00::66",
      "prefixlen": 126,
      "subnet": "fc00::64/126"
    },
    {
      "addr": "10.0.0.52",
      "attachto": "Ethernet208",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.53",
      "prefixlen": 31,
      "subnet": "10.0.0.52/31"
    },
    {
      "addr": "fc00::69",
      "attachto": "Ethernet208",
      "mask": "126",
      "peer_addr": "fc00::6a",
      "prefixlen": 126,
      "subnet": "fc00::68/126"
    },
    {
      "addr": "10.0.0.54",
      "attachto": "Ethernet216",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.55",
      "prefixlen": 31,
      "subnet": "10.0.0.54/31"
    },
    {
      "addr": "fc00::6d",
      "attachto": "Ethernet216",
      "mask": "126",
      "peer_addr": "fc00::6e",
      "prefixlen": 126,
      "subnet": "fc00::6c/126"
    },
    {
      "addr": "10.0.0.56",
      "attachto": "Ethernet224",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.57",
      "prefixlen": 31,
      "subnet": "10.0.0.56/31"
    },
    {
      "addr": "fc00::71",
      "attachto": "Ethernet224",
      "mask": "126",
      "peer_addr": "fc00::72",
      "prefixlen": 126,
      "subnet": "fc00::70/126"
    },
    {
      "addr": "10.0.0.58",
      "attachto": "Ethernet232",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.59",
      "prefixlen": 31,
      "subnet": "10.0.0.58/31"
    },
    {
      "addr": "fc00::75",
      "attachto": "Ethernet232",
      "mask": "126",
      "peer_addr": "fc00::76",
      "prefixlen": 126,
      "subnet": "fc00::74/126"
    },
    {
      "addr": "10.0.0.6",
      "attachto": "Ethernet24",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.7",
      "prefixlen": 31,
      "subnet": "10.0.0.6/31"
    },
    {
      "addr": "fc00::d",
      "attachto": "Ethernet24",
      "mask": "126",
      "peer_addr": "fc00::e",
      "prefixlen": 126,
      "subnet": "fc00::c/126"
    },
    {
      "addr": "10.0.0.60",
      "attachto": "Ethernet240",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.61",
      "prefixlen": 31,
      "subnet": "10.0.0.60/31"
    },
    {
      "addr": "fc00::79",
      "attachto": "Ethernet240",
      "mask": "126",
      "peer_addr": "fc00::7a",
      "prefixlen": 126,
      "subnet": "fc00::78/126"
    },
    {
      "addr": "10.0.0.62",
      "attachto": "Ethernet248",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.63",
      "prefixlen": 31,
      "subnet": "10.0.0.62/31"
    },
    {
      "addr": "fc00::7d",
      "attachto": "Ethernet248",
      "mask": "126",
      "peer_addr": "fc00::7e",
      "prefixlen": 126,
      "subnet": "fc00::7c/126"
    },
    {
      "addr": "10.0.0.64",
      "attachto": "Ethernet256",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.65",
      "prefixlen": 31,
      "subnet": "10.0.0.64/31"
    },
    {
      "addr": "fc00::81",
      "attachto": "Ethernet256",
      "mask": "126",
      "peer_addr": "fc00::82",
      "prefixlen": 126,
      "subnet": "fc00::80/126"
    },
    {
      "addr": "10.0.0.66",
      "attachto": "Ethernet264",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.67",
      "prefixlen": 31,
      "subnet": "10.0.0.66/31"
    },
    {
      "addr": "fc00::85",
      "attachto": "Ethernet264",
      "mask": "126",
      "peer_addr": "fc00::86",
      "prefixlen": 126,
      "subnet": "fc00::84/126"
    },
    {
      "addr": "10.0.0.68",
      "attachto": "Ethernet272",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.69",
      "prefixlen": 31,
      "subnet": "10.0.0.68/31"
    },
    {
      "addr": "fc00::89",
      "attachto": "Ethernet272",
      "mask": "126",
      "peer_addr": "fc00::8a",
      "prefixlen": 126,
      "subnet": "fc00::88/126"
    },
    {
      "addr": "10.0.0.70",
      "attachto": "Ethernet280",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.71",
      "prefixlen": 31,
      "subnet": "10.0.0.70/31"
    },
    {
      "addr": "fc00::8d",
      "attachto": "Ethernet280",
      "mask": "126",
      "peer_addr": "fc00::8e",
      "prefixlen": 126,
      "subnet": "fc00::8c/126"
    },
    {
      "addr": "10.0.0.72",
      "attachto": "Ethernet288",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.73",
      "prefixlen": 31,
      "subnet": "10.0.0.72/31"
    },
    {
      "addr": "fc00::91",
      "attachto": "Ethernet288",
      "mask": "126",
      "peer_addr": "fc00::92",
      "prefixlen": 126,
      "subnet": "fc00::90/126"
    },
    {
      "addr": "10.0.0.74",
      "attachto": "Ethernet296",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.75",
      "prefixlen": 31,
      "subnet": "10.0.0.74/31"
    },
    {
      "addr": "fc00::95",
      "attachto": "Ethernet296",
      "mask": "126",
      "peer_addr": "fc00::96",
      "prefixlen": 126,
      "subnet": "fc00::94/126"
    },
    {
      "addr": "10.0.0.76",
      "attachto": "Ethernet304",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.77",
      "prefixlen": 31,
      "subnet": "10.0.0.76/31"
    },
    {
      "addr": "fc00::99",
      "attachto": "Ethernet304",
      "mask": "126",
      "peer_addr": "fc00::9a",
      "prefixlen": 126,
      "subnet": "fc00::98/126"
    },
    {
      "addr": "10.0.0.78",
      "attachto": "Ethernet312",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.79",
      "prefixlen": 31,
      "subnet": "10.0.0.78/31"
    },
    {
      "addr": "fc00::9d",
      "attachto": "Ethernet312",
      "mask": "126",
      "peer_addr": "fc00::9e",
      "prefixlen": 126,
      "subnet": "fc00::9c/126"
    },
    {
      "addr": "10.0.0.8",
      "attachto": "Ethernet32",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.9",
      "prefixlen": 31,
      "subnet": "10.0.0.8/31"
    },
    {
      "addr": "fc00::11",
      "attachto": "Ethernet32",
      "mask": "126",
      "peer_addr": "fc00::12",
      "prefixlen": 126,
      "subnet": "fc00::10/126"
    },
    {
      "addr": "10.0.0.80",
      "attachto": "Ethernet320",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.81",
      "prefixlen": 31,
      "subnet": "10.0.0.80/31"
    },
    {
      "addr": "fc00::a1",
      "attachto": "Ethernet320",
      "mask": "126",
      "peer_addr": "fc00::a2",
      "prefixlen": 126,
      "subnet": "fc00::a0/126"
    },
    {
      "addr": "10.0.0.82",
      "attachto": "Ethernet328",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.83",
      "prefixlen": 31,
      "subnet": "10.0.0.82/31"
    },
    {
      "addr": "fc00::a5",
      "attachto": "Ethernet328",
      "mask": "126",
      "peer_addr": "fc00::a6",
      "prefixlen": 126,
      "subnet": "fc00::a4/126"
    },
    {
      "addr": "10.0.0.84",
      "attachto": "Ethernet336",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.85",
      "prefixlen": 31,
      "subnet": "10.0.0.84/31"
    },
    {
      "addr": "fc00::a9",
      "attachto": "Ethernet336",
      "mask": "126",
      "peer_addr": "fc00::aa",
      "prefixlen": 126,
      "subnet": "fc00::a8/126"
    },
    {
      "addr": "10.0.0.86",
      "attachto": "Ethernet344",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.87",
      "prefixlen": 31,
      "subnet": "10.0.0.86/31"
    },
    {
      "addr": "fc00::ad",
      "attachto": "Ethernet344",
      "mask": "126",
      "peer_addr": "fc00::ae",
      "prefixlen": 126,
      "subnet": "fc00::ac/126"
    },
    {
      "addr": "10.0.0.88",
      "attachto": "Ethernet352",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.89",
      "prefixlen": 31,
      "subnet": "10.0.0.88/31"
    },
    {
      "addr": "fc00::b1",
      "attachto": "Ethernet352",
      "mask": "126",
      "peer_addr": "fc00::b2",
      "prefixlen": 126,
      "subnet": "fc00::b0/126"
    },
    {
      "addr": "10.0.0.90",
      "attachto": "Ethernet360",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.91",
      "prefixlen": 31,
      "subnet": "10.0.0.90/31"
    },
    {
      "addr": "fc00::b5",
      "attachto": "Ethernet360",
      "mask": "126",
      "peer_addr": "fc00::b6",
      "prefixlen": 126,
      "subnet": "fc00::b4/126"
    },
    {
      "addr": "10.0.0.92",
      "attachto": "Ethernet368",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.93",
      "prefixlen": 31,
      "subnet": "10.0.0.92/31"
    },
    {
      "addr": "fc00::b9",
      "attachto": "Ethernet368",
      "mask": "126",
      "peer_addr": "fc00::ba",
      "prefixlen": 126,
      "subnet": "fc00::b8/126"
    },
    {
      "addr": "10.0.0.94",
      "attachto": "Ethernet376",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.95",
      "prefixlen": 31,
      "subnet": "10.0.0.94/31"
    },
    {
      "addr": "fc00::bd",
      "attachto": "Ethernet376",
      "mask": "126",
      "peer_addr": "fc00::be",
      "prefixlen": 126,
      "subnet": "fc00::bc/126"
    },
    {
      "addr": "10.0.0.96",
      "attachto": "Ethernet384",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.97",
      "prefixlen": 31,
      "subnet": "10.0.0.96/31"
    },
    {
      "addr": "fc00::c1",
      "attachto": "Ethernet384",
      "mask": "126",
      "peer_addr": "fc00::c2",
      "prefixlen": 126,
      "subnet": "fc00::c0/126"
    },
    {
      "addr": "10.0.0.98",
      "attachto": "Ethernet392",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.99",
      "prefixlen": 31,
      "subnet": "10.0.0.98/31"
    },
    {
      "addr": "fc00::c5",
      "attachto": "Ethernet392",
      "mask": "126",
      "peer_addr": "fc00::c6",
      "prefixlen": 126,
      "subnet": "fc00::c4/126"
    },
    {
      "addr": "10.0.0.10",
      "attachto": "Ethernet40",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.11",
      "prefixlen": 31,
      "subnet": "10.0.0.10/31"
    },
    {
      "addr": "fc00::15",
      "attachto": "Ethernet40",
      "mask": "126",
      "peer_addr": "fc00::16",
      "prefixlen": 126,
      "subnet": "fc00::14/126"
    },
    {
      "addr": "10.0.0.100",
      "attachto": "Ethernet400",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.101",
      "prefixlen": 31,
      "subnet": "10.0.0.100/31"
    },
    {
      "addr": "fc00::c9",
      "attachto": "Ethernet400",
      "mask": "126",
      "peer_addr": "fc00::ca",
      "prefixlen": 126,
      "subnet": "fc00::c8/126"
    },
    {
      "addr": "10.0.0.102",
      "attachto": "Ethernet408",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.103",
      "prefixlen": 31,
      "subnet": "10.0.0.102/31"
    },
    {
      "addr": "fc00::cd",
      "attachto": "Ethernet408",
      "mask": "126",
      "peer_addr": "fc00::ce",
      "prefixlen": 126,
      "subnet": "fc00::cc/126"
    },
    {
      "addr": "10.0.0.104",
      "attachto": "Ethernet416",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.105",
      "prefixlen": 31,
      "subnet": "10.0.0.104/31"
    },
    {
      "addr": "fc00::d1",
      "attachto": "Ethernet416",
      "mask": "126",
      "peer_addr": "fc00::d2",
      "prefixlen": 126,
      "subnet": "fc00::d0/126"
    },
    {
      "addr": "20.1.1.0",
      "attachto": "Ethernet420",
      "mask": "255.255.255.254",
      "peer_addr": "20.1.1.1",
      "prefixlen": 31,
      "subnet": "20.1.1.0/31"
    },
    {
      "addr": "2000:1::1",
      "attachto": "Ethernet420",
      "mask": "126",
      "peer_addr": "2000:1::2",
      "prefixlen": 126,
      "subnet": "2000:1::/126"
    },
    {
      "addr": "10.0.0.106",
      "attachto": "Ethernet424",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.107",
      "prefixlen": 31,
      "subnet": "10.0.0.106/31"
    },
    {
      "addr": "fc00::d5",
      "attachto": "Ethernet424",
      "mask": "126",
      "peer_addr": "fc00::d6",
      "prefixlen": 126,
      "subnet": "fc00::d4/126"
    },
    {
      "addr": "20.1.1.2",
      "attachto": "Ethernet428",
      "mask": "255.255.255.254",
      "peer_addr": "20.1.1.3",
      "prefixlen": 31,
      "subnet": "20.1.1.2/31"
    },
    {
      "addr": "2000:1::5",
      "attachto": "Ethernet428",
      "mask": "126",
      "peer_addr": "2000:1::6",
      "prefixlen": 126,
      "subnet": "2000:1::4/126"
    },
    {
      "addr": "10.0.0.108",
      "attachto": "Ethernet432",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.109",
      "prefixlen": 31,
      "subnet": "10.0.0.108/31"
    },
    {
      "addr": "fc00::d9",
      "attachto": "Ethernet432",
      "mask": "126",
      "peer_addr": "fc00::da",
      "prefixlen": 126,
      "subnet": "fc00::d8/126"
    },
    {
      "addr": "20.1.1.4",
      "attachto": "Ethernet436",
      "mask": "255.255.255.254",
      "peer_addr": "20.1.1.5",
      "prefixlen": 31,
      "subnet": "20.1.1.4/31"
    },
    {
      "addr": "2000:1::9",
      "attachto": "Ethernet436",
      "mask": "126",
      "peer_addr": "2000:1::a",
      "prefixlen": 126,
      "subnet": "2000:1::8/126"
    },
    {
      "addr": "10.0.0.110",
      "attachto": "Ethernet440",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.111",
      "prefixlen": 31,
      "subnet": "10.0.0.110/31"
    },
    {
      "addr": "fc00::dd",
      "attachto": "Ethernet440",
      "mask": "126",
      "peer_addr": "fc00::de",
      "prefixlen": 126,
      "subnet": "fc00::dc/126"
    },
    {
      "addr": "20.1.1.6",
      "attachto": "Ethernet444",
      "mask": "255.255.255.254",
      "peer_addr": "20.1.1.7",
      "prefixlen": 31,
      "subnet": "20.1.1.6/31"
    },
    {
      "addr": "2000:1::d",
      "attachto": "Ethernet444",
      "mask": "126",
      "peer_addr": "2000:1::e",
      "prefixlen": 126,
      "subnet": "2000:1::c/126"
    },
    {
      "addr": "10.0.0.112",
      "attachto": "Ethernet448",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.113",
      "prefixlen": 31,
      "subnet": "10.0.0.112/31"
    },
    {
      "addr": "fc00::e1",
      "attachto": "Ethernet448",
      "mask": "126",
      "peer_addr": "fc00::e2",
      "prefixlen": 126,
      "subnet": "fc00::e0/126"
    },
    {
      "addr": "10.0.0.114",
      "attachto": "Ethernet456",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.115",
      "prefixlen": 31,
      "subnet": "10.0.0.114/31"
    },
    {
      "addr": "fc00::e5",
      "attachto": "Ethernet456",
      "mask": "126",
      "peer_addr": "fc00::e6",
      "prefixlen": 126,
      "subnet": "fc00::e4/126"
    },
    {
      "addr": "10.0.0.116",
      "attachto": "Ethernet464",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.117",
      "prefixlen": 31,
      "subnet": "10.0.0.116/31"
    },
    {
      "addr": "fc00::e9",
      "attachto": "Ethernet464",
      "mask": "126",
      "peer_addr": "fc00::ea",
      "prefixlen": 126,
      "subnet": "fc00::e8/126"
    },
    {
      "addr": "10.0.0.118",
      "attachto": "Ethernet472",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.119",
      "prefixlen": 31,
      "subnet": "10.0.0.118/31"
    },
    {
      "addr": "fc00::ed",
      "attachto": "Ethernet472",
      "mask": "126",
      "peer_addr": "fc00::ee",
      "prefixlen": 126,
      "subnet": "fc00::ec/126"
    },
    {
      "addr": "10.0.0.12",
      "attachto": "Ethernet48",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.13",
      "prefixlen": 31,
      "subnet": "10.0.0.12/31"
    },
    {
      "addr": "fc00::19",
      "attachto": "Ethernet48",
      "mask": "126",
      "peer_addr": "fc00::1a",
      "prefixlen": 126,
      "subnet": "fc00::18/126"
    },
    {
      "addr": "10.0.0.120",
      "attachto": "Ethernet480",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.121",
      "prefixlen": 31,
      "subnet": "10.0.0.120/31"
    },
    {
      "addr": "fc00::f1",
      "attachto": "Ethernet480",
      "mask": "126",
      "peer_addr": "fc00::f2",
      "prefixlen": 126,
      "subnet": "fc00::f0/126"
    },
    {
      "addr": "10.0.0.122",
      "attachto": "Ethernet488",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.123",
      "prefixlen": 31,
      "subnet": "10.0.0.122/31"
    },
    {
      "addr": "fc00::f5",
      "attachto": "Ethernet488",
      "mask": "126",
      "peer_addr": "fc00::f6",
      "prefixlen": 126,
      "subnet": "fc00::f4/126"
    },
    {
      "addr": "10.0.0.124",
      "attachto": "Ethernet496",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.125",
      "prefixlen": 31,
      "subnet": "10.0.0.124/31"
    },
    {
      "addr": "fc00::f9",
      "attachto": "Ethernet496",
      "mask": "126",
      "peer_addr": "fc00::fa",
      "prefixlen": 126,
      "subnet": "fc00::f8/126"
    },
    {
      "addr": "10.0.0.126",
      "attachto": "Ethernet504",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.127",
      "prefixlen": 31,
      "subnet": "10.0.0.126/31"
    },
    {
      "addr": "fc00::fd",
      "attachto": "Ethernet504",
      "mask": "126",
      "peer_addr": "fc00::fe",
      "prefixlen": 126,
      "subnet": "fc00::fc/126"
    },
    {
      "addr": "10.0.0.14",
      "attachto": "Ethernet56",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.15",
      "prefixlen": 31,
      "subnet": "10.0.0.14/31"
    },
    {
      "addr": "fc00::1d",
      "attachto": "Ethernet56",
      "mask": "126",
      "peer_addr": "fc00::1e",
      "prefixlen": 126,
      "subnet": "fc00::1c/126"
    },
    {
      "addr": "10.0.0.16",
      "attachto": "Ethernet64",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.17",
      "prefixlen": 31,
      "subnet": "10.0.0.16/31"
    },
    {
      "addr": "fc00::21",
      "attachto": "Ethernet64",
      "mask": "126",
      "peer_addr": "fc00::22",
      "prefixlen": 126,
      "subnet": "fc00::20/126"
    },
    {
      "addr": "10.0.0.18",
      "attachto": "Ethernet72",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.19",
      "prefixlen": 31,
      "subnet": "10.0.0.18/31"
    },
    {
      "addr": "fc00::25",
      "attachto": "Ethernet72",
      "mask": "126",
      "peer_addr": "fc00::26",
      "prefixlen": 126,
      "subnet": "fc00::24/126"
    },
    {
      "addr": "10.0.0.2",
      "attachto": "Ethernet8",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.3",
      "prefixlen": 31,
      "subnet": "10.0.0.2/31"
    },
    {
      "addr": "fc00::5",
      "attachto": "Ethernet8",
      "mask": "126",
      "peer_addr": "fc00::6",
      "prefixlen": 126,
      "subnet": "fc00::4/126"
    },
    {
      "addr": "10.0.0.20",
      "attachto": "Ethernet80",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.21",
      "prefixlen": 31,
      "subnet": "10.0.0.20/31"
    },
    {
      "addr": "fc00::29",
      "attachto": "Ethernet80",
      "mask": "126",
      "peer_addr": "fc00::2a",
      "prefixlen": 126,
      "subnet": "fc00::28/126"
    },
    {
      "addr": "10.0.0.22",
      "attachto": "Ethernet88",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.23",
      "prefixlen": 31,
      "subnet": "10.0.0.22/31"
    },
    {
      "addr": "fc00::2d",
      "attachto": "Ethernet88",
      "mask": "126",
      "peer_addr": "fc00::2e",
      "prefixlen": 126,
      "subnet": "fc00::2c/126"
    },
    {
      "addr": "10.0.0.24",
      "attachto": "Ethernet96",
      "mask": "255.255.255.254",
      "peer_addr": "10.0.0.25",
      "prefixlen": 31,
      "subnet": "10.0.0.24/31"
    },
    {
      "addr": "fc00::31",
      "attachto": "Ethernet96",
      "mask": "126",
      "peer_addr": "fc00::32",
      "prefixlen": 126,
      "subnet": "fc00::30/126"
    }
  ],
  "minigraph_portchannels": {},
  "minigraph_portchannel_interfaces": [],
  "minigraph_neighbors": {
    "Ethernet0": {
      "name": "ARISTA01T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet104": {
      "name": "ARISTA14T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet112": {
      "name": "ARISTA15T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet120": {
      "name": "ARISTA16T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet128": {
      "name": "ARISTA01T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet136": {
      "name": "ARISTA02T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet144": {
      "name": "ARISTA03T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet152": {
      "name": "ARISTA04T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet16": {
      "name": "ARISTA03T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet160": {
      "name": "ARISTA05T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet168": {
      "name": "ARISTA06T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet176": {
      "name": "ARISTA07T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet184": {
      "name": "ARISTA08T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet192": {
      "name": "ARISTA09T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet200": {
      "name": "ARISTA10T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet208": {
      "name": "ARISTA11T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet216": {
      "name": "ARISTA12T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet224": {
      "name": "ARISTA13T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet232": {
      "name": "ARISTA14T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet24": {
      "name": "ARISTA04T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet240": {
      "name": "ARISTA15T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet248": {
      "name": "ARISTA16T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet256": {
      "name": "ARISTA17T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet264": {
      "name": "ARISTA18T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet272": {
      "name": "ARISTA19T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet280": {
      "name": "ARISTA20T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet288": {
      "name": "ARISTA21T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet296": {
      "name": "ARISTA22T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet304": {
      "name": "ARISTA23T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet312": {
      "name": "ARISTA24T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet32": {
      "name": "ARISTA05T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet320": {
      "name": "ARISTA25T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet328": {
      "name": "ARISTA26T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet336": {
      "name": "ARISTA27T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet344": {
      "name": "ARISTA28T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet352": {
      "name": "ARISTA29T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet360": {
      "name": "ARISTA30T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet368": {
      "name": "ARISTA31T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet376": {
      "name": "ARISTA32T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet384": {
      "name": "ARISTA33T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet392": {
      "name": "ARISTA34T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet40": {
      "name": "ARISTA06T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet400": {
      "name": "ARISTA35T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet408": {
      "name": "ARISTA36T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet416": {
      "name": "ARISTA37T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet420": {
      "name": "ARISTA37T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet424": {
      "name": "ARISTA38T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet428": {
      "name": "ARISTA38T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet432": {
      "name": "ARISTA39T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet436": {
      "name": "ARISTA39T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet440": {
      "name": "ARISTA40T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet444": {
      "name": "ARISTA40T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet448": {
      "name": "ARISTA41T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet456": {
      "name": "ARISTA42T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet464": {
      "name": "ARISTA43T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet472": {
      "name": "ARISTA44T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet48": {
      "name": "ARISTA07T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet480": {
      "name": "ARISTA45T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet488": {
      "name": "ARISTA46T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet496": {
      "name": "ARISTA47T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet504": {
      "name": "ARISTA48T0",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet56": {
      "name": "ARISTA08T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet64": {
      "name": "ARISTA09T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet72": {
      "name": "ARISTA10T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet8": {
      "name": "ARISTA02T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet80": {
      "name": "ARISTA11T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet88": {
      "name": "ARISTA12T2",
      "namespace": "",
      "port": "Ethernet1"
    },
    "Ethernet96": {
      "name": "ARISTA13T2",
      "namespace": "",
      "port": "Ethernet1"
    }
  },
  "VOQ_INBAND_INTERFACE": {},
  "BGP_VOQ_CHASSIS_NEIGHBOR": {},
  "INTERFACE": {
    "Ethernet0": {
      "10.0.0.0/31": {}
    },
    "Ethernet104": {
      "10.0.0.26/31": {}
    },
    "Ethernet112": {
      "10.0.0.28/31": {}
    },
    "Ethernet120": {
      "10.0.0.30/31": {}
    },
    "Ethernet128": {
      "10.0.0.32/31": {}
    },
    "Ethernet136": {
      "10.0.0.34/31": {}
    },
    "Ethernet144": {
      "10.0.0.36/31": {}
    },
    "Ethernet152": {
      "10.0.0.38/31": {}
    },
    "Ethernet16": {
      "10.0.0.4/31": {}
    },
    "Ethernet160": {
      "10.0.0.40/31": {}
    },
    "Ethernet168": {
      "10.0.0.42/31": {}
    },
    "Ethernet176": {
      "10.0.0.44/31": {}
    },
    "Ethernet184": {
      "10.0.0.46/31": {}
    },
    "Ethernet192": {
      "10.0.0.48/31": {}
    },
    "Ethernet200": {
      "10.0.0.50/31": {}
    },
    "Ethernet208": {
      "10.0.0.52/31": {}
    },
    "Ethernet216": {
      "10.0.0.54/31": {}
    },
    "Ethernet224": {
      "10.0.0.56/31": {}
    },
    "Ethernet232": {
      "10.0.0.58/31": {}
    },
    "Ethernet24": {
      "10.0.0.6/31": {}
    },
    "Ethernet240": {
      "10.0.0.60/31": {}
    },
    "Ethernet248": {
      "10.0.0.62/31": {}
    },
    "Ethernet256": {
      "10.0.0.64/31": {}
    },
    "Ethernet264": {
      "10.0.0.66/31": {}
    },
    "Ethernet272": {
      "10.0.0.68/31": {}
    },
    "Ethernet280": {
      "10.0.0.70/31": {}
    },
    "Ethernet288": {
      "10.0.0.72/31": {}
    },
    "Ethernet296": {
      "10.0.0.74/31": {}
    },
    "Ethernet304": {
      "10.0.0.76/31": {}
    },
    "Ethernet312": {
      "10.0.0.78/31": {}
    },
    "Ethernet32": {
      "10.0.0.8/31": {}
    },
    "Ethernet320": {
      "10.0.0.80/31": {}
    },
    "Ethernet328": {
      "10.0.0.82/31": {}
    },
    "Ethernet336": {
      "10.0.0.84/31": {}
    },
    "Ethernet344": {
      "10.0.0.86/31": {}
    },
    "Ethernet352": {
      "10.0.0.88/31": {}
    },
    "Ethernet360": {
      "10.0.0.90/31": {}
    },
    "Ethernet368": {
      "10.0.0.92/31": {}
    },
    "Ethernet376": {
      "10.0.0.94/31": {}
    },
    "Ethernet384": {
      "10.0.0.96/31": {}
    },
    "Ethernet392": {
      "10.0.0.98/31": {}
    },
    "Ethernet40": {
      "10.0.0.10/31": {}
    },
    "Ethernet400": {
      "10.0.0.100/31": {}
    },
    "Ethernet408": {
      "10.0.0.102/31": {}
    },
    "Ethernet420": {
      "20.1.1.0/31": {},
      "2000:1::1/126": {}
    },
    "Ethernet428": {
      "20.1.1.2/31": {},
      "2000:1::5/126": {}
    },
    "Ethernet436": {
      "20.1.1.4/31": {},
      "2000:1::9/126": {}
    },
    "Ethernet444": {
      "20.1.1.6/31": {},
      "2000:1::d/126": {}
    },
    "Ethernet448": {
      "10.0.0.112/31": {}
    },
    "Ethernet456": {
      "10.0.0.114/31": {}
    },
    "Ethernet464": {
      "10.0.0.116/31": {}
    },
    "Ethernet472": {
      "10.0.0.118/31": {}
    },
    "Ethernet48": {
      "10.0.0.12/31": {}
    },
    "Ethernet480": {
      "10.0.0.120/31": {}
    },
    "Ethernet488": {
      "10.0.0.122/31": {}
    },
    "Ethernet496": {
      "10.0.0.124/31": {}
    },
    "Ethernet504": {
      "10.0.0.126/31": {}
    },
    "Ethernet512": {
      "10.0.0.128/31": {}
    },
    "Ethernet513": {
      "10.0.0.130/31": {}
    },
    "Ethernet56": {
      "10.0.0.14/31": {}
    },
    "Ethernet64": {
      "10.0.0.16/31": {}
    },
    "Ethernet72": {
      "10.0.0.18/31": {}
    },
    "Ethernet8": {
      "10.0.0.2/31": {}
    },
    "Ethernet80": {
      "10.0.0.20/31": {}
    },
    "Ethernet88": {
      "10.0.0.22/31": {}
    },
    "Ethernet96": {
      "10.0.0.24/31": {}
    }
  },
  "switch_type": "",
  "type": "LeafRouter",
  "switch": {
    "ACL_ACTION|PACKET_ACTION": "COPY,DROP,FORWARD",
    "ASIC_SDK_HEALTH_EVENT": "false",
    "ECMP_HASH_ALGORITHM": "CRC,XOR,CRC_32LO,CRC_32HI,CRC_CCITT,CRC_XOR",
    "ECMP_HASH_ALGORITHM_CAPABLE": "true",
    "ECMP_HASH_CAPABLE": "false",
    "HASH|NATIVE_HASH_FIELD_LIST": "SRC_IP,DST_IP,VLAN_ID,IP_PROTOCOL,ETHERTYPE,L4_SRC_PORT,L4_DST_PORT,SRC_MAC,DST_MAC,IN_PORT,IPV6_FLOW_LABEL",
    "ICMP_OFFLOAD_CAPABLE": "false",
    "LAG_HASH_ALGORITHM": "CRC,XOR,CRC_32LO,CRC_32HI,CRC_CCITT,CRC_XOR",
    "LAG_HASH_ALGORITHM_CAPABLE": "true",
    "LAG_HASH_CAPABLE": "false",
    "LAG_TPID_CAPABLE": "false",
    "MAX_NEXTHOP_GROUP_COUNT": "4095",
    "MIRROR": "true",
    "MIRRORV6": "true",
    "ORDERED_ECMP_CAPABLE": "true",
    "PATH_TRACING_CAPABLE": "false",
    "PFC_DLR_INIT_CAPABLE": "true",
    "PORT_EGRESS_SAMPLE_CAPABLE": "false",
    "PORT_TPID_CAPABLE": "true",
    "REG_FATAL_ASIC_SDK_HEALTH_CATEGORY": "false",
    "REG_NOTICE_ASIC_SDK_HEALTH_CATEGORY": "false",
    "REG_WARNING_ASIC_SDK_HEALTH_CATEGORY": "false",
    "SWITCH_TRIMMING_CAPABLE": "false",
    "SWITCH|NUMBER_OF_TRAFFIC_CLASSES": "8",
    "SWITCH|NUMBER_OF_UNICAST_QUEUES": "8",
    "SWITCH|PACKET_TRIMMING_DSCP_RESOLUTION_MODE": "N/A",
    "SWITCH|PACKET_TRIMMING_QUEUE_RESOLUTION_MODE": "N/A"
  },
  "macsec_en": false
}
25/11/2025 08:44:22 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_single_lossless_prio_reboot[warm] setup  ====================
25/11/2025 08:44:22 __init__.set_default                     L0053 INFO   | Completeness level not set during test execution. Setting to default level: CompletenessLevel.basic
25/11/2025 08:44:22 __init__.check_test_completeness         L0151 INFO   | Test has no defined levels. Continue without test completeness checks
25/11/2025 08:44:22 conftest.enhance_inventory               L0345 INFO   | Inventory file: ['../ansible/snappi-sonic']
25/11/2025 08:44:25 ptfhost_utils.run_icmp_responder_session L0326 INFO   | Skip running icmp_responder at session level, it is only for dualtor testbed with active-active mux ports.
25/11/2025 08:44:26 __init__._sanity_check                   L0442 INFO   | Skip sanity check according to command line argument
25/11/2025 08:44:26 conftest.collect_before_test             L2741 INFO   | Dumping Disk and Memory Space information before test on crdc-garnet-sonic-ud
25/11/2025 08:44:26 conftest.collect_before_test             L2745 INFO   | Collecting core dumps before test on crdc-garnet-sonic-ud
25/11/2025 08:44:27 conftest.collect_before_test             L2754 INFO   | Collecting running config before test on crdc-garnet-sonic-ud
25/11/2025 08:44:30 conftest.temporarily_disable_route_check L3034 INFO   | Skipping temporarily_disable_route_check fixture
25/11/2025 08:44:30 conftest.generate_params_dut_hostname    L1544 INFO   | Using DUTs ['crdc-garnet-sonic-ud'] in testbed 'vms-snappi-sonic'
25/11/2025 08:44:30 conftest.set_rand_one_dut_hostname       L0658 INFO   | Randomly select dut crdc-garnet-sonic-ud for testing
25/11/2025 08:44:30 __init__._fixture_generator_decorator    L0081 INFO   | -------------------- fixture enable_packet_aging_after_test setup starts --------------------
25/11/2025 08:44:30 __init__._fixture_generator_decorator    L0085 INFO   | -------------------- fixture enable_packet_aging_after_test setup ends --------------------
25/11/2025 08:44:30 __init__._fixture_generator_decorator    L0081 INFO   | -------------------- fixture rand_lossless_prio setup starts --------------------
25/11/2025 08:44:30 __init__._fixture_generator_decorator    L0085 INFO   | -------------------- fixture rand_lossless_prio setup ends --------------------
25/11/2025 08:44:30 __init__._fixture_generator_decorator    L0081 INFO   | -------------------- fixture rand_lossy_prio setup starts --------------------
25/11/2025 08:44:30 __init__._fixture_generator_decorator    L0085 INFO   | -------------------- fixture rand_lossy_prio setup ends --------------------
25/11/2025 08:44:30 __init__._fixture_generator_decorator    L0081 INFO   | -------------------- fixture start_pfcwd_after_test setup starts --------------------
25/11/2025 08:44:30 __init__._fixture_generator_decorator    L0085 INFO   | -------------------- fixture start_pfcwd_after_test setup ends --------------------
25/11/2025 08:44:30 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture snappi_api_serv_ip setup starts --------------------
25/11/2025 08:44:30 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture snappi_api_serv_ip setup ends --------------------
25/11/2025 08:44:30 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture snappi_api_serv_port setup starts --------------------
25/11/2025 08:44:30 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture snappi_api_serv_port setup ends --------------------
25/11/2025 08:44:30 __init__._fixture_generator_decorator    L0081 INFO   | -------------------- fixture snappi_api setup starts --------------------
25/11/2025 08:44:30 __init__._fixture_generator_decorator    L0085 INFO   | -------------------- fixture snappi_api setup ends --------------------
25/11/2025 08:44:30 conftest.rand_one_dut_front_end_hostname L0694 INFO   | Randomly select dut crdc-garnet-sonic-ud for testing
25/11/2025 08:44:30 conftest.generate_port_lists             L1636 INFO   | Generate dut_port_map: {'crdc-garnet-sonic-ud': ['crdc-garnet-sonic-ud|Ethernet420', 'crdc-garnet-sonic-ud|Ethernet428', 'crdc-garnet-sonic-ud|Ethernet436', 'crdc-garnet-sonic-ud|Ethernet444']}
25/11/2025 08:44:30 conftest.generate_port_lists             L1659 INFO   | Generate port_list: ['crdc-garnet-sonic-ud|Ethernet420', 'crdc-garnet-sonic-ud|Ethernet428', 'crdc-garnet-sonic-ud|Ethernet436', 'crdc-garnet-sonic-ud|Ethernet444']
25/11/2025 08:44:30 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture prio_dscp_map setup starts --------------------
25/11/2025 08:44:31 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture prio_dscp_map setup ends --------------------
25/11/2025 08:44:31 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture all_prio_list setup starts --------------------
25/11/2025 08:44:31 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture all_prio_list setup ends --------------------
25/11/2025 08:44:31 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture get_snappi_ports setup starts --------------------
25/11/2025 08:44:31 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture get_snappi_ports_single_dut setup starts --------------------
25/11/2025 08:44:31 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture get_snappi_ports_single_dut setup ends --------------------
25/11/2025 08:44:31 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture get_snappi_ports setup ends --------------------
25/11/2025 08:44:31 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture is_pfc_enabled setup starts --------------------
25/11/2025 08:44:32 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture is_pfc_enabled setup ends --------------------
25/11/2025 08:44:32 __init__.loganalyzer                     L0077 INFO   | Log analyzer is disabled
25/11/2025 08:44:32 __init__.memory_utilization              L0143 INFO   | Hostname: crdc-garnet-sonic-ud, Hwsku: Juniper-QFX5241-64-OD, Platform: x86_64-juniper_qfx5241-r0
25/11/2025 08:44:32 memory_utilization.parse_and_register_co L0365 INFO   | Loading memory monitoring commands for hwsku: Juniper-QFX5241-64-OD
25/11/2025 08:44:32 memory_utilization.register_command      L0023 INFO   | Registering command: name=monit, cmd=sudo monit validate, memory_params={'memory_usage': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 10}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 70}}}, memory_check=<function parse_monit_validate_output at 0x729141fa9940>
25/11/2025 08:44:32 memory_utilization.register_command      L0023 INFO   | Registering command: name=top, cmd=top -b -n 1, memory_params={'bgpd': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}, 'zebra': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}}, memory_check=<function parse_top_output at 0x729141fa9160>
25/11/2025 08:44:32 memory_utilization.register_command      L0023 INFO   | Registering command: name=free, cmd=free -m, memory_params={'used': {'memory_increase_threshold': {'type': 'percentage', 'value': '20%'}, 'memory_high_threshold': None}}, memory_check=<function parse_free_output at 0x729141fa98b0>
25/11/2025 08:44:32 memory_utilization.register_command      L0023 INFO   | Registering command: name=docker, cmd=docker stats --no-stream, memory_params={'snmp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'pmon': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'lldp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'gnmi': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}, 'radv': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 3}}, 'syncd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 5}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 18}}, 'bgp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 4}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 14}}, 'teamd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 5}}, 'swss': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 3}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'database': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}}, memory_check=<function parse_docker_stats_output at 0x729141fa99d0>
25/11/2025 08:44:32 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_bgp, cmd=vtysh -c "show memory bgp", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 256}}}, memory_check=<function parse_frr_memory_output at 0x729141fa9a60>
25/11/2025 08:44:32 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_zebra, cmd=vtysh -c "show memory zebra", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 128}}}, memory_check=<function parse_frr_memory_output at 0x729141fa9a60>
25/11/2025 08:44:32 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture snappi_testbed_config setup starts --------------------
25/11/2025 08:44:33 snappi_fixtures.snappi_testbed_config    L0526 INFO   | Configuring TGEN L1: link_training=False, rs_fec=False (DUT derived)
25/11/2025 08:44:39 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture snappi_testbed_config setup ends --------------------
25/11/2025 08:44:39 conftest.setup_dualtor_mux_ports         L3494 INFO   | skip setup dualtor mux cables on non-dualtor testbed
25/11/2025 08:44:44 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 61.0 MB, holding: 54525952.0 bytes, small: 0.0 bytes, ordinary: 9479168.0 bytes
25/11/2025 08:44:45 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 39.7 MB, holding: 35651584.0 bytes, small: 0.0 bytes, ordinary: 5965824.0 bytes
25/11/2025 08:44:45 __init__.pytest_runtest_setup            L0064 INFO   | Before test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {'memory_usage': 13.5}, 'top': {'zebra': 43.4, 'bgpd': 75.5}, 'free': {'used': 4254}, 'docker': {'snmp': 0.2, 'pmon': 0.8, 'lldp': 0.2, 'gnmi': 0.5, 'bgp': 0.6, 'radv': 0.1, 'syncd': 4.1, 'teamd': 0.1, 'swss': 0.4, 'database': 0.3}, 'frr_bgp': {'used': 61.0}, 'frr_zebra': {'used': 39.7}}}, 'after_test': {'crdc-garnet-sonic-ud': {}}}
25/11/2025 08:44:45 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_single_lossless_prio_reboot[warm] call ====================
25/11/2025 08:44:45 sonic.get_asic_name                      L1860 INFO   | asic: th5
25/11/2025 08:44:45 helper.skip_warm_reboot                  L0050 INFO   | Reboot type warm is  supported on broadcom switches
25/11/2025 08:44:58 connection._warn                         L0332 WARNING| Verification of certificates is disabled
25/11/2025 08:44:58 connection._info                         L0329 INFO   | Determining the platform and rest_port using the 10.204.99.60 address...
25/11/2025 08:44:58 connection._warn                         L0332 WARNING| Unable to connect to http://10.204.99.60:8001.
25/11/2025 08:44:58 connection._warn                         L0332 WARNING| Unable to connect to https://10.204.99.60:8001.
25/11/2025 08:44:58 connection._warn                         L0332 WARNING| Unable to connect to http://10.204.99.60:443.
25/11/2025 08:44:58 connection._info                         L0329 INFO   | Connection established to `https://10.204.99.60:443 on linux`
25/11/2025 08:45:09 connection._info                         L0329 INFO   | Using IxNetwork api server version 10.80.2412.18
25/11/2025 08:45:09 connection._info                         L0329 INFO   | User info IxNetwork/apiserver/admin-209-705870
25/11/2025 08:45:10 snappi_api.info                          L1419 INFO   | snappi-1.27.1
25/11/2025 08:45:10 snappi_api.info                          L1419 INFO   | snappi_ixnetwork-1.27.2
25/11/2025 08:45:10 snappi_api.info                          L1419 INFO   | ixnetwork_restpy-1.6.1
25/11/2025 08:45:10 snappi_api.info                          L1419 INFO   | Config validation 0.014s
25/11/2025 08:45:13 snappi_api.info                          L1419 INFO   | Ports configuration 1.637s
25/11/2025 08:45:13 snappi_api.info                          L1419 INFO   | Captures configuration 0.172s
25/11/2025 08:45:16 snappi_api.info                          L1419 INFO   | Add location hosts [10.207.65.42] 2.404s
25/11/2025 08:45:18 snappi_api.info                          L1419 INFO   | Location hosts ready [10.207.65.42] 2.227s
25/11/2025 08:45:19 snappi_api.info                          L1419 INFO   | Speed conversion is not require for (port.name, speed) : [('Port 0', 'starTwoByFourHundredGigNonFannedOutPAM4'), ('Port 1', 'starTwoByFourHundredGigNonFannedOutPAM4'), ('Port 2', 'starTwoByFourHundredGigNonFannedOutPAM4'), ('Port 3', 'starTwoByFourHundredGigNonFannedOutPAM4')]
25/11/2025 08:45:19 snappi_api.info                          L1419 INFO   | Aggregation mode speed change 0.710s
25/11/2025 08:45:20 snappi_api.info                          L1419 INFO   | Location preemption [10.207.65.42;1;1, 10.207.65.42;1;2, 10.207.65.42;1;3, 10.207.65.42;1;4] 0.169s
25/11/2025 08:45:36 snappi_api.info                          L1419 INFO   | Location connect [Port 0, Port 1, Port 2, Port 3] 16.669s
25/11/2025 08:45:37 snappi_api.info                          L1419 INFO   | Location state check [Port 0, Port 1, Port 2, Port 3] 0.322s
25/11/2025 08:45:37 snappi_api.info                          L1419 INFO   | Location configuration 24.070s
25/11/2025 08:45:50 snappi_api.info                          L1419 INFO   | Layer1 configuration 13.541s
25/11/2025 08:45:51 snappi_api.info                          L1419 INFO   | Lag Configuration 0.118s
25/11/2025 08:45:51 snappi_api.info                          L1419 INFO   | Convert device config : 0.306s
25/11/2025 08:45:51 snappi_api.info                          L1419 INFO   | Create IxNetwork device config : 0.001s
25/11/2025 08:45:52 snappi_api.info                          L1419 INFO   | Push IxNetwork device config : 0.731s
25/11/2025 08:45:52 snappi_api.info                          L1419 INFO   | Devices configuration 1.138s
25/11/2025 08:46:01 snappi_api.info                          L1419 INFO   | Flows configuration 9.718s
25/11/2025 08:46:08 snappi_api.info                          L1419 INFO   | Start interfaces 5.948s
25/11/2025 08:46:08 snappi_api.info                          L1419 INFO   | IxNet - The Traffic Item was modified. Please perform a Traffic Generate to update the associated traffic Flow Groups
25/11/2025 08:46:08 traffic_generation.run_traffic           L0616 INFO   | Wait for Arp to Resolve ...
25/11/2025 08:46:21 traffic_generation.run_traffic           L0638 INFO   | Starting transmit on all flows ...
25/11/2025 08:46:25 snappi_api.info                          L1419 INFO   | Flows generate/apply 3.111s
25/11/2025 08:46:37 snappi_api.info                          L1419 INFO   | Flows clear statistics 12.171s
25/11/2025 08:46:37 snappi_api.info                          L1419 INFO   | Captures start 0.000s
25/11/2025 08:46:41 snappi_api.info                          L1419 INFO   | Flows start 3.011s
25/11/2025 08:46:41 traffic_generation.run_traffic           L0643 INFO   | Issuing a warm reboot on the dut crdc-garnet-sonic-ud
25/11/2025 08:46:41 parallel_utils.wrapper                   L0326 INFO   | Running reboot via synchronized decorator
25/11/2025 08:46:41 parallel_utils.wrapper                   L0335 INFO   | Running original reboot as par_followers is -1
25/11/2025 08:46:41 reboot.reboot                            L0333 INFO   | Reboot crdc-garnet-sonic-ud: wait[0.01], timeout[300]
25/11/2025 08:46:41 reboot.reboot                            L0335 INFO   | DUT crdc-garnet-sonic-ud create a file /dev/shm/test_reboot before rebooting
25/11/2025 08:46:41 reboot.reboot                            L0338 INFO   | DUT OS Version: 202505.58-dirty-20251117.113556
25/11/2025 08:46:41 dut_utils.creds_on_dut                   L0487 INFO   | dut crdc-garnet-sonic-ud belongs to groups ['snappi-sonic', 'sonic', 'sonic_juniper_qfx5241', 'fanout']
25/11/2025 08:46:41 dut_utils.creds_on_dut                   L0512 INFO   | skip empty var file /data/harshit/sonic-mgmt/tests/common/helpers/../../../ansible/group_vars/all/env.yml
25/11/2025 08:46:41 dut_utils.creds_on_dut                   L0512 INFO   | skip empty var file /data/harshit/sonic-mgmt/tests/common/helpers/../../../ansible/group_vars/all/corefile_uploader.yml
25/11/2025 08:46:42 transport._log                           L1873 INFO   | Connected (version 2.0, client OpenSSH_9.2p1)
25/11/2025 08:46:42 transport._log                           L1873 INFO   | Auth banner: b'Debian GNU/Linux 12 \\n \\l\n\n'
25/11/2025 08:46:42 transport._log                           L1873 INFO   | Authentication (password) successful!
25/11/2025 08:46:42 reboot.try_create_dut_console            L0680 WARNING| Fail to create dut console. Please check console config or if console works ro not. 'ManagementIp'
25/11/2025 08:46:42 reboot.collect_console_log               L0695 WARNING| dut console is not ready, we cannot get log by console
25/11/2025 08:46:48 reboot.wait_for_shutdown                 L0186 INFO   | waiting for ssh to drop on crdc-garnet-sonic-ud
25/11/2025 08:46:48 reboot.execute_reboot_command            L0230 INFO   | rebooting crdc-garnet-sonic-ud with command "warm-reboot"
25/11/2025 08:47:46 reboot.wait_for_startup                  L0207 INFO   | waiting for ssh to startup on crdc-garnet-sonic-ud
25/11/2025 08:47:46 reboot.ssh_connection_with_retry         L0736 INFO   | Checking ssh connection using the following params: {'host_ip': '10.207.64.65', 'port': 22, 'delay': 0, 'timeout': 300, 'search_regex': 'OpenSSH_[\\w\\.]+ Debian'}
25/11/2025 08:48:22 reboot.ssh_connection_with_retry         L0742 INFO   | Connection succeeded
25/11/2025 08:48:22 reboot.wait_for_startup                  L0222 INFO   | ssh has started up on crdc-garnet-sonic-ud
25/11/2025 08:48:22 traffic_generation.run_traffic           L0654 INFO   | Polling DUT for traffic statistics for 23 seconds ...
25/11/2025 08:49:22 __init__.pytest_runtest_call             L0040 ERROR  | Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/_pytest/python.py", line 1788, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_hooks.py", line 513, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_callers.py", line 139, in _multicall
    raise exception.with_traceback(exception.__traceback__)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_callers.py", line 103, in _multicall
    res = hook_impl.function(*args)
  File "/usr/local/lib/python3.8/dist-packages/_pytest/python.py", line 194, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File "/data/harshit/sonic-mgmt/tests/snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py", line 80, in test_pfc_pause_single_lossless_prio_reboot
    run_pfc_test(api=snappi_api,
  File "/data/harshit/sonic-mgmt/tests/snappi_tests/pfc/files/helper.py", line 255, in run_pfc_test
    tgen_flow_stats, switch_flow_stats, in_flight_flow_metrics = run_traffic(duthost=duthost,
  File "/data/harshit/sonic-mgmt/tests/common/snappi_tests/traffic_generation.py", line 666, in run_traffic
    switch_device_results["tx_frames"][lossless_prio].append(get_egress_queue_count(duthost, switch_tx_port,
  File "/data/harshit/sonic-mgmt/tests/common/snappi_tests/common_helpers.py", line 1123, in get_egress_queue_count
    raw_out = duthost.shell("show queue counters {} | sed -n '/UC{}/p'".format(port, priority))['stdout']
  File "/data/harshit/sonic-mgmt/tests/common/devices/multi_asic.py", line 151, in _run_on_asics
    return getattr(self.sonichost, self.multi_asic_attr)(*module_args, **complex_args)
  File "/data/harshit/sonic-mgmt/tests/common/devices/base.py", line 107, in _run
    res = self.module(*module_args, **complex_args)[self.hostname]
  File "/usr/local/lib/python3.8/dist-packages/pytest_ansible/module_dispatcher/v213.py", line 232, in _run
    raise AnsibleConnectionFailure(
pytest_ansible.errors.AnsibleConnectionFailure: Host unreachable in the inventory

25/11/2025 08:49:23 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_single_lossless_prio_reboot[warm] teardown ====================
25/11/2025 08:49:28 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 61.0 MB, holding: 54525952.0 bytes, small: 0.0 bytes, ordinary: 9466880.0 bytes
25/11/2025 08:49:29 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 40.0 MB, holding: 35651584.0 bytes, small: 0.0 bytes, ordinary: 6262784.0 bytes
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: monit-memory_usage
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for monit-memory_usage due to zero value
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-bgpd
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for top:bgpd: 128.0
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-zebra
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for top:zebra: 128.0
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: free-used
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for free:used: 850.8
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-snmp
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-snmp due to zero value
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-pmon
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-pmon due to zero value
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-lldp
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-lldp due to zero value
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-gnmi
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-gnmi due to zero value
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-radv
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:radv: 3.0
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:radv: 2.0
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-syncd
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:syncd: 18.0
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:syncd: 5.0
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-bgp
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:bgp: 14.0
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:bgp: 4.0
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-teamd
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:teamd: 5.0
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:teamd: 2.0
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-swss
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:swss: 8.0
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:swss: 3.0
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-database
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:database: 6.0
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:database: 2.0
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_bgp-used
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_bgp:used: 256.0
25/11/2025 08:49:29 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [30.5, 64.0])
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_bgp:used: 64.0
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_zebra-used
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_zebra:used: 128.0
25/11/2025 08:49:29 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [19.9, 64.0])
25/11/2025 08:49:29 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_zebra:used: 64.0
25/11/2025 08:49:29 __init__.pytest_runtest_teardown         L0124 INFO   | After test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {'memory_usage': 13.5}, 'top': {'zebra': 43.4, 'bgpd': 75.5}, 'free': {'used': 4254}, 'docker': {'snmp': 0.2, 'pmon': 0.8, 'lldp': 0.2, 'gnmi': 0.5, 'bgp': 0.6, 'radv': 0.1, 'syncd': 4.1, 'teamd': 0.1, 'swss': 0.4, 'database': 0.3}, 'frr_bgp': {'used': 61.0}, 'frr_zebra': {'used': 39.7}}}, 'after_test': {'crdc-garnet-sonic-ud': {'monit': {}, 'top': {'zebra': 22.1, 'bgpd': 64.8}, 'free': {'used': 3291}, 'docker': {'bgp': 0.5, 'radv': 0.1, 'syncd': 4.7, 'teamd': 0.1, 'swss': 0.4, 'database': 0.3}, 'frr_bgp': {'used': 61.0}, 'frr_zebra': {'used': 40.0}}}}
25/11/2025 08:49:29 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_single_lossless_prio_reboot[fast] setup  ====================
25/11/2025 08:49:29 __init__.set_default                     L0053 INFO   | Completeness level not set during test execution. Setting to default level: CompletenessLevel.basic
25/11/2025 08:49:29 __init__.check_test_completeness         L0151 INFO   | Test has no defined levels. Continue without test completeness checks
25/11/2025 08:49:29 __init__.loganalyzer                     L0077 INFO   | Log analyzer is disabled
25/11/2025 08:49:29 __init__.memory_utilization              L0143 INFO   | Hostname: crdc-garnet-sonic-ud, Hwsku: Juniper-QFX5241-64-OD, Platform: x86_64-juniper_qfx5241-r0
25/11/2025 08:49:29 memory_utilization.parse_and_register_co L0365 INFO   | Loading memory monitoring commands for hwsku: Juniper-QFX5241-64-OD
25/11/2025 08:49:29 memory_utilization.register_command      L0023 INFO   | Registering command: name=monit, cmd=sudo monit validate, memory_params={'memory_usage': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 10}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 70}}}, memory_check=<function parse_monit_validate_output at 0x729141fa9940>
25/11/2025 08:49:29 memory_utilization.register_command      L0023 INFO   | Registering command: name=top, cmd=top -b -n 1, memory_params={'bgpd': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}, 'zebra': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}}, memory_check=<function parse_top_output at 0x729141fa9160>
25/11/2025 08:49:29 memory_utilization.register_command      L0023 INFO   | Registering command: name=free, cmd=free -m, memory_params={'used': {'memory_increase_threshold': {'type': 'percentage', 'value': '20%'}, 'memory_high_threshold': None}}, memory_check=<function parse_free_output at 0x729141fa98b0>
25/11/2025 08:49:29 memory_utilization.register_command      L0023 INFO   | Registering command: name=docker, cmd=docker stats --no-stream, memory_params={'snmp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'pmon': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'lldp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'gnmi': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}, 'radv': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 3}}, 'syncd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 5}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 18}}, 'bgp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 4}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 14}}, 'teamd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 5}}, 'swss': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 3}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'database': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}}, memory_check=<function parse_docker_stats_output at 0x729141fa99d0>
25/11/2025 08:49:29 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_bgp, cmd=vtysh -c "show memory bgp", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 256}}}, memory_check=<function parse_frr_memory_output at 0x729141fa9a60>
25/11/2025 08:49:29 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_zebra, cmd=vtysh -c "show memory zebra", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 128}}}, memory_check=<function parse_frr_memory_output at 0x729141fa9a60>
25/11/2025 08:49:29 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture snappi_testbed_config setup starts --------------------
25/11/2025 08:49:30 snappi_fixtures.snappi_testbed_config    L0526 INFO   | Configuring TGEN L1: link_training=False, rs_fec=False (DUT derived)
25/11/2025 08:49:34 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture snappi_testbed_config setup ends --------------------
25/11/2025 08:49:34 conftest.setup_dualtor_mux_ports         L3494 INFO   | skip setup dualtor mux cables on non-dualtor testbed
25/11/2025 08:49:39 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 61.0 MB, holding: 54525952.0 bytes, small: 0.0 bytes, ordinary: 9466880.0 bytes
25/11/2025 08:49:40 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 40.0 MB, holding: 35651584.0 bytes, small: 0.0 bytes, ordinary: 6262784.0 bytes
25/11/2025 08:49:40 __init__.pytest_runtest_setup            L0064 INFO   | Before test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {}, 'top': {'zebra': 22.1, 'bgpd': 64.8}, 'free': {'used': 3263}, 'docker': {'bgp': 0.5, 'radv': 0.1, 'syncd': 4.7, 'teamd': 0.1, 'swss': 0.4, 'database': 0.3}, 'frr_bgp': {'used': 61.0}, 'frr_zebra': {'used': 40.0}}}, 'after_test': {'crdc-garnet-sonic-ud': {}}}
25/11/2025 08:49:40 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_single_lossless_prio_reboot[fast] call ====================
25/11/2025 08:49:40 sonic.get_asic_name                      L1860 INFO   | asic: th5
25/11/2025 08:49:40 helper.skip_warm_reboot                  L0050 INFO   | Reboot type fast is  supported on broadcom switches
25/11/2025 08:49:54 snappi_api.info                          L1419 INFO   | Config validation 0.015s
25/11/2025 08:49:55 snappi_api.info                          L1419 INFO   | Ports configuration 0.323s
25/11/2025 08:49:56 snappi_api.info                          L1419 INFO   | Captures configuration 0.188s
25/11/2025 08:49:56 snappi_api.info                          L1419 INFO   | Location hosts ready [10.207.65.42] 0.107s
25/11/2025 08:49:56 snappi_api.info                          L1419 INFO   | Speed change not require due to redundant Layer1 config
25/11/2025 08:49:56 snappi_api.info                          L1419 INFO   | Aggregation mode speed change 0.020s
25/11/2025 08:49:57 snappi_api.info                          L1419 INFO   | Location preemption [10.207.65.42;1;1, 10.207.65.42;1;2, 10.207.65.42;1;3, 10.207.65.42;1;4] 0.132s
25/11/2025 08:49:57 snappi_api.info                          L1419 INFO   | Location connect [Port 0, Port 1, Port 2, Port 3] 0.109s
25/11/2025 08:49:58 snappi_api.info                          L1419 INFO   | Location state check [Port 0, Port 1, Port 2, Port 3] 0.312s
25/11/2025 08:49:58 snappi_api.info                          L1419 INFO   | Location configuration 2.219s
25/11/2025 08:50:06 snappi_api.info                          L1419 INFO   | Layer1 configuration 8.476s
25/11/2025 08:50:06 snappi_api.info                          L1419 INFO   | Lag Configuration 0.096s
25/11/2025 08:50:07 snappi_api.info                          L1419 INFO   | Convert device config : 0.828s
25/11/2025 08:50:07 snappi_api.info                          L1419 INFO   | Create IxNetwork device config : 0.001s
25/11/2025 08:50:08 snappi_api.info                          L1419 INFO   | Push IxNetwork device config : 0.373s
25/11/2025 08:50:08 snappi_api.info                          L1419 INFO   | Devices configuration 1.299s
25/11/2025 08:50:17 snappi_api.info                          L1419 INFO   | Flows configuration 9.501s
25/11/2025 08:50:20 snappi_api.info                          L1419 INFO   | Start interfaces 2.825s
25/11/2025 08:50:21 snappi_api.info                          L1419 INFO   | IxNet - The Traffic Item was modified. Please perform a Traffic Generate to update the associated traffic Flow Groups
25/11/2025 08:50:21 snappi_api.info                          L1419 INFO   | IxNet - Only 9% of Disk is now available. Please ensure enough Disk space is available for Linux API Server. You can try deleting old logs and diagnostics in SETTINGS -> Remove Old Logs
25/11/2025 08:50:21 traffic_generation.run_traffic           L0616 INFO   | Wait for Arp to Resolve ...
25/11/2025 08:50:29 traffic_generation.run_traffic           L0638 INFO   | Starting transmit on all flows ...
25/11/2025 08:50:33 snappi_api.info                          L1419 INFO   | Flows generate/apply 2.539s
25/11/2025 08:50:43 snappi_api.info                          L1419 INFO   | Flows clear statistics 10.304s
25/11/2025 08:50:43 snappi_api.info                          L1419 INFO   | Captures start 0.000s
25/11/2025 08:50:47 snappi_api.info                          L1419 INFO   | Flows start 3.560s
25/11/2025 08:50:47 traffic_generation.run_traffic           L0643 INFO   | Issuing a fast reboot on the dut crdc-garnet-sonic-ud
25/11/2025 08:50:47 parallel_utils.wrapper                   L0326 INFO   | Running reboot via synchronized decorator
25/11/2025 08:50:47 parallel_utils.wrapper                   L0335 INFO   | Running original reboot as par_followers is -1
25/11/2025 08:50:47 reboot.reboot                            L0333 INFO   | Reboot crdc-garnet-sonic-ud: wait[0.01], timeout[180]
25/11/2025 08:50:47 reboot.reboot                            L0335 INFO   | DUT crdc-garnet-sonic-ud create a file /dev/shm/test_reboot before rebooting
25/11/2025 08:50:48 reboot.reboot                            L0338 INFO   | DUT OS Version: 202505.58-dirty-20251117.113556
25/11/2025 08:50:48 dut_utils.creds_on_dut                   L0487 INFO   | dut crdc-garnet-sonic-ud belongs to groups ['snappi-sonic', 'sonic', 'sonic_juniper_qfx5241', 'fanout']
25/11/2025 08:50:48 dut_utils.creds_on_dut                   L0512 INFO   | skip empty var file /data/harshit/sonic-mgmt/tests/common/helpers/../../../ansible/group_vars/all/env.yml
25/11/2025 08:50:48 dut_utils.creds_on_dut                   L0512 INFO   | skip empty var file /data/harshit/sonic-mgmt/tests/common/helpers/../../../ansible/group_vars/all/corefile_uploader.yml
25/11/2025 08:50:54 reboot.wait_for_shutdown                 L0186 INFO   | waiting for ssh to drop on crdc-garnet-sonic-ud
25/11/2025 08:50:54 reboot.execute_reboot_command            L0230 INFO   | rebooting crdc-garnet-sonic-ud with command "fast-reboot"
25/11/2025 08:50:58 utilities._paramiko_ssh                  L1338 INFO   | Cannot access device 10.207.64.65 via ssh, error: timed out
25/11/2025 08:50:58 utilities._paramiko_ssh                  L1338 INFO   | Cannot access device None via ssh, error: [Errno None] Unable to connect to port 22 on 127.0.0.1 or ::1
25/11/2025 08:51:00 reboot.wait_for_startup                  L0207 INFO   | waiting for ssh to startup on crdc-garnet-sonic-ud
25/11/2025 08:51:00 reboot.ssh_connection_with_retry         L0736 INFO   | Checking ssh connection using the following params: {'host_ip': '10.207.64.65', 'port': 22, 'delay': 0, 'timeout': 180, 'search_regex': 'OpenSSH_[\\w\\.]+ Debian'}
25/11/2025 08:51:18 reboot.ssh_connection_with_retry         L0742 INFO   | Connection succeeded
25/11/2025 08:51:18 reboot.wait_for_startup                  L0222 INFO   | ssh has started up on crdc-garnet-sonic-ud
25/11/2025 08:51:18 reboot.reboot                            L0371 WARNING| Failed to get console thread result: [Errno None] Unable to connect to port 22 on 127.0.0.1 or ::1
25/11/2025 08:51:18 traffic_generation.run_traffic           L0654 INFO   | Polling DUT for traffic statistics for 27 seconds ...
25/11/2025 08:52:49 __init__.pytest_runtest_call             L0040 ERROR  | Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/_pytest/python.py", line 1788, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_hooks.py", line 513, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_callers.py", line 139, in _multicall
    raise exception.with_traceback(exception.__traceback__)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_callers.py", line 103, in _multicall
    res = hook_impl.function(*args)
  File "/usr/local/lib/python3.8/dist-packages/_pytest/python.py", line 194, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File "/data/harshit/sonic-mgmt/tests/snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py", line 80, in test_pfc_pause_single_lossless_prio_reboot
    run_pfc_test(api=snappi_api,
  File "/data/harshit/sonic-mgmt/tests/snappi_tests/pfc/files/helper.py", line 255, in run_pfc_test
    tgen_flow_stats, switch_flow_stats, in_flight_flow_metrics = run_traffic(duthost=duthost,
  File "/data/harshit/sonic-mgmt/tests/common/snappi_tests/traffic_generation.py", line 666, in run_traffic
    switch_device_results["tx_frames"][lossless_prio].append(get_egress_queue_count(duthost, switch_tx_port,
  File "/data/harshit/sonic-mgmt/tests/common/snappi_tests/common_helpers.py", line 1123, in get_egress_queue_count
    raw_out = duthost.shell("show queue counters {} | sed -n '/UC{}/p'".format(port, priority))['stdout']
  File "/data/harshit/sonic-mgmt/tests/common/devices/multi_asic.py", line 151, in _run_on_asics
    return getattr(self.sonichost, self.multi_asic_attr)(*module_args, **complex_args)
  File "/data/harshit/sonic-mgmt/tests/common/devices/base.py", line 134, in _run
    raise RunAnsibleModuleFail("run module {} failed".format(self.module_name), res)
tests.common.errors.RunAnsibleModuleFail: run module shell failed, Ansible Results =>
failed = True
msg = Timeout (62s) waiting for privilege escalation prompt: 
_ansible_no_log = False
stdout =
stderr =


25/11/2025 08:52:49 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_single_lossless_prio_reboot[fast] teardown ====================
25/11/2025 08:53:43 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 61.0 MB, holding: 54525952.0 bytes, small: 0.0 bytes, ordinary: 9420800.0 bytes
25/11/2025 08:53:44 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 39.9 MB, holding: 35651584.0 bytes, small: 0.0 bytes, ordinary: 6149120.0 bytes
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: monit-memory_usage
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for monit-memory_usage due to zero value
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-bgpd
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for top:bgpd: 128.0
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-zebra
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for top:zebra: 128.0
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: free-used
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for free:used: 652.6
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-snmp
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-snmp due to zero value
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-pmon
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-pmon due to zero value
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-lldp
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-lldp due to zero value
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-gnmi
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-gnmi due to zero value
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-radv
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:radv: 3.0
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:radv: 2.0
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-syncd
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:syncd: 18.0
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:syncd: 5.0
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-bgp
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:bgp: 14.0
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:bgp: 4.0
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-teamd
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:teamd: 5.0
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:teamd: 2.0
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-swss
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:swss: 8.0
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:swss: 3.0
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-database
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:database: 6.0
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:database: 2.0
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_bgp-used
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_bgp:used: 256.0
25/11/2025 08:53:44 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [30.5, 64.0])
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_bgp:used: 64.0
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_zebra-used
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_zebra:used: 128.0
25/11/2025 08:53:44 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [20.0, 64.0])
25/11/2025 08:53:44 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_zebra:used: 64.0
25/11/2025 08:53:44 __init__.pytest_runtest_teardown         L0124 INFO   | After test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {}, 'top': {'zebra': 22.1, 'bgpd': 64.8}, 'free': {'used': 3263}, 'docker': {'bgp': 0.5, 'radv': 0.1, 'syncd': 4.7, 'teamd': 0.1, 'swss': 0.4, 'database': 0.3}, 'frr_bgp': {'used': 61.0}, 'frr_zebra': {'used': 40.0}}}, 'after_test': {'crdc-garnet-sonic-ud': {'monit': {}, 'top': {'zebra': 19.4, 'bgpd': 64.8}, 'free': {'used': 3047}, 'docker': {'bgp': 0.5, 'radv': 0.1, 'syncd': 4.0, 'teamd': 0.1, 'swss': 0.5, 'database': 0.3}, 'frr_bgp': {'used': 61.0}, 'frr_zebra': {'used': 39.9}}}}
25/11/2025 08:53:44 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_multi_lossless_prio_reboot[warm] setup  ====================
25/11/2025 08:53:44 __init__.set_default                     L0053 INFO   | Completeness level not set during test execution. Setting to default level: CompletenessLevel.basic
25/11/2025 08:53:44 __init__.check_test_completeness         L0151 INFO   | Test has no defined levels. Continue without test completeness checks
25/11/2025 08:53:44 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture lossless_prio_list setup starts --------------------
25/11/2025 08:53:45 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture lossless_prio_list setup ends --------------------
25/11/2025 08:53:45 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture lossy_prio_list setup starts --------------------
25/11/2025 08:53:45 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture lossy_prio_list setup ends --------------------
25/11/2025 08:53:45 __init__.loganalyzer                     L0077 INFO   | Log analyzer is disabled
25/11/2025 08:53:45 __init__.memory_utilization              L0143 INFO   | Hostname: crdc-garnet-sonic-ud, Hwsku: Juniper-QFX5241-64-OD, Platform: x86_64-juniper_qfx5241-r0
25/11/2025 08:53:45 memory_utilization.parse_and_register_co L0365 INFO   | Loading memory monitoring commands for hwsku: Juniper-QFX5241-64-OD
25/11/2025 08:53:45 memory_utilization.register_command      L0023 INFO   | Registering command: name=monit, cmd=sudo monit validate, memory_params={'memory_usage': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 10}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 70}}}, memory_check=<function parse_monit_validate_output at 0x729141fa9940>
25/11/2025 08:53:45 memory_utilization.register_command      L0023 INFO   | Registering command: name=top, cmd=top -b -n 1, memory_params={'bgpd': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}, 'zebra': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}}, memory_check=<function parse_top_output at 0x729141fa9160>
25/11/2025 08:53:45 memory_utilization.register_command      L0023 INFO   | Registering command: name=free, cmd=free -m, memory_params={'used': {'memory_increase_threshold': {'type': 'percentage', 'value': '20%'}, 'memory_high_threshold': None}}, memory_check=<function parse_free_output at 0x729141fa98b0>
25/11/2025 08:53:45 memory_utilization.register_command      L0023 INFO   | Registering command: name=docker, cmd=docker stats --no-stream, memory_params={'snmp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'pmon': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'lldp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'gnmi': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}, 'radv': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 3}}, 'syncd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 5}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 18}}, 'bgp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 4}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 14}}, 'teamd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 5}}, 'swss': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 3}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'database': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}}, memory_check=<function parse_docker_stats_output at 0x729141fa99d0>
25/11/2025 08:53:45 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_bgp, cmd=vtysh -c "show memory bgp", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 256}}}, memory_check=<function parse_frr_memory_output at 0x729141fa9a60>
25/11/2025 08:53:45 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_zebra, cmd=vtysh -c "show memory zebra", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 128}}}, memory_check=<function parse_frr_memory_output at 0x729141fa9a60>
25/11/2025 08:53:45 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture snappi_testbed_config setup starts --------------------
25/11/2025 08:53:46 snappi_fixtures.snappi_testbed_config    L0526 INFO   | Configuring TGEN L1: link_training=False, rs_fec=False (DUT derived)
25/11/2025 08:53:48 multi_asic.get_dut_iface_mac             L0185 ERROR  | Failed to get MAC address for interface "Ethernet420", exception: run module command failed, Ansible Results =>
failed = True
changed = True
rc = 1
cmd = ['cat', '/sys/class/net/Ethernet420/address']
start = 2025-06-26 15:00:38.134612
end = 2025-06-26 15:00:38.140077
delta = 0:00:00.005465
msg = non-zero return code
invocation = {'module_args': {'_raw_params': '  cat /sys/class/net/Ethernet420/address', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}
_ansible_no_log = None
stdout =
stderr =
cat: /sys/class/net/Ethernet420/address: No such file or directory
25/11/2025 08:53:49 multi_asic.get_dut_iface_mac             L0185 ERROR  | Failed to get MAC address for interface "Ethernet428", exception: run module command failed, Ansible Results =>
failed = True
changed = True
rc = 1
cmd = ['cat', '/sys/class/net/Ethernet428/address']
start = 2025-06-26 15:00:38.537682
end = 2025-06-26 15:00:38.543152
delta = 0:00:00.005470
msg = non-zero return code
invocation = {'module_args': {'_raw_params': '  cat /sys/class/net/Ethernet428/address', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}
_ansible_no_log = None
stdout =
stderr =
cat: /sys/class/net/Ethernet428/address: No such file or directory
25/11/2025 08:53:49 multi_asic.get_dut_iface_mac             L0185 ERROR  | Failed to get MAC address for interface "Ethernet436", exception: run module command failed, Ansible Results =>
failed = True
changed = True
rc = 1
cmd = ['cat', '/sys/class/net/Ethernet436/address']
start = 2025-06-26 15:00:38.931534
end = 2025-06-26 15:00:38.937072
delta = 0:00:00.005538
msg = non-zero return code
invocation = {'module_args': {'_raw_params': '  cat /sys/class/net/Ethernet436/address', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}
_ansible_no_log = None
stdout =
stderr =
cat: /sys/class/net/Ethernet436/address: No such file or directory
25/11/2025 08:53:50 multi_asic.get_dut_iface_mac             L0185 ERROR  | Failed to get MAC address for interface "Ethernet444", exception: run module command failed, Ansible Results =>
failed = True
changed = True
rc = 1
cmd = ['cat', '/sys/class/net/Ethernet444/address']
start = 2025-06-26 15:00:39.325307
end = 2025-06-26 15:00:39.330785
delta = 0:00:00.005478
msg = non-zero return code
invocation = {'module_args': {'_raw_params': '  cat /sys/class/net/Ethernet444/address', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}
_ansible_no_log = None
stdout =
stderr =
cat: /sys/class/net/Ethernet444/address: No such file or directory
25/11/2025 08:53:50 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture snappi_testbed_config setup ends --------------------
25/11/2025 08:53:50 conftest.setup_dualtor_mux_ports         L3494 INFO   | skip setup dualtor mux cables on non-dualtor testbed
25/11/2025 08:53:55 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 61.0 MB, holding: 54525952.0 bytes, small: 0.0 bytes, ordinary: 9420800.0 bytes
25/11/2025 08:53:56 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 39.9 MB, holding: 35651584.0 bytes, small: 0.0 bytes, ordinary: 6149120.0 bytes
25/11/2025 08:53:56 __init__.pytest_runtest_setup            L0064 INFO   | Before test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {}, 'top': {'zebra': 19.4, 'bgpd': 64.8}, 'free': {'used': 3037}, 'docker': {'bgp': 0.5, 'radv': 0.1, 'syncd': 4.0, 'teamd': 0.1, 'swss': 0.5, 'database': 0.3}, 'frr_bgp': {'used': 61.0}, 'frr_zebra': {'used': 39.9}}}, 'after_test': {'crdc-garnet-sonic-ud': {}}}
25/11/2025 08:53:56 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_multi_lossless_prio_reboot[warm] call ====================
25/11/2025 08:53:56 sonic.get_asic_name                      L1860 INFO   | asic: th5
25/11/2025 08:53:56 helper.skip_warm_reboot                  L0050 INFO   | Reboot type warm is  supported on broadcom switches
25/11/2025 08:54:08 snappi_api.info                          L1419 INFO   | Config validation 0.019s
25/11/2025 08:54:10 snappi_api.info                          L1419 INFO   | Ports configuration 0.268s
25/11/2025 08:54:10 snappi_api.info                          L1419 INFO   | Captures configuration 0.215s
25/11/2025 08:54:11 snappi_api.info                          L1419 INFO   | Location hosts ready [10.207.65.42] 0.108s
25/11/2025 08:54:11 snappi_api.info                          L1419 INFO   | Speed change not require due to redundant Layer1 config
25/11/2025 08:54:11 snappi_api.info                          L1419 INFO   | Aggregation mode speed change 0.021s
25/11/2025 08:54:12 snappi_api.info                          L1419 INFO   | Location preemption [10.207.65.42;1;1, 10.207.65.42;1;2, 10.207.65.42;1;3, 10.207.65.42;1;4] 0.133s
25/11/2025 08:54:12 snappi_api.info                          L1419 INFO   | Location connect [Port 0, Port 1, Port 2, Port 3] 0.110s
25/11/2025 08:54:12 snappi_api.info                          L1419 INFO   | Location state check [Port 0, Port 1, Port 2, Port 3] 0.270s
25/11/2025 08:54:12 snappi_api.info                          L1419 INFO   | Location configuration 2.029s
25/11/2025 08:54:21 snappi_api.info                          L1419 INFO   | Layer1 configuration 8.597s
25/11/2025 08:54:21 snappi_api.info                          L1419 INFO   | Lag Configuration 0.097s
25/11/2025 08:54:22 snappi_api.info                          L1419 INFO   | Convert device config : 0.784s
25/11/2025 08:54:22 snappi_api.info                          L1419 INFO   | Create IxNetwork device config : 0.001s
25/11/2025 08:54:22 snappi_api.info                          L1419 INFO   | Push IxNetwork device config : 0.332s
25/11/2025 08:54:22 snappi_api.info                          L1419 INFO   | Devices configuration 1.215s
25/11/2025 08:54:32 snappi_api.info                          L1419 INFO   | Flows configuration 9.694s
25/11/2025 08:54:35 snappi_api.info                          L1419 INFO   | Start interfaces 2.924s
25/11/2025 08:54:36 snappi_api.info                          L1419 INFO   | IxNet - The Traffic Item was modified. Please perform a Traffic Generate to update the associated traffic Flow Groups
25/11/2025 08:54:36 traffic_generation.run_traffic           L0616 INFO   | Wait for Arp to Resolve ...
25/11/2025 08:57:08 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_multi_lossless_prio_reboot[warm] teardown ====================
25/11/2025 08:57:13 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 61.0 MB, holding: 54525952.0 bytes, small: 0.0 bytes, ordinary: 9421824.0 bytes
25/11/2025 08:57:14 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 39.9 MB, holding: 35651584.0 bytes, small: 0.0 bytes, ordinary: 6151168.0 bytes
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: monit-memory_usage
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for monit-memory_usage due to zero value
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-bgpd
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for top:bgpd: 128.0
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-zebra
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for top:zebra: 128.0
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: free-used
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for free:used: 607.4
25/11/2025 08:57:14 memory_utilization._handle_memory_thresh L0252 INFO   | free:used, previous_values: {'monit': {}, 'top': {'zebra': 19.4, 'bgpd': 64.8}, 'free': {'used': 3037}, 'docker': {'bgp': 0.5, 'radv': 0.1, 'syncd': 4.0, 'teamd': 0.1, 'swss': 0.5, 'database': 0.3}, 'frr_bgp': {'used': 61.0}, 'frr_zebra': {'used': 39.9}}
25/11/2025 08:57:14 memory_utilization._handle_memory_thresh L0253 INFO   | free:used, current_values: {'monit': {'memory_usage': 12.1}, 'top': {'zebra': 19.4, 'bgpd': 64.8}, 'free': {'used': 3842}, 'docker': {'snmp': 0.2, 'pmon': 0.8, 'lldp': 0.2, 'gnmi': 0.3, 'bgp': 0.5, 'radv': 0.1, 'syncd': 4.0, 'teamd': 0.1, 'swss': 0.3, 'database': 0.3}, 'frr_bgp': {'used': 61.0}, 'frr_zebra': {'used': 39.9}}
25/11/2025 08:57:14 memory_utilization._handle_memory_thresh L0313 ERROR  | [ALARM]: free:used memory usage increased by 805.0%, exceeds increase threshold 20%% (previous: 3037.0 MB, current: 3842.0 MB)
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-snmp
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-snmp due to zero value
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-pmon
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-pmon due to zero value
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-lldp
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-lldp due to zero value
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-gnmi
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-gnmi due to zero value
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-radv
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:radv: 3.0
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:radv: 2.0
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-syncd
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:syncd: 18.0
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:syncd: 5.0
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-bgp
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:bgp: 14.0
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:bgp: 4.0
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-teamd
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:teamd: 5.0
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:teamd: 2.0
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-swss
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:swss: 8.0
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:swss: 3.0
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-database
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for docker:database: 6.0
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for docker:database: 2.0
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_bgp-used
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_bgp:used: 256.0
25/11/2025 08:57:14 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [30.5, 64.0])
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_bgp:used: 64.0
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_zebra-used
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_zebra:used: 128.0
25/11/2025 08:57:14 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [19.9, 64.0])
25/11/2025 08:57:14 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_zebra:used: 64.0
25/11/2025 08:57:14 __init__.pytest_runtest_teardown         L0122 ERROR  | Memory errors detected: [ALARM]: free:used memory usage increased by 805.0%, exceeds increase threshold 20%% (previous: 3037.0 MB, current: 3842.0 MB)
25/11/2025 08:57:14 __init__.pytest_runtest_teardown         L0124 INFO   | After test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {}, 'top': {'zebra': 19.4, 'bgpd': 64.8}, 'free': {'used': 3037}, 'docker': {'bgp': 0.5, 'radv': 0.1, 'syncd': 4.0, 'teamd': 0.1, 'swss': 0.5, 'database': 0.3}, 'frr_bgp': {'used': 61.0}, 'frr_zebra': {'used': 39.9}}}, 'after_test': {'crdc-garnet-sonic-ud': {'monit': {'memory_usage': 12.1}, 'top': {'zebra': 19.4, 'bgpd': 64.8}, 'free': {'used': 3842}, 'docker': {'snmp': 0.2, 'pmon': 0.8, 'lldp': 0.2, 'gnmi': 0.3, 'bgp': 0.5, 'radv': 0.1, 'syncd': 4.0, 'teamd': 0.1, 'swss': 0.3, 'database': 0.3}, 'frr_bgp': {'used': 61.0}, 'frr_zebra': {'used': 39.9}}}}
25/11/2025 08:57:14 __init__.memory_utilization              L0155 ERROR  | Memory errors detected in fixture teardown: [ALARM]: free:used memory usage increased by 805.0%, exceeds increase threshold 20%% (previous: 3037.0 MB, current: 3842.0 MB)
25/11/2025 08:57:14 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_multi_lossless_prio_reboot[fast] setup  ====================
25/11/2025 08:57:14 __init__.set_default                     L0053 INFO   | Completeness level not set during test execution. Setting to default level: CompletenessLevel.basic
25/11/2025 08:57:14 __init__.check_test_completeness         L0151 INFO   | Test has no defined levels. Continue without test completeness checks
25/11/2025 08:57:14 __init__.loganalyzer                     L0077 INFO   | Log analyzer is disabled
25/11/2025 08:57:14 __init__.memory_utilization              L0143 INFO   | Hostname: crdc-garnet-sonic-ud, Hwsku: Juniper-QFX5241-64-OD, Platform: x86_64-juniper_qfx5241-r0
25/11/2025 08:57:14 memory_utilization.parse_and_register_co L0365 INFO   | Loading memory monitoring commands for hwsku: Juniper-QFX5241-64-OD
25/11/2025 08:57:14 memory_utilization.register_command      L0023 INFO   | Registering command: name=monit, cmd=sudo monit validate, memory_params={'memory_usage': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 10}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 70}}}, memory_check=<function parse_monit_validate_output at 0x729141fa9940>
25/11/2025 08:57:14 memory_utilization.register_command      L0023 INFO   | Registering command: name=top, cmd=top -b -n 1, memory_params={'bgpd': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}, 'zebra': {'memory_increase_threshold': {'type': 'value', 'value': 128}, 'memory_high_threshold': None}}, memory_check=<function parse_top_output at 0x729141fa9160>
25/11/2025 08:57:14 memory_utilization.register_command      L0023 INFO   | Registering command: name=free, cmd=free -m, memory_params={'used': {'memory_increase_threshold': {'type': 'percentage', 'value': '20%'}, 'memory_high_threshold': None}}, memory_check=<function parse_free_output at 0x729141fa98b0>
25/11/2025 08:57:14 memory_utilization.register_command      L0023 INFO   | Registering command: name=docker, cmd=docker stats --no-stream, memory_params={'snmp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'pmon': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'lldp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 4}}, 'gnmi': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}, 'radv': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 3}}, 'syncd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 5}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 18}}, 'bgp': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 4}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 14}}, 'teamd': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 5}}, 'swss': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 3}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 8}}, 'database': {'memory_increase_threshold': {'type': 'percentage_points', 'value': 2}, 'memory_high_threshold': {'type': 'percentage_points', 'value': 6}}}, memory_check=<function parse_docker_stats_output at 0x729141fa99d0>
25/11/2025 08:57:14 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_bgp, cmd=vtysh -c "show memory bgp", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 256}}}, memory_check=<function parse_frr_memory_output at 0x729141fa9a60>
25/11/2025 08:57:14 memory_utilization.register_command      L0023 INFO   | Registering command: name=frr_zebra, cmd=vtysh -c "show memory zebra", memory_params={'used': {'memory_increase_threshold': [{'type': 'percentage', 'value': '50%'}, {'type': 'value', 'value': 64}, {'type': 'comparison', 'value': 'max'}], 'memory_high_threshold': {'type': 'value', 'value': 128}}}, memory_check=<function parse_frr_memory_output at 0x729141fa9a60>
25/11/2025 08:57:14 __init__._fixture_func_decorator         L0069 INFO   | -------------------- fixture snappi_testbed_config setup starts --------------------
25/11/2025 08:57:15 snappi_fixtures.snappi_testbed_config    L0526 INFO   | Configuring TGEN L1: link_training=False, rs_fec=False (DUT derived)
25/11/2025 08:57:17 multi_asic.get_dut_iface_mac             L0185 ERROR  | Failed to get MAC address for interface "Ethernet420", exception: run module command failed, Ansible Results =>
failed = True
changed = True
rc = 1
cmd = ['cat', '/sys/class/net/Ethernet420/address']
start = 2025-06-26 15:04:07.056720
end = 2025-06-26 15:04:07.062286
delta = 0:00:00.005566
msg = non-zero return code
invocation = {'module_args': {'_raw_params': '  cat /sys/class/net/Ethernet420/address', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}
_ansible_no_log = None
stdout =
stderr =
cat: /sys/class/net/Ethernet420/address: No such file or directory
25/11/2025 08:57:18 multi_asic.get_dut_iface_mac             L0185 ERROR  | Failed to get MAC address for interface "Ethernet428", exception: run module command failed, Ansible Results =>
failed = True
changed = True
rc = 1
cmd = ['cat', '/sys/class/net/Ethernet428/address']
start = 2025-06-26 15:04:07.450125
end = 2025-06-26 15:04:07.455732
delta = 0:00:00.005607
msg = non-zero return code
invocation = {'module_args': {'_raw_params': '  cat /sys/class/net/Ethernet428/address', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}
_ansible_no_log = None
stdout =
stderr =
cat: /sys/class/net/Ethernet428/address: No such file or directory
25/11/2025 08:57:18 multi_asic.get_dut_iface_mac             L0185 ERROR  | Failed to get MAC address for interface "Ethernet436", exception: run module command failed, Ansible Results =>
failed = True
changed = True
rc = 1
cmd = ['cat', '/sys/class/net/Ethernet436/address']
start = 2025-06-26 15:04:07.843546
end = 2025-06-26 15:04:07.849108
delta = 0:00:00.005562
msg = non-zero return code
invocation = {'module_args': {'_raw_params': '  cat /sys/class/net/Ethernet436/address', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}
_ansible_no_log = None
stdout =
stderr =
cat: /sys/class/net/Ethernet436/address: No such file or directory
25/11/2025 08:57:19 multi_asic.get_dut_iface_mac             L0185 ERROR  | Failed to get MAC address for interface "Ethernet444", exception: run module command failed, Ansible Results =>
failed = True
changed = True
rc = 1
cmd = ['cat', '/sys/class/net/Ethernet444/address']
start = 2025-06-26 15:04:08.240019
end = 2025-06-26 15:04:08.245676
delta = 0:00:00.005657
msg = non-zero return code
invocation = {'module_args': {'_raw_params': '  cat /sys/class/net/Ethernet444/address', '_uses_shell': False, 'warn': False, 'stdin_add_newline': True, 'strip_empty_ends': True, 'argv': None, 'chdir': None, 'executable': None, 'creates': None, 'removes': None, 'stdin': None}}
_ansible_no_log = None
stdout =
stderr =
cat: /sys/class/net/Ethernet444/address: No such file or directory
25/11/2025 08:57:19 __init__._fixture_func_decorator         L0076 INFO   | -------------------- fixture snappi_testbed_config setup ends --------------------
25/11/2025 08:57:19 conftest.setup_dualtor_mux_ports         L3494 INFO   | skip setup dualtor mux cables on non-dualtor testbed
25/11/2025 08:57:24 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 61.0 MB, holding: 54525952.0 bytes, small: 0.0 bytes, ordinary: 9421824.0 bytes
25/11/2025 08:57:25 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 39.9 MB, holding: 35651584.0 bytes, small: 0.0 bytes, ordinary: 6151168.0 bytes
25/11/2025 08:57:25 __init__.pytest_runtest_setup            L0064 INFO   | Before test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {'memory_usage': 12.1}, 'top': {'zebra': 19.4, 'bgpd': 64.8}, 'free': {'used': 3830}, 'docker': {'snmp': 0.2, 'pmon': 0.8, 'lldp': 0.2, 'gnmi': 0.3, 'bgp': 0.5, 'radv': 0.1, 'syncd': 4.0, 'teamd': 0.1, 'swss': 0.3, 'database': 0.3}, 'frr_bgp': {'used': 61.0}, 'frr_zebra': {'used': 39.9}}}, 'after_test': {'crdc-garnet-sonic-ud': {}}}
25/11/2025 08:57:25 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_multi_lossless_prio_reboot[fast] call ====================
25/11/2025 08:57:25 sonic.get_asic_name                      L1860 INFO   | asic: th5
25/11/2025 08:57:25 helper.skip_warm_reboot                  L0050 INFO   | Reboot type fast is  supported on broadcom switches
25/11/2025 08:57:37 snappi_api.info                          L1419 INFO   | Config validation 0.017s
25/11/2025 08:57:39 snappi_api.info                          L1419 INFO   | Ports configuration 0.281s
25/11/2025 08:57:39 snappi_api.info                          L1419 INFO   | Captures configuration 0.147s
25/11/2025 08:57:39 snappi_api.info                          L1419 INFO   | Location hosts ready [10.207.65.42] 0.080s
25/11/2025 08:57:40 snappi_api.info                          L1419 INFO   | Speed change not require due to redundant Layer1 config
25/11/2025 08:57:40 snappi_api.info                          L1419 INFO   | Aggregation mode speed change 0.020s
25/11/2025 08:57:41 snappi_api.info                          L1419 INFO   | Location preemption [10.207.65.42;1;1, 10.207.65.42;1;2, 10.207.65.42;1;3, 10.207.65.42;1;4] 0.119s
25/11/2025 08:57:41 snappi_api.info                          L1419 INFO   | Location connect [Port 0, Port 1, Port 2, Port 3] 0.085s
25/11/2025 08:57:41 snappi_api.info                          L1419 INFO   | Location state check [Port 0, Port 1, Port 2, Port 3] 0.325s
25/11/2025 08:57:41 snappi_api.info                          L1419 INFO   | Location configuration 2.078s
25/11/2025 08:57:50 snappi_api.info                          L1419 INFO   | Layer1 configuration 9.100s
25/11/2025 08:57:50 snappi_api.info                          L1419 INFO   | Lag Configuration 0.095s
25/11/2025 08:57:51 snappi_api.info                          L1419 INFO   | Convert device config : 0.721s
25/11/2025 08:57:51 snappi_api.info                          L1419 INFO   | Create IxNetwork device config : 0.001s
25/11/2025 08:57:51 snappi_api.info                          L1419 INFO   | Push IxNetwork device config : 0.318s
25/11/2025 08:57:51 snappi_api.info                          L1419 INFO   | Devices configuration 1.137s
25/11/2025 08:58:01 snappi_api.info                          L1419 INFO   | Flows configuration 9.428s
25/11/2025 08:58:04 snappi_api.info                          L1419 INFO   | Start interfaces 2.802s
25/11/2025 08:58:04 snappi_api.info                          L1419 INFO   | IxNet - The Traffic Item was modified. Please perform a Traffic Generate to update the associated traffic Flow Groups
25/11/2025 08:58:04 traffic_generation.run_traffic           L0616 INFO   | Wait for Arp to Resolve ...
25/11/2025 08:59:36 traffic_generation.run_traffic           L0638 INFO   | Starting transmit on all flows ...
25/11/2025 08:59:39 snappi_api.info                          L1419 INFO   | Flows generate/apply 2.384s
25/11/2025 08:59:52 snappi_api.info                          L1419 INFO   | Flows clear statistics 12.300s
25/11/2025 08:59:52 snappi_api.info                          L1419 INFO   | Captures start 0.000s
25/11/2025 08:59:55 snappi_api.info                          L1419 INFO   | Flows start 3.546s
25/11/2025 08:59:56 traffic_generation.run_traffic           L0643 INFO   | Issuing a fast reboot on the dut crdc-garnet-sonic-ud
25/11/2025 08:59:56 parallel_utils.wrapper                   L0326 INFO   | Running reboot via synchronized decorator
25/11/2025 08:59:56 parallel_utils.wrapper                   L0335 INFO   | Running original reboot as par_followers is -1
25/11/2025 08:59:56 reboot.reboot                            L0333 INFO   | Reboot crdc-garnet-sonic-ud: wait[0.01], timeout[180]
25/11/2025 08:59:56 reboot.reboot                            L0335 INFO   | DUT crdc-garnet-sonic-ud create a file /dev/shm/test_reboot before rebooting
25/11/2025 08:59:56 reboot.reboot                            L0338 INFO   | DUT OS Version: 202505.58-dirty-20251117.113556
25/11/2025 08:59:56 dut_utils.creds_on_dut                   L0487 INFO   | dut crdc-garnet-sonic-ud belongs to groups ['snappi-sonic', 'sonic', 'sonic_juniper_qfx5241', 'fanout']
25/11/2025 08:59:56 dut_utils.creds_on_dut                   L0512 INFO   | skip empty var file /data/harshit/sonic-mgmt/tests/common/helpers/../../../ansible/group_vars/all/env.yml
25/11/2025 08:59:56 dut_utils.creds_on_dut                   L0512 INFO   | skip empty var file /data/harshit/sonic-mgmt/tests/common/helpers/../../../ansible/group_vars/all/corefile_uploader.yml
25/11/2025 08:59:57 transport._log                           L1873 INFO   | Connected (version 2.0, client OpenSSH_9.2p1)
25/11/2025 08:59:57 transport._log                           L1873 INFO   | Auth banner: b'Debian GNU/Linux 12 \\n \\l\n\n'
25/11/2025 08:59:57 transport._log                           L1873 INFO   | Authentication (password) successful!
25/11/2025 08:59:57 reboot.try_create_dut_console            L0680 WARNING| Fail to create dut console. Please check console config or if console works ro not. 'ManagementIp'
25/11/2025 08:59:57 reboot.collect_console_log               L0695 WARNING| dut console is not ready, we cannot get log by console
25/11/2025 09:00:03 reboot.wait_for_shutdown                 L0186 INFO   | waiting for ssh to drop on crdc-garnet-sonic-ud
25/11/2025 09:00:03 reboot.execute_reboot_command            L0230 INFO   | rebooting crdc-garnet-sonic-ud with command "fast-reboot"
25/11/2025 09:01:08 reboot.wait_for_startup                  L0207 INFO   | waiting for ssh to startup on crdc-garnet-sonic-ud
25/11/2025 09:01:08 reboot.ssh_connection_with_retry         L0736 INFO   | Checking ssh connection using the following params: {'host_ip': '10.207.64.65', 'port': 22, 'delay': 0, 'timeout': 180, 'search_regex': 'OpenSSH_[\\w\\.]+ Debian'}
25/11/2025 09:01:38 reboot.ssh_connection_with_retry         L0742 INFO   | Connection succeeded
25/11/2025 09:01:38 reboot.wait_for_startup                  L0222 INFO   | ssh has started up on crdc-garnet-sonic-ud
25/11/2025 09:01:38 traffic_generation.run_traffic           L0654 INFO   | Polling DUT for traffic statistics for 35 seconds ...
25/11/2025 09:02:40 __init__.pytest_runtest_call             L0040 ERROR  | Traceback (most recent call last):
  File "/usr/local/lib/python3.8/dist-packages/_pytest/python.py", line 1788, in runtest
    self.ihook.pytest_pyfunc_call(pyfuncitem=self)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_hooks.py", line 513, in __call__
    return self._hookexec(self.name, self._hookimpls.copy(), kwargs, firstresult)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_manager.py", line 120, in _hookexec
    return self._inner_hookexec(hook_name, methods, kwargs, firstresult)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_callers.py", line 139, in _multicall
    raise exception.with_traceback(exception.__traceback__)
  File "/usr/local/lib/python3.8/dist-packages/pluggy/_callers.py", line 103, in _multicall
    res = hook_impl.function(*args)
  File "/usr/local/lib/python3.8/dist-packages/_pytest/python.py", line 194, in pytest_pyfunc_call
    result = testfunction(**testargs)
  File "/data/harshit/sonic-mgmt/tests/snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py", line 154, in test_pfc_pause_multi_lossless_prio_reboot
    run_pfc_test(api=snappi_api,
  File "/data/harshit/sonic-mgmt/tests/snappi_tests/pfc/files/helper.py", line 255, in run_pfc_test
    tgen_flow_stats, switch_flow_stats, in_flight_flow_metrics = run_traffic(duthost=duthost,
  File "/data/harshit/sonic-mgmt/tests/common/snappi_tests/traffic_generation.py", line 666, in run_traffic
    switch_device_results["tx_frames"][lossless_prio].append(get_egress_queue_count(duthost, switch_tx_port,
  File "/data/harshit/sonic-mgmt/tests/common/snappi_tests/common_helpers.py", line 1123, in get_egress_queue_count
    raw_out = duthost.shell("show queue counters {} | sed -n '/UC{}/p'".format(port, priority))['stdout']
  File "/data/harshit/sonic-mgmt/tests/common/devices/multi_asic.py", line 151, in _run_on_asics
    return getattr(self.sonichost, self.multi_asic_attr)(*module_args, **complex_args)
  File "/data/harshit/sonic-mgmt/tests/common/devices/base.py", line 134, in _run
    raise RunAnsibleModuleFail("run module {} failed".format(self.module_name), res)
tests.common.errors.RunAnsibleModuleFail: run module shell failed, Ansible Results =>
failed = True
msg = Timeout (62s) waiting for privilege escalation prompt: 
_ansible_no_log = False
stdout =
stderr =


25/11/2025 09:02:40 __init__._log_sep_line                   L0170 INFO   | ==================== snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py::test_pfc_pause_multi_lossless_prio_reboot[fast] teardown ====================
25/11/2025 09:03:51 memory_utilization.execute_command       L0042 WARNING| Error executing command 'sudo monit validate': Host unreachable in the inventory
25/11/2025 09:03:51 memory_utilization.parse_monit_validate_ L0503 WARNING| Empty output for monit validate command, returning empty values
25/11/2025 09:04:51 memory_utilization.execute_command       L0042 WARNING| Error executing command 'top -b -n 1': Host unreachable in the inventory
25/11/2025 09:04:51 memory_utilization.parse_top_output      L0443 WARNING| Empty output for top command, returning empty values
25/11/2025 09:05:51 memory_utilization.execute_command       L0042 WARNING| Error executing command 'free -m': Host unreachable in the inventory
25/11/2025 09:05:51 memory_utilization.parse_free_output     L0476 WARNING| Empty output for free command, returning empty values
25/11/2025 09:06:52 memory_utilization.execute_command       L0042 WARNING| Error executing command 'docker stats --no-stream': Host unreachable in the inventory
25/11/2025 09:06:52 memory_utilization.parse_docker_stats_ou L0535 WARNING| Empty output for docker stats command, returning empty values
25/11/2025 09:06:53 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 61.0 MB, holding: 54525952.0 bytes, small: 0.0 bytes, ordinary: 9423872.0 bytes
25/11/2025 09:06:53 memory_utilization.parse_frr_memory_outp L0631 INFO   | Total FRR memory used: 39.9 MB, holding: 35651584.0 bytes, small: 0.0 bytes, ordinary: 6161408.0 bytes
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: monit-memory_usage
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for monit-memory_usage due to zero value
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-bgpd
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for top-bgpd due to zero value
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: top-zebra
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for top-zebra due to zero value
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: free-used
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for free-used due to zero value
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-snmp
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-snmp due to zero value
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-pmon
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-pmon due to zero value
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-lldp
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-lldp due to zero value
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-gnmi
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-gnmi due to zero value
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-radv
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-radv due to zero value
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-syncd
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-syncd due to zero value
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-bgp
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-bgp due to zero value
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-teamd
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-teamd due to zero value
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-swss
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-swss due to zero value
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: docker-database
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0064 WARNING| Skipping memory check for docker-database due to zero value
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_bgp-used
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_bgp:used: 256.0
25/11/2025 09:06:53 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [30.5, 64.0])
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_bgp:used: 64.0
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0053 INFO   | Checking thresholds for command: frr_zebra-used
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0075 INFO   | Calculated high threshold for frr_zebra:used: 128.0
25/11/2025 09:06:53 memory_utilization._parse_threshold      L0216 INFO   | Selected max threshold from list: 64.0 (from values: [19.9, 64.0])
25/11/2025 09:06:53 memory_utilization.check_memory_threshol L0093 INFO   | Calculated increase threshold for frr_zebra:used: 64.0
25/11/2025 09:06:53 __init__.pytest_runtest_teardown         L0124 INFO   | After test: collected memory_values {'before_test': {'crdc-garnet-sonic-ud': {'monit': {'memory_usage': 12.1}, 'top': {'zebra': 19.4, 'bgpd': 64.8}, 'free': {'used': 3830}, 'docker': {'snmp': 0.2, 'pmon': 0.8, 'lldp': 0.2, 'gnmi': 0.3, 'bgp': 0.5, 'radv': 0.1, 'syncd': 4.0, 'teamd': 0.1, 'swss': 0.3, 'database': 0.3}, 'frr_bgp': {'used': 61.0}, 'frr_zebra': {'used': 39.9}}}, 'after_test': {'crdc-garnet-sonic-ud': {'monit': {}, 'top': {}, 'free': {}, 'docker': {}, 'frr_bgp': {'used': 61.0}, 'frr_zebra': {'used': 39.9}}}}
25/11/2025 09:06:53 __init__._fixture_generator_decorator    L0093 INFO   | -------------------- fixture snappi_api teardown starts --------------------
25/11/2025 09:06:59 __init__._fixture_generator_decorator    L0102 INFO   | -------------------- fixture snappi_api teardown ends --------------------
25/11/2025 09:06:59 __init__._fixture_generator_decorator    L0093 INFO   | -------------------- fixture start_pfcwd_after_test teardown starts --------------------
25/11/2025 09:07:00 __init__._fixture_generator_decorator    L0102 INFO   | -------------------- fixture start_pfcwd_after_test teardown ends --------------------
25/11/2025 09:07:00 __init__._fixture_generator_decorator    L0093 INFO   | -------------------- fixture rand_lossy_prio teardown starts --------------------
25/11/2025 09:07:00 __init__._fixture_generator_decorator    L0102 INFO   | -------------------- fixture rand_lossy_prio teardown ends --------------------
25/11/2025 09:07:00 __init__._fixture_generator_decorator    L0093 INFO   | -------------------- fixture rand_lossless_prio teardown starts --------------------
25/11/2025 09:07:00 __init__._fixture_generator_decorator    L0102 INFO   | -------------------- fixture rand_lossless_prio teardown ends --------------------
25/11/2025 09:07:00 __init__._fixture_generator_decorator    L0093 INFO   | -------------------- fixture enable_packet_aging_after_test teardown starts --------------------
25/11/2025 09:07:00 __init__._fixture_generator_decorator    L0102 INFO   | -------------------- fixture enable_packet_aging_after_test teardown ends --------------------
25/11/2025 09:07:00 conftest.temporarily_disable_route_check L3036 INFO   | Skipping temporarily_disable_route_check fixture
25/11/2025 09:07:01 conftest.collect_after_test              L2823 INFO   | Dumping Disk and Memory Space information after test on crdc-garnet-sonic-ud
25/11/2025 09:07:02 conftest.collect_after_test              L2827 INFO   | Collecting core dumps after test on crdc-garnet-sonic-ud
25/11/2025 09:07:02 conftest.collect_after_test              L2838 INFO   | Collecting running config after test on crdc-garnet-sonic-ud
25/11/2025 09:07:03 conftest.core_dump_and_config_check      L2984 INFO   | Core dump and config check passed for snappi_tests/pfc/warm_reboot/test_pfc_pause_lossless_warm_reboot.py
25/11/2025 09:07:03 __init__.pytest_terminal_summary         L0067 INFO   | Can not get Allure report URL. Please check logs
